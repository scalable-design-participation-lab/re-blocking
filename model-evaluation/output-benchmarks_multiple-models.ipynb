{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: All imports\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "from skimage.metrics import (\n",
    "    mean_squared_error, \n",
    "    structural_similarity as ssim, \n",
    "    peak_signal_noise_ratio\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration Parameters\n",
    "\n",
    "# Core Parcel Evaluation Configuration\n",
    "PARCEL_CONFIG = {\n",
    "    # Core Parameters\n",
    "    'MIN_AREA': 50,                    # Minimum area for a parcel to be considered valid\n",
    "    'COLOR_DIST_THRESHOLD': 30,        # Maximum color distance for matching\n",
    "    'WIN_SIZE_FOR_SSIM': 3,           # Window size for SSIM calculation\n",
    "    \n",
    "    # Processing Parameters\n",
    "    'CHUNK_SIZE': 100,                # Size of chunks for processing large images\n",
    "    'CONTOUR_EPSILON_FACTOR': 0.02,   # Factor for polygon approximation\n",
    "    \n",
    "    # Visualization Parameters\n",
    "    'SAVE_VISUALIZATIONS': True,      # Whether to save comparison visualizations\n",
    "    'DPI': 150,                       # DPI for saved visualizations\n",
    "    'FIGURE_SIZE': (12, 6),           # Size of comparison figures\n",
    "    'POLYGON_LINE_WIDTH': 1,          # Width of polygon outlines in visualizations\n",
    "    'REAL_POLYGON_COLOR': 'g',        # Color for real polygons\n",
    "    'FAKE_POLYGON_COLOR': 'r'         # Color for fake polygons\n",
    "}\n",
    "\n",
    "# Directory Evaluation Configuration\n",
    "DIR_EVAL_CONFIG = {\n",
    "    # File Pattern Settings\n",
    "    'REAL_SUFFIX': 'real_B.png',      # Suffix for real images\n",
    "    'FAKE_SUFFIX': 'fake_B.png',      # Suffix for fake images\n",
    "    'DEFAULT_PATTERN': '*.png',        # Default file pattern for searching\n",
    "    \n",
    "    # Output Settings\n",
    "    'DEFAULT_BENCHMARK_DIR': 'benchmark-outputs',  # Default output directory\n",
    "    'SAVE_DETAILED_METRICS': True,     # Whether to save per-image metrics\n",
    "    'SAVE_SUMMARY_STATS': True,        # Whether to save summary statistics\n",
    "    \n",
    "    # Processing Settings\n",
    "    'PARALLEL_PROCESSING': False,      # Whether to use parallel processing\n",
    "    'NUM_WORKERS': 4,                  # Number of workers for parallel processing\n",
    "    \n",
    "    # Reporting Settings\n",
    "    'REPORT_DECIMAL_PLACES': 4,        # Number of decimal places in reports\n",
    "    'INCLUDE_TIMESTAMPS': True         # Whether to include timestamps in reports\n",
    "}\n",
    "\n",
    "# Multi-model Comparison Configuration\n",
    "COMPARISON_CONFIG = {\n",
    "    # Output Settings\n",
    "    'DEFAULT_OUTPUT_DIR': 'benchmark-outputs/multi_model_comparison',\n",
    "    'PLOTS_SUBDIRECTORY': 'comparative_plots',\n",
    "    'REPORTS_SUBDIRECTORY': 'reports',\n",
    "    \n",
    "    # Visualization Settings\n",
    "    'PLOT_DPI': 300,\n",
    "    'BAR_PLOT_SIZE': (12, 6),\n",
    "    'HEATMAP_SIZE': (15, 10),\n",
    "    'VIOLIN_PLOT_SIZE': (12, 6),\n",
    "    'COLOR_PALETTE': 'husl',          # seaborn color palette\n",
    "    'PLOT_GRID_STYLE': '--',\n",
    "    'PLOT_GRID_ALPHA': 0.7,\n",
    "    \n",
    "    # Metrics Configuration\n",
    "    'KEY_METRICS': {\n",
    "        'mean_iou': 'Mean IoU',\n",
    "        'ssim': 'SSIM',\n",
    "        'psnr': 'PSNR',\n",
    "        'polygon_count_ratio': 'Polygon Count Ratio',\n",
    "        'mean_area_ratio': 'Mean Area Ratio'\n",
    "    },\n",
    "    \n",
    "    # Metrics where higher values are better\n",
    "    'HIGHER_BETTER_METRICS': [\n",
    "        'ssim', \n",
    "        'psnr', \n",
    "        'mean_iou'\n",
    "    ],\n",
    "    \n",
    "    # Report Settings\n",
    "    'DECIMAL_PLACES': 4,\n",
    "    'INCLUDE_TIMESTAMPS': True,\n",
    "    \n",
    "    # Model Paths\n",
    "    'MODEL_PATHS': {\n",
    "        'brooklyn-boston-model': \"../data/ny-brooklyn/ma-boston-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "        'brooklyn-charlotte-model': \"../data/ny-brooklyn/nc-charlotte-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "        'brooklyn-manhattan-model': \"../data/ny-brooklyn/ny-manhattan-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "        'brooklyn-pittsburgh-model': \"../data/ny-brooklyn/pa-pittsburgh-p2p-500-150-v100/test_latest_500e-Brooklyn/images\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: ParcelEvaluator Class\n",
    "\n",
    "class ParcelEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_area=PARCEL_CONFIG['MIN_AREA'],\n",
    "        color_dist_threshold=PARCEL_CONFIG['COLOR_DIST_THRESHOLD'],\n",
    "        win_size_for_ssim=PARCEL_CONFIG['WIN_SIZE_FOR_SSIM'],\n",
    "        save_visualizations=PARCEL_CONFIG['SAVE_VISUALIZATIONS']\n",
    "    ):\n",
    "        \"\"\"Initialize the ParcelEvaluator with configuration parameters.\"\"\"\n",
    "        self.min_area = min_area\n",
    "        self.color_dist_threshold = color_dist_threshold\n",
    "        self.win_size_for_ssim = win_size_for_ssim\n",
    "        self.save_visualizations = save_visualizations\n",
    "        self.chunk_size = PARCEL_CONFIG['CHUNK_SIZE']\n",
    "        self.contour_epsilon_factor = PARCEL_CONFIG['CONTOUR_EPSILON_FACTOR']\n",
    "\n",
    "    def create_benchmark_id(self, model_name=None):\n",
    "        \"\"\"Create a unique identifier for this benchmark run.\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        if model_name:\n",
    "            return f\"{model_name}_{timestamp}\"\n",
    "        return f\"benchmark_{timestamp}\"\n",
    "\n",
    "    def setup_output_directory(self, base_dir, benchmark_id):\n",
    "        \"\"\"Create and setup output directory structure.\"\"\"\n",
    "        output_dir = Path(base_dir) / benchmark_id\n",
    "        \n",
    "        # Create subdirectories\n",
    "        (output_dir / \"visualizations\").mkdir(parents=True, exist_ok=True)\n",
    "        (output_dir / \"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        return output_dir\n",
    "\n",
    "    def save_config(self, output_dir):\n",
    "        \"\"\"Save evaluator configuration.\"\"\"\n",
    "        config = {\n",
    "            'min_area': self.min_area,\n",
    "            'color_dist_threshold': self.color_dist_threshold,\n",
    "            'win_size_for_ssim': self.win_size_for_ssim,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(output_dir / \"metrics\" / \"config.json\", 'w') as f:\n",
    "            json.dump(config, f, indent=4)\n",
    "\n",
    "    def load_and_preprocess(self, image_path):\n",
    "        \"\"\"Load and preprocess image to RGB.\"\"\"\n",
    "        img_bgr = cv2.imread(str(image_path))\n",
    "        if img_bgr is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        return cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    def parse_color_coded_image(self, image_rgb):\n",
    "        \"\"\"Extract exact color polygons from real image.\"\"\"\n",
    "        h, w = image_rgb.shape[:2]\n",
    "        color2mask = {}\n",
    "        image_rgb = image_rgb.astype(np.float32)\n",
    "\n",
    "        for y_start in range(0, h, self.chunk_size):\n",
    "            y_end = min(y_start + self.chunk_size, h)\n",
    "            for x_start in range(0, w, self.chunk_size):\n",
    "                x_end = min(x_start + self.chunk_size, w)\n",
    "                chunk = image_rgb[y_start:y_end, x_start:x_end]\n",
    "                \n",
    "                for y in range(chunk.shape[0]):\n",
    "                    for x in range(chunk.shape[1]):\n",
    "                        c = tuple(chunk[y, x])\n",
    "                        if c not in color2mask:\n",
    "                            color2mask[c] = np.zeros((h, w), dtype=np.uint8)\n",
    "                        color2mask[c][y_start + y, x_start + x] = 1\n",
    "\n",
    "        return self._masks_to_polygons(color2mask)\n",
    "\n",
    "    def parse_fake_image(self, fake_rgb, target_colors):\n",
    "        \"\"\"Parse fake image using color threshold approach.\"\"\"\n",
    "        h, w = fake_rgb.shape[:2]\n",
    "        fake_rgb = fake_rgb.astype(np.float32)\n",
    "        color2mask = {tuple(float(x) for x in c): np.zeros((h, w), dtype=np.uint8) \n",
    "                     for c in target_colors}\n",
    "\n",
    "        for y_start in tqdm(range(0, h, self.chunk_size), desc=\"Processing fake image\", leave=False):\n",
    "            y_end = min(y_start + self.chunk_size, h)\n",
    "            for x_start in range(0, w, self.chunk_size):\n",
    "                x_end = min(x_start + self.chunk_size, w)\n",
    "                chunk = fake_rgb[y_start:y_end, x_start:x_end]\n",
    "                \n",
    "                for y in range(chunk.shape[0]):\n",
    "                    for x in range(chunk.shape[1]):\n",
    "                        pixel = chunk[y, x]\n",
    "                        best_color = min(\n",
    "                            color2mask.keys(),\n",
    "                            key=lambda c: np.sqrt(np.sum((pixel - c) ** 2))\n",
    "                        )\n",
    "                        if np.sqrt(np.sum((pixel - best_color) ** 2)) < self.color_dist_threshold:\n",
    "                            color2mask[best_color][y_start + y, x_start + x] = 1\n",
    "\n",
    "        return self._masks_to_polygons(color2mask)\n",
    "\n",
    "    def _masks_to_polygons(self, color2mask):\n",
    "        \"\"\"Convert masks to merged polygons.\"\"\"\n",
    "        color2poly = {}\n",
    "        for color, mask in color2mask.items():\n",
    "            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "            polys = []\n",
    "            for cnt in contours:\n",
    "                area = cv2.contourArea(cnt)\n",
    "                if area < self.min_area:\n",
    "                    continue\n",
    "                ep = self.contour_epsilon_factor * cv2.arcLength(cnt, True)\n",
    "                approx = cv2.approxPolyDP(cnt, ep, True)\n",
    "                coords = np.squeeze(approx).reshape(-1, 2) if approx.size > 0 else np.array([])\n",
    "                if coords.size > 0:\n",
    "                    try:\n",
    "                        p = Polygon(coords)\n",
    "                        if p.is_valid and p.area >= self.min_area:\n",
    "                            polys.append(p)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    \n",
    "            if not polys:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                merged = unary_union(polys)\n",
    "                if merged.geom_type == 'MultiPolygon':\n",
    "                    big = max(merged.geoms, key=lambda g: g.area)\n",
    "                    color2poly[color] = big\n",
    "                else:\n",
    "                    color2poly[color] = merged\n",
    "            except Exception as e:\n",
    "                print(f\"Error merging polygons for color {color}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "        return color2poly\n",
    "\n",
    "    def compute_geometric_metrics(self, real_polys, fake_polys):\n",
    "        \"\"\"Compute geometric comparison metrics.\"\"\"\n",
    "        metrics = {\n",
    "            'polygon_count_ratio': len(fake_polys) / len(real_polys) if real_polys else 0,\n",
    "            'area_ratios': [],\n",
    "            'iou_scores': []\n",
    "        }\n",
    "        \n",
    "        for color, real_poly in real_polys.items():\n",
    "            if color in fake_polys:\n",
    "                fake_poly = fake_polys[color]\n",
    "                # Area ratio\n",
    "                area_ratio = fake_poly.area / real_poly.area if real_poly.area > 0 else 0\n",
    "                metrics['area_ratios'].append(area_ratio)\n",
    "                \n",
    "                # IoU\n",
    "                try:\n",
    "                    intersection = real_poly.intersection(fake_poly).area\n",
    "                    union = real_poly.union(fake_poly).area\n",
    "                    iou = intersection / union if union > 0 else 0\n",
    "                    metrics['iou_scores'].append(iou)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "        metrics['mean_area_ratio'] = np.mean(metrics['area_ratios']) if metrics['area_ratios'] else 0\n",
    "        metrics['mean_iou'] = np.mean(metrics['iou_scores']) if metrics['iou_scores'] else 0\n",
    "        return metrics\n",
    "\n",
    "    def compute_image_metrics(self, real_rgb, fake_rgb):\n",
    "        \"\"\"Compute traditional image comparison metrics.\"\"\"\n",
    "        if real_rgb.shape != fake_rgb.shape:\n",
    "            return {'mse': float('inf'), 'psnr': 0, 'ssim': 0}\n",
    "            \n",
    "        real_f = real_rgb.astype(np.float32) / 255\n",
    "        fake_f = fake_rgb.astype(np.float32) / 255\n",
    "        \n",
    "        metrics = {\n",
    "            'mse': mean_squared_error(real_f, fake_f),\n",
    "            'psnr': peak_signal_noise_ratio(real_f, fake_f, data_range=1.0),\n",
    "            'ssim': ssim(real_f, fake_f, data_range=1.0, \n",
    "                        channel_axis=2, win_size=self.win_size_for_ssim)\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def visualize_comparison(self, real_rgb, fake_rgb, real_polys, fake_polys, output_path):\n",
    "        \"\"\"Create visualization of the comparison.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=PARCEL_CONFIG['FIGURE_SIZE'])\n",
    "        \n",
    "        # Plot fake image and its polygons\n",
    "        ax1.imshow(fake_rgb)\n",
    "        for poly in fake_polys.values():\n",
    "            if poly.is_valid:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax1.plot(x, y, f\"{PARCEL_CONFIG['FAKE_POLYGON_COLOR']}-\", \n",
    "                        linewidth=PARCEL_CONFIG['POLYGON_LINE_WIDTH'])\n",
    "        ax1.set_title('Generated Image with Detected Parcels')\n",
    "        ax1.axis('off')\n",
    "        \n",
    "        # Plot real image and its polygons\n",
    "        ax2.imshow(real_rgb)\n",
    "        for poly in real_polys.values():\n",
    "            if poly.is_valid:\n",
    "                x, y = poly.exterior.xy\n",
    "                ax2.plot(x, y, f\"{PARCEL_CONFIG['REAL_POLYGON_COLOR']}-\", \n",
    "                        linewidth=PARCEL_CONFIG['POLYGON_LINE_WIDTH'])\n",
    "        ax2.set_title('Ground Truth with Parcels')\n",
    "        ax2.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_path, dpi=PARCEL_CONFIG['DPI'], bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    def evaluate(self, real_path, fake_path, benchmark_dir=\"benchmark-outputs\", model_name=None):\n",
    "        \"\"\"Main evaluation function.\"\"\"\n",
    "        benchmark_id = self.create_benchmark_id(model_name)\n",
    "        output_dir = self.setup_output_directory(benchmark_dir, benchmark_id)\n",
    "        \n",
    "        self.save_config(output_dir)\n",
    "        \n",
    "        real_rgb = self.load_and_preprocess(real_path)\n",
    "        fake_rgb = self.load_and_preprocess(fake_path)\n",
    "        \n",
    "        real_polys = self.parse_color_coded_image(real_rgb)\n",
    "        fake_polys = self.parse_fake_image(fake_rgb, list(real_polys.keys()))\n",
    "        \n",
    "        geometric_metrics = self.compute_geometric_metrics(real_polys, fake_polys)\n",
    "        image_metrics = self.compute_image_metrics(real_rgb, fake_rgb)\n",
    "        \n",
    "        all_metrics = {**geometric_metrics, **image_metrics}\n",
    "        \n",
    "        metrics_df = pd.DataFrame([all_metrics])\n",
    "        metrics_df.to_csv(output_dir / \"metrics\" / \"single_image_metrics.csv\", index=False)\n",
    "        \n",
    "        if self.save_visualizations:\n",
    "            viz_path = output_dir / \"visualizations\" / f\"{Path(fake_path).stem}_comparison.png\"\n",
    "            self.visualize_comparison(real_rgb, fake_rgb, real_polys, fake_polys, viz_path)\n",
    "        \n",
    "        return all_metrics, benchmark_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Directory Evaluation Functions\n",
    "\n",
    "def evaluate_directory(\n",
    "    real_dir, \n",
    "    fake_dir, \n",
    "    benchmark_dir=DIR_EVAL_CONFIG['DEFAULT_BENCHMARK_DIR'], \n",
    "    model_name=None, \n",
    "    pattern=DIR_EVAL_CONFIG['DEFAULT_PATTERN'], \n",
    "    **kwargs\n",
    "):\n",
    "    \"\"\"Evaluate all matching images in directories with organized outputs.\"\"\"\n",
    "    evaluator = ParcelEvaluator(**kwargs)\n",
    "    \n",
    "    # Create benchmark ID and setup directories\n",
    "    benchmark_id = evaluator.create_benchmark_id(model_name)\n",
    "    output_dir = evaluator.setup_output_directory(benchmark_dir, benchmark_id)\n",
    "    \n",
    "    # Save configuration\n",
    "    evaluator.save_config(output_dir)\n",
    "    \n",
    "    # Get all matching files\n",
    "    real_dir = Path(real_dir)\n",
    "    fake_dir = Path(fake_dir)\n",
    "    \n",
    "    # Find pairs of real and fake images\n",
    "    real_files = []\n",
    "    fake_files = []\n",
    "    \n",
    "    # Look for real_B images and their corresponding fake_B images\n",
    "    for real_file in sorted(real_dir.glob(f\"*{DIR_EVAL_CONFIG['REAL_SUFFIX']}\")):\n",
    "        fake_file = fake_dir / real_file.name.replace(\n",
    "            DIR_EVAL_CONFIG['REAL_SUFFIX'], \n",
    "            DIR_EVAL_CONFIG['FAKE_SUFFIX']\n",
    "        )\n",
    "        if fake_file.exists():\n",
    "            real_files.append(real_file)\n",
    "            fake_files.append(fake_file)\n",
    "    \n",
    "    if not real_files:\n",
    "        raise ValueError(f\"No matching image pairs found in {real_dir} and {fake_dir}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results = []\n",
    "    \n",
    "    for real_file, fake_file in tqdm(zip(real_files, fake_files), \n",
    "                                    total=len(real_files),\n",
    "                                    desc=f\"Processing {model_name if model_name else 'images'}\"):\n",
    "        try:\n",
    "            metrics, _ = evaluator.evaluate(\n",
    "                real_file, \n",
    "                fake_file, \n",
    "                output_dir / \"visualizations\"\n",
    "            )\n",
    "            metrics['file_name'] = fake_file.name\n",
    "            all_results.append(metrics)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {fake_file.name}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    if DIR_EVAL_CONFIG['SAVE_DETAILED_METRICS']:\n",
    "        df.to_csv(output_dir / \"metrics\" / \"detailed_metrics.csv\", index=False)\n",
    "    \n",
    "    if DIR_EVAL_CONFIG['SAVE_SUMMARY_STATS']:\n",
    "        # Compute and save summary statistics\n",
    "        summary_stats = df.describe()\n",
    "        summary_stats.to_csv(output_dir / \"metrics\" / \"summary_statistics.csv\")\n",
    "        \n",
    "        # Save averages in a more readable format\n",
    "        avg_metrics = df.mean(numeric_only=True).round(DIR_EVAL_CONFIG['REPORT_DECIMAL_PLACES'])\n",
    "        summary_dict = {\n",
    "            'model_name': model_name,\n",
    "            'average_metrics': avg_metrics.to_dict(),\n",
    "            'total_images_processed': len(all_results),\n",
    "            'successful_evaluations': len(all_results),\n",
    "            'failed_evaluations': len(real_files) - len(all_results)\n",
    "        }\n",
    "        \n",
    "        if DIR_EVAL_CONFIG['INCLUDE_TIMESTAMPS']:\n",
    "            summary_dict['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "        with open(output_dir / \"metrics\" / \"summary.json\", 'w') as f:\n",
    "            json.dump(summary_dict, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_dir}\")\n",
    "    print(\"\\nAverage Metrics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for metric, value in avg_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{metric}: {value:.{DIR_EVAL_CONFIG['REPORT_DECIMAL_PLACES']}f}\")\n",
    "    \n",
    "    return df, benchmark_id\n",
    "\n",
    "def create_metric_summary(metrics_df, metric_name):\n",
    "    \"\"\"Create a summary of a specific metric from the results DataFrame.\"\"\"\n",
    "    summary = {\n",
    "        'mean': metrics_df[metric_name].mean(),\n",
    "        'std': metrics_df[metric_name].std(),\n",
    "        'min': metrics_df[metric_name].min(),\n",
    "        'max': metrics_df[metric_name].max(),\n",
    "        'median': metrics_df[metric_name].median()\n",
    "    }\n",
    "    return {k: round(v, DIR_EVAL_CONFIG['REPORT_DECIMAL_PLACES']) for k, v in summary.items()}\n",
    "\n",
    "def generate_model_report(df, model_name, output_dir):\n",
    "    \"\"\"Generate a detailed report for a single model's performance.\"\"\"\n",
    "    report = {\n",
    "        'model_name': model_name,\n",
    "        'number_of_images': len(df),\n",
    "        'metrics': {}\n",
    "    }\n",
    "    \n",
    "    # Get metrics (excluding non-numeric columns)\n",
    "    metric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    for metric in metric_columns:\n",
    "        report['metrics'][metric] = create_metric_summary(df, metric)\n",
    "    \n",
    "    if DIR_EVAL_CONFIG['INCLUDE_TIMESTAMPS']:\n",
    "        report['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Save report\n",
    "    output_path = Path(output_dir) / f\"{model_name}_detailed_report.json\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "    \n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Multi-model Comparison Functions\n",
    "\n",
    "def evaluate_multiple_models(\n",
    "    model_paths=COMPARISON_CONFIG['MODEL_PATHS'], \n",
    "    benchmark_dir=COMPARISON_CONFIG['DEFAULT_OUTPUT_DIR']\n",
    "):\n",
    "    \"\"\"Evaluate multiple models and compare their performance.\"\"\"\n",
    "    \n",
    "    # Create parent directory for this comparison\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    parent_dir = Path(benchmark_dir) / f\"comparison_{timestamp}\"\n",
    "    parent_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    plots_dir = parent_dir / COMPARISON_CONFIG['PLOTS_SUBDIRECTORY']\n",
    "    reports_dir = parent_dir / COMPARISON_CONFIG['REPORTS_SUBDIRECTORY']\n",
    "    plots_dir.mkdir(exist_ok=True)\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Store results for each model\n",
    "    model_results = {}\n",
    "    \n",
    "    # Process each model\n",
    "    for model_name, base_path in model_paths.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        base_path = Path(base_path)\n",
    "        \n",
    "        # Evaluate this model\n",
    "        df, benchmark_id = evaluate_directory(\n",
    "            real_dir=base_path,\n",
    "            fake_dir=base_path,\n",
    "            benchmark_dir=parent_dir / model_name,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        # Generate detailed report\n",
    "        report = generate_model_report(df, model_name, reports_dir)\n",
    "        \n",
    "        # Store results\n",
    "        model_results[model_name] = {\n",
    "            'metrics_df': df,\n",
    "            'benchmark_id': benchmark_id,\n",
    "            'avg_metrics': df.mean(numeric_only=True),\n",
    "            'report': report\n",
    "        }\n",
    "    \n",
    "    # Create comparison visualizations\n",
    "    create_comparison_plots(model_results, plots_dir)\n",
    "    \n",
    "    # Save comparative summary\n",
    "    save_comparative_summary(model_results, parent_dir)\n",
    "    \n",
    "    return model_results, parent_dir\n",
    "\n",
    "def create_comparison_plots(model_results, output_dir):\n",
    "    \"\"\"Create comparative visualizations of model performances.\"\"\"\n",
    "    # Get consistent colors for models\n",
    "    colors = sns.color_palette(COMPARISON_CONFIG['COLOR_PALETTE'], \n",
    "                             n_colors=len(model_results))\n",
    "    model_colors = dict(zip(model_results.keys(), colors))\n",
    "    \n",
    "    # Create individual metric comparisons\n",
    "    for metric, metric_label in COMPARISON_CONFIG['KEY_METRICS'].items():\n",
    "        # Bar plot\n",
    "        plt.figure(figsize=COMPARISON_CONFIG['BAR_PLOT_SIZE'])\n",
    "        values = [results['avg_metrics'][metric] for results in model_results.values()]\n",
    "        models = list(model_results.keys())\n",
    "        \n",
    "        bars = plt.bar(models, values, color=[model_colors[model] for model in models])\n",
    "        \n",
    "        plt.title(f'Comparison of {metric_label}', pad=20)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel(metric_label)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.{COMPARISON_CONFIG[\"DECIMAL_PLACES\"]}f}',\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "        plt.grid(True, axis='y', linestyle=COMPARISON_CONFIG['PLOT_GRID_STYLE'], \n",
    "                alpha=COMPARISON_CONFIG['PLOT_GRID_ALPHA'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{metric}_comparison.png', \n",
    "                   dpi=COMPARISON_CONFIG['PLOT_DPI'], \n",
    "                   bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Violin plot\n",
    "        plt.figure(figsize=COMPARISON_CONFIG['VIOLIN_PLOT_SIZE'])\n",
    "        data_dict = {model_name: results['metrics_df'][metric].values \n",
    "                    for model_name, results in model_results.items()}\n",
    "        \n",
    "        violin_parts = plt.violinplot([data_dict[model] for model in models],\n",
    "                                    showmeans=True, showmedians=True)\n",
    "        \n",
    "        plt.title(f'Distribution of {metric_label} Across Images', pad=20)\n",
    "        plt.xticks(range(1, len(models) + 1), models, rotation=45, ha='right')\n",
    "        plt.ylabel(metric_label)\n",
    "        plt.grid(True, axis='y', linestyle=COMPARISON_CONFIG['PLOT_GRID_STYLE'], \n",
    "                alpha=COMPARISON_CONFIG['PLOT_GRID_ALPHA'])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_dir / f'{metric}_distribution.png', \n",
    "                   dpi=COMPARISON_CONFIG['PLOT_DPI'], \n",
    "                   bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=COMPARISON_CONFIG['HEATMAP_SIZE'])\n",
    "    metrics_data = pd.DataFrame({\n",
    "        model_name: results['avg_metrics'] \n",
    "        for model_name, results in model_results.items()\n",
    "    })\n",
    "    \n",
    "    sns.heatmap(metrics_data, annot=True, \n",
    "                fmt=f'.{COMPARISON_CONFIG[\"DECIMAL_PLACES\"]}f', \n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'Metric Value'})\n",
    "    plt.title('Model Comparison Heatmap')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'metrics_heatmap.png', \n",
    "                dpi=COMPARISON_CONFIG['PLOT_DPI'], \n",
    "                bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def save_comparative_summary(model_results, output_dir):\n",
    "    \"\"\"Save comparative summary of all models.\"\"\"\n",
    "    summary = {\n",
    "        'models_compared': list(model_results.keys()),\n",
    "        'model_metrics': {\n",
    "            model_name: results['avg_metrics'].to_dict()\n",
    "            for model_name, results in model_results.items()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if COMPARISON_CONFIG['INCLUDE_TIMESTAMPS']:\n",
    "        summary['timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # Determine best model for each metric\n",
    "    best_models = {}\n",
    "    for metric in COMPARISON_CONFIG['KEY_METRICS'].keys():\n",
    "        metric_values = {model: metrics[metric] \n",
    "                        for model, metrics in summary['model_metrics'].items()}\n",
    "        \n",
    "        if metric in COMPARISON_CONFIG['HIGHER_BETTER_METRICS']:\n",
    "            best_model = max(metric_values.items(), key=lambda x: x[1])\n",
    "        else:  # For metrics where closer to 1.0 is better\n",
    "            best_model = min(metric_values.items(), \n",
    "                           key=lambda x: abs(x[1] - 1.0))\n",
    "        \n",
    "        best_models[metric] = {\n",
    "            'best_model': best_model[0],\n",
    "            'value': round(best_model[1], COMPARISON_CONFIG['DECIMAL_PLACES']),\n",
    "            'all_values': {k: round(v, COMPARISON_CONFIG['DECIMAL_PLACES']) \n",
    "                          for k, v in metric_values.items()}\n",
    "        }\n",
    "    \n",
    "    summary['best_models'] = best_models\n",
    "    \n",
    "    # Calculate overall ranking\n",
    "    model_scores = {model: 0 for model in model_results.keys()}\n",
    "    for metric_result in best_models.values():\n",
    "        model_scores[metric_result['best_model']] += 1\n",
    "    \n",
    "    summary['overall_ranking'] = dict(sorted(model_scores.items(), \n",
    "                                           key=lambda x: x[1], \n",
    "                                           reverse=True))\n",
    "    \n",
    "    # Save summary\n",
    "    with open(output_dir / 'comparative_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nComparative Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"\\nBest performing models by metric:\")\n",
    "    for metric, result in best_models.items():\n",
    "        print(f\"{metric}: {result['best_model']} \"\n",
    "              f\"({result['value']:.{COMPARISON_CONFIG['DECIMAL_PLACES']}f})\")\n",
    "    \n",
    "    print(\"\\nOverall Ranking (number of metrics won):\")\n",
    "    for model, score in summary['overall_ranking'].items():\n",
    "        print(f\"{model}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing brooklyn-boston-model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d69788aee24567b5483693b1a8ead7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing brooklyn-boston-model:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e956e172cde469f84196c10bb7c5fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52adfd243c8f4a478e6554709d77cfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a6ec42a463e401abc754ed11546eb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b485974d8a4a0da7e683f7ec7b0f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49985d73f07d43fea460f011c0f55d65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed32c954ced4749a1a76f0055323cb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4599c2fa186b41deb88374a52eb3617f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3441a3f4884cc9ae88787877a0ba18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a134566acb2d407ab6a7462c97787733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438b71bad10d47dc82503fd921417abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c16a7e81d2d5458c9d6b0c58a408b201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b527e4894a642e996f8c94122a018fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing fake image:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 6: Run the Multi-model Evaluation\n",
    "\n",
    "# Run evaluation using the configured model paths\n",
    "results, output_dir = evaluate_multiple_models()\n",
    "print(f\"\\nAll results saved to: {output_dir}\")\n",
    "\n",
    "# Access results for specific models if needed\n",
    "for model_name, model_data in results.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    print(\"Average Metrics:\")\n",
    "    for metric, value in model_data['avg_metrics'].items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
