{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.ops import unary_union\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import gc\n",
    "\n",
    "# For geometry metrics\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# For image-level metrics\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import mean_squared_error, peak_signal_noise_ratio\n",
    "\n",
    "# For KL-divergence\n",
    "from scipy.stats import entropy as kl_divergence\n",
    "\n",
    "###############################################################################\n",
    "# Global config\n",
    "###############################################################################\n",
    "\n",
    "matplotlib.use('Agg')  # non-interactive backend for saving plots\n",
    "\n",
    "SAMPLE_SIZE = 5\n",
    "TEST_FOLDERS = {\n",
    "    'brooklyn-boston-model': \"../data/ny-brooklyn/ma-boston-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-charlotte-model': \"../data/ny-brooklyn/nc-charlotte-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-manhattan-model': \"../data/ny-brooklyn/ny-manhattan-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-pittsburgh-model': \"../data/ny-brooklyn/pa-pittsburgh-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "}\n",
    "\n",
    "def cleanup_memory():\n",
    "    gc.collect()\n",
    "\n",
    "def generate_output_dir(test_folders, sample_size):\n",
    "    first_folder = list(test_folders.keys())[0]\n",
    "    base_name = first_folder.split('-')[0]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "    identifier = f\"{base_name}_n{sample_size}_{timestamp}\"\n",
    "    return os.path.join(\"benchmark-output\", identifier)\n",
    "\n",
    "OUTPUT_DIR = generate_output_dir(TEST_FOLDERS, SAMPLE_SIZE)\n",
    "print(f\"All results will be saved under: {OUTPUT_DIR}\")\n",
    "\n",
    "MIN_AREA = 50  # Minimum area for a valid parcel\n",
    "\n",
    "###############################################################################\n",
    "# 1) Parse color-coded image into dict[color] -> Polygon\n",
    "###############################################################################\n",
    "\n",
    "def parse_color_coded_image(image_rgb, min_area=MIN_AREA):\n",
    "    \"\"\"\n",
    "    Given an RGB image where each parcel is a unique color,\n",
    "    return a dict: { (R,G,B) -> shapely Polygon }\n",
    "\n",
    "    If there's more than one connected component for a given color,\n",
    "    we merge them via unary_union, picking the biggest if it's multipolygon.\n",
    "    \"\"\"\n",
    "    h, w = image_rgb.shape[:2]\n",
    "    color2mask = {}\n",
    "\n",
    "    # Build up a mask for each color\n",
    "    # shape: (H, W, 3) => each pixel is (R, G, B)\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            color = tuple(image_rgb[y, x])  # (R, G, B)\n",
    "            if color not in color2mask:\n",
    "                color2mask[color] = np.zeros((h, w), dtype=np.uint8)\n",
    "            color2mask[color][y, x] = 1\n",
    "\n",
    "    color2poly = {}\n",
    "    for color, mask in color2mask.items():\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        polys = []\n",
    "        for cnt in contours:\n",
    "            area = cv2.contourArea(cnt)\n",
    "            if area < min_area:\n",
    "                continue\n",
    "            epsilon = 0.02 * cv2.arcLength(cnt, True)\n",
    "            approx = cv2.approxPolyDP(cnt, epsilon, True)\n",
    "            coords = np.squeeze(approx).reshape(-1, 2)\n",
    "            poly = Polygon(coords)\n",
    "            if poly.is_valid and poly.area >= min_area:\n",
    "                polys.append(poly)\n",
    "\n",
    "        if not polys:\n",
    "            continue\n",
    "\n",
    "        merged = unary_union(polys)\n",
    "        # If multipolygon, pick the largest\n",
    "        if merged.geom_type == 'MultiPolygon':\n",
    "            bigpoly = max(merged.geoms, key=lambda g: g.area)\n",
    "            color2poly[color] = bigpoly\n",
    "        else:\n",
    "            color2poly[color] = merged\n",
    "\n",
    "    return color2poly\n",
    "\n",
    "###############################################################################\n",
    "# 2) Compare dict[color]->Polygon across fake vs real\n",
    "###############################################################################\n",
    "\n",
    "def compare_color_coded_parcels(fake_color2poly, real_color2poly):\n",
    "    \"\"\"\n",
    "    For each color in fake_color2poly, see if it exists in real_color2poly.\n",
    "    We compute IoU, Dice, Hausdorff, etc.\n",
    "\n",
    "    Return:\n",
    "      - a list of records, each with { color, iou, dice, hausdorff, fake_area, real_area }\n",
    "      - plus overall means\n",
    "    \"\"\"\n",
    "    # store one row per color\n",
    "    records = []\n",
    "    # shared colors\n",
    "    shared_colors = set(fake_color2poly.keys()).intersection(set(real_color2poly.keys()))\n",
    "\n",
    "    for color in shared_colors:\n",
    "        fpoly = fake_color2poly[color]\n",
    "        rpoly = real_color2poly[color]\n",
    "        if not fpoly.is_valid or not rpoly.is_valid:\n",
    "            continue\n",
    "        inter_area = fpoly.intersection(rpoly).area\n",
    "        union_area = fpoly.union(rpoly).area\n",
    "        iou = inter_area / union_area if union_area>0 else 0\n",
    "\n",
    "        area_sum = fpoly.area + rpoly.area\n",
    "        dice = 2.0*inter_area / area_sum if area_sum>0 else 0\n",
    "\n",
    "        # Hausdorff\n",
    "        try:\n",
    "            haus = fpoly.hausdorff_distance(rpoly)\n",
    "        except AttributeError:\n",
    "            # older shapely\n",
    "            from shapely.ops import hausdorff_distance\n",
    "            haus = hausdorff_distance(fpoly, rpoly)\n",
    "\n",
    "        rec = {\n",
    "            'color': color,\n",
    "            'iou': iou,\n",
    "            'dice': dice,\n",
    "            'hausdorff': haus,\n",
    "            'fake_area': fpoly.area,\n",
    "            'real_area': rpoly.area\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    # Missing in real\n",
    "    missing_in_real = set(fake_color2poly.keys()) - set(real_color2poly.keys())\n",
    "    for c in missing_in_real:\n",
    "        poly = fake_color2poly[c]\n",
    "        rec = {\n",
    "            'color': c,\n",
    "            'iou': 0,\n",
    "            'dice': 0,\n",
    "            'hausdorff': None,\n",
    "            'fake_area': poly.area if poly.is_valid else 0,\n",
    "            'real_area': 0\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    # Missing in fake\n",
    "    missing_in_fake = set(real_color2poly.keys()) - set(fake_color2poly.keys())\n",
    "    for c in missing_in_fake:\n",
    "        poly = real_color2poly[c]\n",
    "        rec = {\n",
    "            'color': c,\n",
    "            'iou': 0,\n",
    "            'dice': 0,\n",
    "            'hausdorff': None,\n",
    "            'fake_area': 0,\n",
    "            'real_area': poly.area if poly.is_valid else 0\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    # Summaries\n",
    "    if len(records)==0:\n",
    "        return {\n",
    "            'per_color': [],\n",
    "            'mean_iou': 0,\n",
    "            'mean_dice': 0,\n",
    "            'mean_hausdorff': 0\n",
    "        }\n",
    "\n",
    "    valid_haus = [r for r in records if r['hausdorff'] is not None]\n",
    "    mean_iou = np.mean([r['iou'] for r in records]) if records else 0\n",
    "    mean_dice = np.mean([r['dice'] for r in records]) if records else 0\n",
    "    mean_haus = np.mean([r['hausdorff'] for r in valid_haus]) if valid_haus else 0\n",
    "\n",
    "    return {\n",
    "        'per_color': records,\n",
    "        'mean_iou': mean_iou,\n",
    "        'mean_dice': mean_dice,\n",
    "        'mean_hausdorff': mean_haus\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 3) Optional image-based metrics (MSE, PSNR, SSIM, etc.)\n",
    "###############################################################################\n",
    "\n",
    "def compute_image_metrics(fake_rgb, real_rgb):\n",
    "    \"\"\"\n",
    "    Compare raw images for MSE, PSNR, SSIM, etc.\n",
    "    \"\"\"\n",
    "    if fake_rgb.shape != real_rgb.shape:\n",
    "        return {'mse': 0, 'psnr': 0, 'ssim': 0}\n",
    "\n",
    "    fake_f = fake_rgb.astype(np.float32)/255.0\n",
    "    real_f = real_rgb.astype(np.float32)/255.0\n",
    "\n",
    "    mse_val = mean_squared_error(real_f, fake_f)\n",
    "    psnr_val = peak_signal_noise_ratio(real_f, fake_f, data_range=1.0)\n",
    "    try:\n",
    "        ssim_val = ssim(real_f, fake_f, data_range=1.0, multichannel=True)\n",
    "    except ValueError as e:\n",
    "        print(f\"SSIM error: {e}\")\n",
    "        ssim_val = 0\n",
    "\n",
    "    return {\n",
    "        'mse': mse_val,\n",
    "        'psnr': psnr_val,\n",
    "        'ssim': ssim_val\n",
    "    }\n",
    "\n",
    "###############################################################################\n",
    "# 4) Visualization: side-by-side\n",
    "###############################################################################\n",
    "\n",
    "def create_side_by_side_visual(fake_rgb, real_rgb,\n",
    "                               fake_color2poly, real_color2poly,\n",
    "                               max_count=50):\n",
    "    \"\"\"\n",
    "    Renders a side-by-side figure: left=Fake, right=Real.\n",
    "    We'll draw polygons in red for Fake, green for Real.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1,2, figsize=(10,6))\n",
    "\n",
    "    # Left: fake\n",
    "    axes[0].imshow(fake_rgb)\n",
    "    # Only draw up to max_count polygons\n",
    "    fc = list(fake_color2poly.keys())\n",
    "    random.shuffle(fc)\n",
    "    for c in fc[:max_count]:\n",
    "        poly = fake_color2poly[c]\n",
    "        if not poly.is_valid:\n",
    "            continue\n",
    "        x,y = poly.exterior.xy\n",
    "        axes[0].plot(x,y,'r',linewidth=1)\n",
    "    axes[0].set_title(f\"FAKE: {len(fc)} color-coded parcels\")\n",
    "\n",
    "    # Right: real\n",
    "    axes[1].imshow(real_rgb)\n",
    "    rc = list(real_color2poly.keys())\n",
    "    random.shuffle(rc)\n",
    "    for c in rc[:max_count]:\n",
    "        poly = real_color2poly[c]\n",
    "        if not poly.is_valid:\n",
    "            continue\n",
    "        x,y = poly.exterior.xy\n",
    "        axes[1].plot(x,y,'g',linewidth=1)\n",
    "    axes[1].set_title(f\"REAL: {len(rc)} color-coded parcels\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "###############################################################################\n",
    "# 5) Process a Single Pair\n",
    "###############################################################################\n",
    "\n",
    "def process_single_pair(fake_path, real_path):\n",
    "    \"\"\"\n",
    "    - Load color-coded images\n",
    "    - Parse each to dict[color]->Polygon\n",
    "    - Compare per color\n",
    "    - Compute image-based metrics\n",
    "    - Return a big list of records, one row per color\n",
    "    \"\"\"\n",
    "    fake_bgr = cv2.imread(fake_path)\n",
    "    real_bgr = cv2.imread(real_path)\n",
    "\n",
    "    if fake_bgr is None or real_bgr is None:\n",
    "        print(f\"Error loading images: {fake_path}, {real_path}\")\n",
    "        return []\n",
    "\n",
    "    # Convert to RGB\n",
    "    fake_rgb = cv2.cvtColor(fake_bgr, cv2.COLOR_BGR2RGB)\n",
    "    real_rgb = cv2.cvtColor(real_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Parse\n",
    "    fake_color2poly = parse_color_coded_image(fake_rgb, MIN_AREA)\n",
    "    real_color2poly = parse_color_coded_image(real_rgb, MIN_AREA)\n",
    "\n",
    "    # Compare geometry\n",
    "    color_metrics = compare_color_coded_parcels(fake_color2poly, real_color2poly)\n",
    "    overall_iou = color_metrics['mean_iou']\n",
    "    overall_dice = color_metrics['mean_dice']\n",
    "    overall_haus = color_metrics['mean_hausdorff']\n",
    "\n",
    "    # Image-based metrics\n",
    "    img_metrics = compute_image_metrics(fake_rgb, real_rgb)\n",
    "    # MSE, PSNR, SSIM\n",
    "\n",
    "    # Create side-by-side figure\n",
    "    fig = create_side_by_side_visual(fake_rgb, real_rgb,\n",
    "                                     fake_color2poly, real_color2poly)\n",
    "    # return fig so the caller can save it.\n",
    "\n",
    "    # produce a row for each color in color_metrics['per_color']\n",
    "    # plus add overall_iou/dice/haus.\n",
    "    results = []\n",
    "    base_name = os.path.basename(fake_path)\n",
    "    file_id = base_name.split('_')[1] if '_' in base_name else base_name\n",
    "\n",
    "    for rec in color_metrics['per_color']:\n",
    "        row = dict(rec)  # iou, dice, color, etc.\n",
    "        row['file_id'] = file_id\n",
    "        row['fake_path'] = fake_path\n",
    "        row['real_path'] = real_path\n",
    "        # Overall geometry\n",
    "        row['mean_iou_overall'] = overall_iou\n",
    "        row['mean_dice_overall'] = overall_dice\n",
    "        row['mean_hausdorff_overall'] = overall_haus\n",
    "        # Image metrics\n",
    "        row['mse'] = img_metrics['mse']\n",
    "        row['psnr'] = img_metrics['psnr']\n",
    "        row['ssim'] = img_metrics['ssim']\n",
    "        results.append(row)\n",
    "\n",
    "    return results, fig, file_id\n",
    "\n",
    "###############################################################################\n",
    "# 6) Process a Folder\n",
    "###############################################################################\n",
    "\n",
    "def process_test_folder(folder_path, output_dir, sample_size=SAMPLE_SIZE):\n",
    "    print(f\"Scanning folder: {folder_path}\")\n",
    "    pairs_dict = {}\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if not fname.endswith('.png'):\n",
    "            continue\n",
    "        m = re.search(r'combined_(\\d+)_(fake|real)_B\\.png', fname)\n",
    "        if not m:\n",
    "            continue\n",
    "        base_num, img_type = m.group(1), m.group(2)\n",
    "        full_path = os.path.join(folder_path, fname)\n",
    "        if base_num not in pairs_dict:\n",
    "            pairs_dict[base_num] = {'fake': None, 'real': None}\n",
    "        pairs_dict[base_num][img_type] = full_path\n",
    "\n",
    "    complete_pairs = [(d['fake'], d['real'])\n",
    "                      for d in pairs_dict.values()\n",
    "                      if d['fake'] and d['real']]\n",
    "    if sample_size and sample_size < len(complete_pairs):\n",
    "        print(f\"Sampling {sample_size} from total {len(complete_pairs)} pairs\")\n",
    "        complete_pairs = random.sample(complete_pairs, sample_size)\n",
    "    else:\n",
    "        print(f\"Processing all {len(complete_pairs)} pairs\")\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    results = []\n",
    "    with tqdm(total=len(complete_pairs), desc=\"Processing pairs\") as pbar:\n",
    "        for (fake_path, real_path) in complete_pairs:\n",
    "            try:\n",
    "                recs, fig, file_id = process_single_pair(fake_path, real_path)\n",
    "                # Save figure\n",
    "                if fig:\n",
    "                    out_fig = os.path.join(output_dir, f\"side_by_side_{file_id}.png\")\n",
    "                    fig.savefig(out_fig, dpi=150, bbox_inches='tight')\n",
    "                    plt.close(fig)\n",
    "\n",
    "                results.extend(recs)\n",
    "            except Exception as e:\n",
    "                print(f\"Error on pair: {fake_path}, {real_path} => {e}\")\n",
    "            finally:\n",
    "                cleanup_memory()\n",
    "            pbar.update(1)\n",
    "\n",
    "    return results\n",
    "\n",
    "###############################################################################\n",
    "# 7) Generate Reports\n",
    "###############################################################################\n",
    "\n",
    "def create_visualizations(df, output_dir):\n",
    "    try:\n",
    "        # pick some columns to visualise\n",
    "        metrics = ['iou','dice','hausdorff','fake_area','real_area',\n",
    "                   'mean_iou_overall','mean_dice_overall','mean_hausdorff_overall',\n",
    "                   'mse','psnr','ssim']\n",
    "        metrics = [m for m in metrics if m in df.columns]\n",
    "\n",
    "        for metric in metrics:\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sns.histplot(df[metric], kde=True)\n",
    "            plt.title(metric)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, f\"{metric}_dist.png\"))\n",
    "            plt.close()\n",
    "\n",
    "        if len(metrics)>1:\n",
    "            corr = df[metrics].corr()\n",
    "            plt.figure(figsize=(12,10))\n",
    "            sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
    "            plt.title(\"Metric Correlations\")\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, \"metric_correlations.png\"))\n",
    "            plt.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in create_visualizations: {e}\")\n",
    "\n",
    "def generate_reports(all_results, output_dir):\n",
    "    df = pd.DataFrame(all_results)\n",
    "    out_csv = os.path.join(output_dir, \"detailed_results.csv\")\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {len(df)} rows to {out_csv}\")\n",
    "\n",
    "    # Summaries\n",
    "    agg_cols = ['iou','dice','hausdorff','fake_area','real_area',\n",
    "                'mean_iou_overall','mean_dice_overall','mean_hausdorff_overall',\n",
    "                'mse','psnr','ssim']\n",
    "    summary = {}\n",
    "    for col in agg_cols:\n",
    "        if col in df.columns and len(df[col])>0:\n",
    "            valid_vals = df[col].dropna()\n",
    "            if len(valid_vals)>0:\n",
    "                summary[col] = {\n",
    "                    'mean': valid_vals.mean(),\n",
    "                    'std':  valid_vals.std(),\n",
    "                    'min':  valid_vals.min(),\n",
    "                    'max':  valid_vals.max(),\n",
    "                    'median': valid_vals.median()\n",
    "                }\n",
    "\n",
    "    with open(os.path.join(output_dir, \"aggregate_metrics.json\"), 'w') as f:\n",
    "        json.dump(summary, f, indent=4)\n",
    "\n",
    "    # Folder-level grouping if 'folder' column is present\n",
    "    if 'folder' in df.columns:\n",
    "        folderwise = df.groupby('folder')[agg_cols].mean().reset_index()\n",
    "        folderwise.to_csv(os.path.join(output_dir, \"folderwise_metrics.csv\"), index=False)\n",
    "\n",
    "    create_visualizations(df, output_dir)\n",
    "\n",
    "###############################################################################\n",
    "# 8) Main\n",
    "###############################################################################\n",
    "\n",
    "def main():\n",
    "    print(\"Color-Coded Parcel Benchmarking\")\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "    valid_folders = {k:v for k,v in TEST_FOLDERS.items() if os.path.exists(v)}\n",
    "    if not valid_folders:\n",
    "        print(\"No valid folders found!\")\n",
    "        return\n",
    "\n",
    "    all_results = []\n",
    "    for folder_name, folder_path in valid_folders.items():\n",
    "        print(f\"\\nProcessing {folder_name} => {folder_path}\")\n",
    "        out_subdir = os.path.join(OUTPUT_DIR, folder_name)\n",
    "        os.makedirs(out_subdir, exist_ok=True)\n",
    "\n",
    "        folder_res = process_test_folder(folder_path, out_subdir, SAMPLE_SIZE)\n",
    "        # Add 'folder' column\n",
    "        for r in folder_res:\n",
    "            r['folder'] = folder_name\n",
    "        all_results.extend(folder_res)\n",
    "\n",
    "    # Once we have everything, generate global summary\n",
    "    if all_results:\n",
    "        print(\"\\nGenerating global summary...\")\n",
    "        generate_reports(all_results, OUTPUT_DIR)\n",
    "    else:\n",
    "        print(\"No results found at all.\")\n",
    "\n",
    "    print(\"Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
