{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1, Option A: Resnet Model Training / Fine tuning for better feature extraction with extended evaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Directory and Data Settings\n",
    "DATA_PARAMS = {\n",
    "    'data_folders': {\n",
    "        'Boston': '../data/ma-boston/buildings',\n",
    "        'Charlotte': '../data/nc-charlotte/buildings',\n",
    "        'Manhattan': '../data/ny-manhattan/buildings',\n",
    "        'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "    },\n",
    "    'output_dir': 'softmax-output',\n",
    "    'model_subdir': 'models',\n",
    "    'log_subdir': 'logs',\n",
    "    'viz_subdir': 'visualizations',\n",
    "    'metrics_subdir': 'metrics'\n",
    "}\n",
    "\n",
    "# Training Parameters\n",
    "TRAINING_PARAMS = {\n",
    "    # Data parameters\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'train_val_split': 0.8,\n",
    "    'num_workers': 0,  # Set to 0 to avoid multiprocessing issues\n",
    "    'max_images_per_class': None,  # Set to None for all images, or a number for limit\n",
    "    \n",
    "    # Model parameters\n",
    "    'model_type': 'ResNet50',  # Options: 'ResNet18', 'ResNet50'\n",
    "    'hidden_dim': 512,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'focal_loss_gamma': 2.0,\n",
    "    \n",
    "    # Scheduler parameters\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 5,\n",
    "    \n",
    "    # Monitoring parameters\n",
    "    'visualization_interval': 5,  # Export plots every N epochs\n",
    "    'checkpoint_interval': 10,    # Save checkpoint every N epochs\n",
    "    'max_checkpoints': 3,        # Maximum number of checkpoints to keep\n",
    "}\n",
    "\n",
    "# Image Transform Parameters\n",
    "TRANSFORM_PARAMS = {\n",
    "    'image_size': (224, 224),\n",
    "    'rotation_degrees': 15,\n",
    "    'color_jitter': {\n",
    "        'brightness': 0.2,\n",
    "        'contrast': 0.2,\n",
    "        'saturation': 0.2,\n",
    "        'hue': 0.1\n",
    "    },\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "    'normalize_std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "# Calculate effective batch size\n",
    "TRAINING_PARAMS['effective_batch_size'] = TRAINING_PARAMS['batch_size'] * TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(TRANSFORM_PARAMS['image_size']),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(TRANSFORM_PARAMS['rotation_degrees']),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=TRANSFORM_PARAMS['color_jitter']['brightness'],\n",
    "        contrast=TRANSFORM_PARAMS['color_jitter']['contrast'],\n",
    "        saturation=TRANSFORM_PARAMS['color_jitter']['saturation'],\n",
    "        hue=TRANSFORM_PARAMS['color_jitter']['hue']\n",
    "    ),\n",
    "    transforms.RandomAffine(degrees=TRANSFORM_PARAMS['rotation_degrees'], \n",
    "                          translate=(0.1, 0.1), \n",
    "                          scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=TRANSFORM_PARAMS['normalize_mean'],\n",
    "        std=TRANSFORM_PARAMS['normalize_std']\n",
    "    )\n",
    "])\n",
    "\n",
    "def manage_checkpoints(output_dir, epoch, model, optimizer, scheduler, metrics, device):\n",
    "    \"\"\"Save checkpoint and maintain maximum number of checkpoints\"\"\"\n",
    "    checkpoint_dir = output_dir / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Handle MPS device when saving model\n",
    "    if device.type == \"mps\":\n",
    "        model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        optimizer_state_dict = {k: v.cpu() if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in optimizer.state_dict().items()}\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "        optimizer_state_dict = optimizer.state_dict()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer_state_dict,\n",
    "        'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Manage number of checkpoints\n",
    "    checkpoints = sorted(checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "    if len(checkpoints) > TRAINING_PARAMS['max_checkpoints']:\n",
    "        oldest_checkpoint = checkpoints[0]\n",
    "        oldest_checkpoint.unlink()  # Delete oldest checkpoint\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "        \n",
    "        print(\"Building dataset...\")\n",
    "        for class_name, folder in tqdm(folders.items(), desc=\"Loading classes\"):\n",
    "            # Get all image files\n",
    "            class_images = [\n",
    "                os.path.join(folder, f) for f in os.listdir(folder)\n",
    "                if (f.lower().endswith(('.jpg', '.jpeg', '.png')) and\n",
    "                    not f.startswith('._') and\n",
    "                    not f.startswith('.DS_Store'))\n",
    "            ]\n",
    "            \n",
    "            # Limit images if specified\n",
    "            if max_images_per_class is not None and len(class_images) > max_images_per_class:\n",
    "                class_images = random.sample(class_images, max_images_per_class)\n",
    "            \n",
    "            print(f\"\\nFound {len(class_images)} images for {class_name}\")\n",
    "            \n",
    "            self.image_paths.extend(class_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Total images: {len(self.image_paths)}\")\n",
    "        for class_name in folders.keys():\n",
    "            class_count = self.labels.count(self.class_to_idx[class_name])\n",
    "            print(f\"{class_name}: {class_count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "def setup_logging(identifier):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler\n",
    "    log_dir = Path(DATA_PARAMS['output_dir']) / identifier / DATA_PARAMS['log_subdir']\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_handler = logging.FileHandler(log_dir / f'{identifier}.log')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "class LRSchedulerWrapper:\n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def step(self, metric=None):\n",
    "        self.scheduler.step(metric)\n",
    "        current_lr = self.scheduler.get_last_lr()[0]\n",
    "        return current_lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return self.scheduler.get_last_lr()\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return self.scheduler.state_dict()\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.scheduler.load_state_dict(state_dict)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model, train_loader, val_loader, device, logger, output_dir, class_names):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.logger = logger\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.viz_dir = self.output_dir / DATA_PARAMS['viz_subdir']\n",
    "        self.metrics_dir = self.output_dir / DATA_PARAMS['metrics_subdir']\n",
    "        self.viz_dir.mkdir(exist_ok=True)\n",
    "        self.metrics_dir.mkdir(exist_ok=True)\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def log_metrics(self, epoch, train_loss, val_loss, accuracy, lr):\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'learning_rate': lr\n",
    "        }\n",
    "        \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.accuracies.append(accuracy)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "            f\"Accuracy: {accuracy:.4f}, LR: {lr:.6f}\"\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def export_metrics_to_csv(self, epoch):\n",
    "        metrics_file = self.metrics_dir / 'training_metrics.csv'\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'accuracy': self.accuracies[-1],\n",
    "            'learning_rate': self.learning_rates[-1]\n",
    "        }\n",
    "        \n",
    "        mode = 'a' if metrics_file.exists() else 'w'\n",
    "        write_header = not metrics_file.exists()\n",
    "        \n",
    "        with open(metrics_file, mode, newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(metrics)\n",
    "    \n",
    "    def plot_learning_curves(self, epoch):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Val Loss')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.accuracies, label='Validation Accuracy')\n",
    "        plt.title('Accuracy Curve')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.learning_rates, label='Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'learning_curves_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_space(self, epoch):\n",
    "        features = []\n",
    "        labels = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in tqdm(self.val_loader, desc=\"Extracting features\", \n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                # Get features from the global average pooling layer\n",
    "                features_batch = self.model.avgpool(\n",
    "                    self.model.layer4(\n",
    "                        self.model.layer3(\n",
    "                            self.model.layer2(\n",
    "                                self.model.layer1(\n",
    "                                    self.model.maxpool(\n",
    "                                        self.model.relu(\n",
    "                                            self.model.bn1(\n",
    "                                                self.model.conv1(images)\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                features_batch = torch.flatten(features_batch, 1)\n",
    "                features.append(features_batch.cpu().numpy())\n",
    "                labels.extend(target.numpy())\n",
    "        \n",
    "        features = np.vstack(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Reduce dimensionality for visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        features_2d = tsne.fit_transform(features)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                            c=labels, cmap='tab10')\n",
    "        plt.colorbar(scatter, label='Classes', ticks=range(len(self.class_names)))\n",
    "        plt.title(f'Feature Space Visualization (t-SNE) - Epoch {epoch}')\n",
    "        plt.xlabel('t-SNE dimension 1')\n",
    "        plt.ylabel('t-SNE dimension 2')\n",
    "        \n",
    "        # Add legend\n",
    "        handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                            markerfacecolor=plt.cm.tab10(i / len(self.class_names)), \n",
    "                            label=name, markersize=8) \n",
    "                  for i, name in enumerate(self.class_names)]\n",
    "        plt.legend(handles=handles, title='Classes', bbox_to_anchor=(1.15, 1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'feature_space_epoch_{epoch}.png', \n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def export_confusion_matrix(self, epoch):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"Computing confusion matrix\",\n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_names,\n",
    "                   yticklabels=self.class_names)\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'confusion_matrix_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def export_class_metrics(self, epoch):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in tqdm(self.val_loader, desc=\"Computing class metrics\",\n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                labels.extend(target.numpy())\n",
    "        \n",
    "        report = classification_report(labels, predictions, \n",
    "                                    target_names=self.class_names, \n",
    "                                    output_dict=True)\n",
    "        \n",
    "        with open(self.metrics_dir / f'class_metrics_epoch_{epoch}.json', 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "    \n",
    "    def export_training_summary(self):\n",
    "        summary = {\n",
    "            'total_epochs': len(self.train_losses),\n",
    "            'best_accuracy': max(self.accuracies),\n",
    "            'final_accuracy': self.accuracies[-1],\n",
    "            'best_val_loss': min(self.val_losses),\n",
    "            'final_val_loss': self.val_losses[-1],\n",
    "            'training_duration': str(datetime.now() - self.start_time),\n",
    "            'learning_rate_progression': self.learning_rates,\n",
    "            'accuracy_progression': self.accuracies,\n",
    "            'val_loss_progression': self.val_losses,\n",
    "            'class_names': self.class_names\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'training_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "\n",
    "def train_final_model(dataset, class_names, identifier):\n",
    "    # Setup\n",
    "    logger = setup_logging(identifier)\n",
    "    output_dir = Path(DATA_PARAMS['output_dir']) / identifier\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Split dataset and create data loaders\n",
    "    train_size = int(TRAINING_PARAMS['train_val_split'] * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoader setup with single-process loading\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Force single process\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Force single process\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # Device setup with MPS support\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        use_mixed_precision = True\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using Apple Silicon (MPS) device\")\n",
    "        use_mixed_precision = False  # MPS doesn't support mixed precision yet\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU device\")\n",
    "        use_mixed_precision = False\n",
    "    \n",
    "    # Set up mixed precision training based on device\n",
    "    if use_mixed_precision:\n",
    "        scaler = GradScaler()\n",
    "        logger.info(\"Using mixed precision training\")\n",
    "    else:\n",
    "        scaler = None\n",
    "        logger.info(\"Mixed precision training not available for this device\")\n",
    "    \n",
    "    # Model setup\n",
    "    if TRAINING_PARAMS['model_type'] == 'ResNet18':\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    else:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, TRAINING_PARAMS['hidden_dim']),\n",
    "        nn.BatchNorm1d(TRAINING_PARAMS['hidden_dim']),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(TRAINING_PARAMS['dropout_rate']),\n",
    "        nn.Linear(TRAINING_PARAMS['hidden_dim'], len(class_names))\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = FocalLoss(gamma=TRAINING_PARAMS['focal_loss_gamma'])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=TRAINING_PARAMS['learning_rate'],\n",
    "        weight_decay=TRAINING_PARAMS['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = LRSchedulerWrapper(\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min',\n",
    "            factor=TRAINING_PARAMS['scheduler_factor'],\n",
    "            patience=TRAINING_PARAMS['scheduler_patience']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Initialize monitor\n",
    "    monitor = TrainingMonitor(model, train_loader, val_loader, device, \n",
    "                          logger, output_dir, class_names)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    steps_without_improvement = 0\n",
    "    total_epochs = TRAINING_PARAMS['num_epochs']\n",
    "    \n",
    "    try:\n",
    "        # Add overall progress bar\n",
    "        epoch_pbar = tqdm(range(total_epochs), \n",
    "                         desc=\"Overall Progress\",\n",
    "                         position=0,\n",
    "                         leave=True,\n",
    "                         dynamic_ncols=True)\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Training phase with nested progress bar\n",
    "            batch_pbar = tqdm(train_loader, \n",
    "                            desc=f\"Epoch {epoch+1}/{total_epochs}\",\n",
    "                            position=1,\n",
    "                            leave=False,\n",
    "                            dynamic_ncols=True)\n",
    "            \n",
    "            for i, (images, labels) in enumerate(batch_pbar):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Handle training step based on mixed precision availability\n",
    "                if use_mixed_precision:\n",
    "                    with autocast(device_type=device.type):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    loss = loss / TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (i + 1) % TRAINING_PARAMS['gradient_accumulation_steps'] == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % TRAINING_PARAMS['gradient_accumulation_steps'] == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                running_loss += loss.item() * TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "                # Empty cache periodically for MPS\n",
    "                if device.type == \"mps\" and (i + 1) % 50 == 0:\n",
    "                    if hasattr(torch.mps, 'empty_cache'):\n",
    "                        torch.mps.empty_cache()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(val_loader, \n",
    "                              desc=\"Validation\",\n",
    "                              position=1,\n",
    "                              leave=False,\n",
    "                              dynamic_ncols=True)\n",
    "                \n",
    "                for images, labels in val_pbar:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    val_pbar.set_postfix({'val_loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            accuracy = correct / total\n",
    "            current_lr = scheduler.step(val_loss)\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            epoch_pbar.set_postfix({\n",
    "                'Train Loss': f'{train_loss:.4f}',\n",
    "                'Val Loss': f'{val_loss:.4f}', \n",
    "                'Accuracy': f'{accuracy:.4f}',\n",
    "                'LR': f'{current_lr:.6f}'\n",
    "            })\n",
    "            \n",
    "            # Log and export metrics\n",
    "            metrics = monitor.log_metrics(epoch, train_loss, val_loss, accuracy, current_lr)\n",
    "            monitor.export_metrics_to_csv(epoch)\n",
    "            \n",
    "            # Generate visualizations on interval\n",
    "            if (epoch + 1) % TRAINING_PARAMS['visualization_interval'] == 0:\n",
    "                monitor.plot_learning_curves(epoch)\n",
    "                monitor.plot_feature_space(epoch)\n",
    "                monitor.export_confusion_matrix(epoch)\n",
    "                monitor.export_class_metrics(epoch)\n",
    "            \n",
    "            # Save checkpoint at interval\n",
    "            if (epoch + 1) % TRAINING_PARAMS['checkpoint_interval'] == 0:\n",
    "                manage_checkpoints(output_dir, epoch, model, optimizer, scheduler, metrics, device)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                steps_without_improvement = 0\n",
    "                \n",
    "                # Handle MPS device when saving model\n",
    "                if device.type == \"mps\":\n",
    "                    model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                    optimizer_state_dict = {k: v.cpu() if isinstance(v, torch.Tensor) else v \n",
    "                                         for k, v in optimizer.state_dict().items()}\n",
    "                else:\n",
    "                    model_state_dict = model.state_dict()\n",
    "                    optimizer_state_dict = optimizer.state_dict()\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model_state_dict,\n",
    "                    'optimizer_state_dict': optimizer_state_dict,\n",
    "                    'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    'metrics': metrics\n",
    "                }, output_dir / 'best_model.pth')\n",
    "            else:\n",
    "                steps_without_improvement += 1\n",
    "            \n",
    "            # Empty MPS cache after each epoch\n",
    "            if device.type == \"mps\" and hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if steps_without_improvement >= TRAINING_PARAMS['scheduler_patience'] * 2:\n",
    "                logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        # Export final training summary before exiting\n",
    "        monitor.export_training_summary()\n",
    "    \n",
    "    return model, monitor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset with optional image limit\n",
    "    dataset = CityDataset(\n",
    "        folders=DATA_PARAMS['data_folders'], \n",
    "        transform=train_transform,\n",
    "        max_images_per_class=TRAINING_PARAMS['max_images_per_class']\n",
    "    )\n",
    "    class_names = list(DATA_PARAMS['data_folders'].keys())\n",
    "\n",
    "    # Create identifier for this run\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    identifier = (f\"softmax-{TRAINING_PARAMS['model_type']}_\"\n",
    "                 f\"{TRAINING_PARAMS['num_epochs']}-ep_\"\n",
    "                 f\"{TRAINING_PARAMS['effective_batch_size']}-bs_\"\n",
    "                 f\"{current_time}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, monitor = train_final_model(dataset, class_names, identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Option B: Resnet Model Training / Fine tuning for better feature extraction without extended evaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "checkpoint_interval = 25\n",
    "max_images_per_class = 25000\n",
    "resnet_model = 'ResNet50'\n",
    "\n",
    "# Setup directories and paths\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"softmax-{resnet_model}_{num_epochs}-ep_{batch_size}-bs_{max_images_per_class}-images_{current_time}\"\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "folders = {\n",
    "    'Boston': '../data/ma-boston/buildings',\n",
    "    'Charlotte': '../data/nc-charlotte/buildings',\n",
    "    'Manhattan': '../data/ny-manhattan/buildings',\n",
    "    'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "}\n",
    "output_folder = os.path.join('softmax-output', identifier)\n",
    "checkpoint_dir = os.path.join(output_folder, 'checkpoints')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model_{identifier}.pth')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Dataset and model setup\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "num_classes = len(class_names)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=max_images_per_class):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "\n",
    "        print(\"Building dataset...\")\n",
    "        for class_name, folder in tqdm(folders.items(), desc=\"Loading classes\"):\n",
    "            # Filter out macOS system files and get only image files\n",
    "            class_images = [\n",
    "                os.path.join(folder, f) for f in os.listdir(folder) \n",
    "                if (f.lower().endswith(('.jpg', '.jpeg', '.png')) and \n",
    "                    not f.startswith('._') and \n",
    "                    not f.startswith('.DS_Store'))\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nFound {len(class_images)} images for {class_name}\")\n",
    "            \n",
    "            if len(class_images) > max_images_per_class:\n",
    "                class_images = random.sample(class_images, max_images_per_class)\n",
    "            \n",
    "            self.image_paths.extend(class_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Total images: {len(self.image_paths)}\")\n",
    "        for class_name in folders.keys():\n",
    "            class_count = self.labels.count(self.class_to_idx[class_name])\n",
    "            print(f\"{class_name}: {class_count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "# Enhanced transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nInitializing dataset...\")\n",
    "dataset = CityDataset(folders, transform=transform)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_final_model(dataset):\n",
    "    print(\"\\nSplitting dataset into train/val sets...\")\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nInitializing {resnet_model}...\")\n",
    "    if resnet_model == 'ResNet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif resnet_model == 'ResNet50':\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = FocalLoss(gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        per_class_correct = torch.zeros(num_classes)\n",
    "        per_class_total = torch.zeros(num_classes)\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", \n",
    "                         leave=False, position=1)\n",
    "        \n",
    "        for images, labels in batch_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels)\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            current_loss = loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(model(images), 1)\n",
    "                for label, pred in zip(labels, predicted):\n",
    "                    per_class_correct[label] += (label == pred).item()\n",
    "                    per_class_total[label] += 1\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", \n",
    "                       leave=False, position=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_val_loss = loss.item()\n",
    "                val_pbar.set_postfix({'val_loss': f'{current_val_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'accuracy': f'{accuracy:.4f}'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Complete:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        print(\"\\nPer-class accuracies:\")\n",
    "        for i in range(num_classes):\n",
    "            if per_class_total[i] > 0:\n",
    "                class_acc = per_class_correct[i] / per_class_total[i]\n",
    "                print(f\"{class_names[i]}: {class_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"\\nSaving best model with val_loss: {val_loss:.4f}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': accuracy\n",
    "            }, model_save_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting training...\")\n",
    "    final_model = train_final_model(dataset)\n",
    "    print(f\"\\nTraining complete! Model saved to {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
