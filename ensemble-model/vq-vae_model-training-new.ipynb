{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14 11:14:50,073 - INFO - Run identifier: vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14\n",
      "2024-11-14 11:14:50,203 - INFO - Using CUDA device: NVIDIA GeForce RTX 3070 Ti\n",
      "2024-11-14 11:14:50,204 - INFO - CUDA memory allocated: 0.00 MB\n",
      "2024-11-14 11:14:50,204 - INFO - Output directory: vae-output/vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import logging\n",
    "import sys\n",
    "import glob\n",
    "import re\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def setup_logging(identifier):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    log_dir = Path('vae-output') / identifier / 'logs'\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_handler = logging.FileHandler(log_dir / f'{identifier}.log')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "# Parameters\n",
    "total_samples = 40000\n",
    "batch_size = 16\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "commitment_cost = 0.25\n",
    "hidden_channels = 64\n",
    "embedding_dim = 32\n",
    "num_embeddings = 1024\n",
    "checkpoint_interval = 10\n",
    "image_size = (512, 512)\n",
    "normalize_mean = (0.5,)\n",
    "normalize_std = (0.5,)\n",
    "\n",
    "# Visualization parameters\n",
    "visualization_interval = 1\n",
    "reconstruction_interval = 50\n",
    "max_saved_checkpoints = 3\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"vq-vae_{batch_size}-batch_{total_samples}-samples_{embedding_dim}-{num_embeddings}-vector_{num_epochs}-epochs_{current_time}\"\n",
    "\n",
    "logger = setup_logging(identifier)\n",
    "logger.info(f\"Run identifier: {identifier}\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.cuda.empty_cache()\n",
    "        if hasattr(torch.cuda, 'memory.set_per_process_memory_fraction'):\n",
    "            torch.cuda.memory.set_per_process_memory_fraction(0.95)\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        logger.info(f\"CUDA memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
    "    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        torch.mps.empty_cache()\n",
    "        logger.info(\"Using MPS (Apple Silicon) device\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        logger.info(f\"Using CPU with {os.cpu_count()} threads\")\n",
    "    return device\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "# DataLoader configuration\n",
    "num_workers = 0 if device.type == 'mps' else min(os.cpu_count(), 8)\n",
    "pin_memory = device.type == 'cuda'\n",
    "\n",
    "output_dir = os.path.join('vae-output', identifier)\n",
    "dataset_dirs = [\n",
    "    '../data/ma-boston/parcels',\n",
    "    '../data/nc-charlotte/parcels', \n",
    "    '../data/ny-manhattan/parcels', \n",
    "    '../data/pa-pittsburgh/parcels'  \n",
    "]\n",
    "\n",
    "logger.info(f\"Output directory: {output_dir}\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "vis_dir = os.path.join(output_dir, 'visualizations')\n",
    "os.makedirs(vis_dir, exist_ok=True)\n",
    "\n",
    "class LRSchedulerWrapper:\n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "    def step(self, metric=None):\n",
    "        self.scheduler.step(metric)\n",
    "        current_lr = self.scheduler.get_last_lr()[0]\n",
    "        logger.info(f\"Learning rate adjusted to: {current_lr:.6f}\")\n",
    "    def get_last_lr(self):\n",
    "        return self.scheduler.get_last_lr()\n",
    "    def state_dict(self):\n",
    "        return self.scheduler.state_dict()\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.size())\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, num_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14 11:14:50,384 - INFO - Found 76605 valid image files\n",
      "2024-11-14 11:14:50,408 - INFO - Sampled 60000 images for training\n",
      "2024-11-14 11:14:50,410 - INFO - Split dataset into 54000 training and 6000 validation samples\n",
      "2024-11-14 11:14:50,411 - INFO - Initializing dataset with 54000 images\n",
      "2024-11-14 11:14:50,411 - INFO - Initializing dataset with 6000 images\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalize_mean, normalize_std)\n",
    "])\n",
    "\n",
    "def load_and_sample_images(dataset_dirs, total_samples):\n",
    "    all_image_paths = []\n",
    "    for dataset_dir in dataset_dirs:\n",
    "        for root, _, files in os.walk(dataset_dir):\n",
    "            for file in files:\n",
    "                if not file.startswith('.') and not file.startswith('._') and file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    all_image_paths.append(os.path.join(root, file))\n",
    "    \n",
    "    logger.info(f\"Found {len(all_image_paths)} valid image files\")\n",
    "    sampled_paths = random.sample(all_image_paths, total_samples)\n",
    "    logger.info(f\"Sampled {len(sampled_paths)} images for training\")\n",
    "    return sampled_paths\n",
    "\n",
    "class SampledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        logger.info(f\"Initializing dataset with {len(image_paths)} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, 0\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def create_data_loaders(sampled_image_paths, batch_size, transform, device):\n",
    "    num_train = int(0.9 * len(sampled_image_paths))\n",
    "    train_paths = sampled_image_paths[:num_train]\n",
    "    val_paths = sampled_image_paths[num_train:]\n",
    "    \n",
    "    logger.info(f\"Split dataset into {len(train_paths)} training and {len(val_paths)} validation samples\")\n",
    "    \n",
    "    train_dataset = SampledImageDataset(train_paths, transform)\n",
    "    val_dataset = SampledImageDataset(val_paths, transform)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=2 if num_workers > 0 else None,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=2 if num_workers > 0 else None,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "sampled_image_paths = load_and_sample_images(dataset_dirs, total_samples)\n",
    "train_loader, val_loader = create_data_loaders(sampled_image_paths, batch_size, transform, device)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, output_dir, num_embeddings):\n",
    "        self.output_dir = output_dir\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.metrics = {\n",
    "            'train_reconstruction_loss': [],\n",
    "            'train_vq_loss': [],\n",
    "            'train_kl_div': [],\n",
    "            'train_total_loss': [],\n",
    "            'val_reconstruction_loss': [],\n",
    "            'val_vq_loss': [],\n",
    "            'val_kl_div': [],\n",
    "            'val_total_loss': [],\n",
    "            'perplexity': [],\n",
    "            'encoding_usage': []\n",
    "        }\n",
    "        \n",
    "        self.visualization_dir = os.path.join(output_dir, 'visualizations')\n",
    "        self.reconstruction_dir = os.path.join(self.visualization_dir, 'reconstructions')\n",
    "        self.metrics_dir = os.path.join(self.visualization_dir, 'metrics')\n",
    "        self.embedding_dir = os.path.join(self.visualization_dir, 'embeddings')\n",
    "        \n",
    "        for directory in [self.visualization_dir, self.reconstruction_dir, \n",
    "                         self.metrics_dir, self.embedding_dir]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        self.log_path = os.path.join(output_dir, 'detailed_training_log.csv')\n",
    "        with open(self.log_path, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['epoch', 'batch', 'train_reconstruction_loss', 'train_vq_loss', \n",
    "                           'train_total_loss', 'train_kl_div', 'val_reconstruction_loss',\n",
    "                           'val_vq_loss', 'val_total_loss', 'val_kl_div', 'perplexity',\n",
    "                           'encoding_usage'])\n",
    "        logger.info(f\"Training monitor initialized. Logging to {self.log_path}\")\n",
    "    \n",
    "    def calculate_kl_divergence(self, encoding_indices):\n",
    "        encoding_hist = torch.histc(encoding_indices.float(), bins=self.num_embeddings, \n",
    "                                  min=0, max=self.num_embeddings-1)\n",
    "        actual_dist = encoding_hist / encoding_hist.sum()\n",
    "        uniform_dist = torch.ones_like(actual_dist) / self.num_embeddings\n",
    "        actual_dist = actual_dist + 1e-10\n",
    "        kl_div = torch.sum(actual_dist * torch.log(actual_dist / uniform_dist))\n",
    "        return kl_div.item()\n",
    "    \n",
    "    def update_metrics(self, recon_loss, vq_loss, encoding_indices, is_training=True):\n",
    "        total_loss = recon_loss + vq_loss\n",
    "        kl_div = self.calculate_kl_divergence(encoding_indices)\n",
    "        \n",
    "        prefix = 'train_' if is_training else 'val_'\n",
    "        self.metrics[f'{prefix}reconstruction_loss'].append(recon_loss)\n",
    "        self.metrics[f'{prefix}vq_loss'].append(vq_loss)\n",
    "        self.metrics[f'{prefix}total_loss'].append(total_loss)\n",
    "        self.metrics[f'{prefix}kl_div'].append(kl_div)\n",
    "        \n",
    "        if is_training:\n",
    "            encoding_hist = torch.histc(encoding_indices.float(), bins=self.num_embeddings, \n",
    "                                      min=0, max=self.num_embeddings-1)\n",
    "            prob = encoding_hist / encoding_hist.sum()\n",
    "            prob = prob[prob > 0]\n",
    "            perplexity = torch.exp(-torch.sum(prob * torch.log(prob)))\n",
    "            \n",
    "            used_encodings = torch.unique(encoding_indices).size(0)\n",
    "            encoding_usage = used_encodings / self.num_embeddings * 100\n",
    "            \n",
    "            self.metrics['perplexity'].append(perplexity.item())\n",
    "            self.metrics['encoding_usage'].append(encoding_usage)\n",
    "    \n",
    "    def should_visualize(self, epoch):\n",
    "        return epoch % visualization_interval == 0 or epoch == 0\n",
    "\n",
    "    def should_save_reconstruction(self, batch_idx):\n",
    "        return batch_idx % reconstruction_interval == 0\n",
    "\n",
    "    def clean_old_checkpoints(self, output_dir, current_epoch):\n",
    "        checkpoints = glob.glob(os.path.join(output_dir, 'checkpoint_epoch_*.pth'))\n",
    "        if len(checkpoints) > max_saved_checkpoints:\n",
    "            checkpoints.sort(key=lambda x: int(re.search(r'epoch_(\\d+)', x).group(1)))\n",
    "            for checkpoint in checkpoints[:-max_saved_checkpoints]:\n",
    "                try:\n",
    "                    os.remove(checkpoint)\n",
    "                    logger.info(f\"Removed old checkpoint: {checkpoint}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to remove checkpoint {checkpoint}: {str(e)}\")\n",
    "\n",
    "    def plot_training_curves(self, epoch):\n",
    "        if not self.should_visualize(epoch):\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(20, 15))\n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.metrics['train_reconstruction_loss'], label='Train')\n",
    "        plt.plot(self.metrics['val_reconstruction_loss'], label='Validation')\n",
    "        plt.title('Reconstruction Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.metrics['train_total_loss'], label='Train')\n",
    "        plt.plot(self.metrics['val_total_loss'], label='Validation')\n",
    "        plt.title('Total Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.plot(self.metrics['train_kl_div'], label='Train')\n",
    "        plt.plot(self.metrics['val_kl_div'], label='Validation')\n",
    "        plt.title('KL Divergence')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('KL Divergence')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.plot(self.metrics['perplexity'], label='Perplexity')\n",
    "        plt.plot(self.metrics['encoding_usage'], label='Codebook Usage %')\n",
    "        plt.title('Codebook Metrics')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        save_path = os.path.join(self.metrics_dir, f'training_metrics_epoch_{epoch}.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved training curves for epoch {epoch}\")\n",
    "\n",
    "    def visualize_reconstructions(self, original_images, reconstructed_images, epoch, batch_idx):\n",
    "        if not self.should_save_reconstruction(batch_idx):\n",
    "            return\n",
    "\n",
    "        original_images = (original_images + 1) / 2\n",
    "        reconstructed_images = (reconstructed_images + 1) / 2\n",
    "        \n",
    "        n = min(4, original_images.size(0))\n",
    "        comparison = torch.cat([\n",
    "            original_images[:n],\n",
    "            reconstructed_images[:n]\n",
    "        ])\n",
    "        \n",
    "        grid = make_grid(comparison, nrow=n)\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Original (top) vs Reconstructed (bottom) - Epoch {epoch}')\n",
    "        save_path = os.path.join(self.reconstruction_dir, f'reconstruction_e{epoch}.png')\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    def visualize_embedding_space(self, model, epoch):\n",
    "        if not self.should_visualize(epoch):\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Generating t-SNE visualization for epoch {epoch}\")\n",
    "        embeddings = model.vq_layer.embedding.weight.detach().cpu().numpy()\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(embeddings)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                            c=range(len(embeddings_2d)), cmap='viridis', alpha=0.6)\n",
    "        plt.colorbar(scatter, label='Embedding Index')\n",
    "        plt.title(f't-SNE Visualization of Embedding Space - Epoch {epoch}')\n",
    "        save_path = os.path.join(self.embedding_dir, f'embedding_space_epoch_{epoch}.png')\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "\n",
    "    def save_final_visualizations(self, epoch):\n",
    "        for dir_name, final_name in [\n",
    "            (self.metrics_dir, 'final_metrics.png'),\n",
    "            (self.embedding_dir, 'final_embedding_space.png'),\n",
    "            (self.reconstruction_dir, 'final_reconstruction.png')\n",
    "        ]:\n",
    "            files = sorted(glob.glob(os.path.join(dir_name, '*')))\n",
    "            if files:\n",
    "                latest_file = files[-1]\n",
    "                final_path = os.path.join(self.visualization_dir, final_name)\n",
    "                shutil.copy2(latest_file, final_path)\n",
    "                logger.info(f\"Saved final visualization: {final_name}\")\n",
    "            \n",
    "    def log_epoch_metrics(self, epoch, train_metrics, val_metrics):\n",
    "        logger.info(f\"Epoch {epoch} metrics:\")\n",
    "        logger.info(f\"Train - Recon: {train_metrics['recon']:.4f}, \"\n",
    "                   f\"VQ: {train_metrics['vq']:.4f}, \"\n",
    "                   f\"Total: {train_metrics['total']:.4f}, \"\n",
    "                   f\"KL: {train_metrics['kl']:.4f}\")\n",
    "        logger.info(f\"Val   - Recon: {val_metrics['recon']:.4f}, \"\n",
    "                   f\"VQ: {val_metrics['vq']:.4f}, \"\n",
    "                   f\"Total: {val_metrics['total']:.4f}, \"\n",
    "                   f\"KL: {val_metrics['kl']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device, monitor):\n",
    "    model.eval()\n",
    "    val_recon_loss = 0\n",
    "    val_vq_loss = 0\n",
    "    val_total_loss = 0\n",
    "    val_kl_div = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            reconstructed, vq_loss = model(images)\n",
    "            recon_loss = criterion(reconstructed, images)\n",
    "            total_loss = recon_loss + vq_loss\n",
    "            \n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                encoded = model.module.encoder(images)\n",
    "                _, _, encoding_indices = model.module.vq_layer(encoded)\n",
    "            else:\n",
    "                encoded = model.encoder(images)\n",
    "                _, _, encoding_indices = model.vq_layer(encoded)\n",
    "            \n",
    "            monitor.update_metrics(recon_loss.item(), vq_loss.item(), encoding_indices, is_training=False)\n",
    "            \n",
    "            val_recon_loss += recon_loss.item()\n",
    "            val_vq_loss += vq_loss.item()\n",
    "            val_total_loss += total_loss.item()\n",
    "            val_kl_div += monitor.calculate_kl_divergence(encoding_indices)\n",
    "            num_batches += 1\n",
    "    \n",
    "    val_metrics = {\n",
    "        'recon': val_recon_loss / num_batches,\n",
    "        'vq': val_vq_loss / num_batches,\n",
    "        'total': val_total_loss / num_batches,\n",
    "        'kl': val_kl_div / num_batches\n",
    "    }\n",
    "    \n",
    "    return val_metrics\n",
    "\n",
    "def train_epoch(epoch, model, train_loader, val_loader, optimizer, criterion, device, monitor, scaler=None):\n",
    "    model.train()\n",
    "    train_recon_loss = 0\n",
    "    train_vq_loss = 0\n",
    "    train_total_loss = 0\n",
    "    train_kl_div = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "    elif device.type == 'mps':\n",
    "        torch.mps.empty_cache()\n",
    "    \n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    progress_bar.set_description(f\"Epoch [{epoch+1}/{num_epochs}] - {identifier}\")\n",
    "    \n",
    "    for batch_idx, (images, _) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                reconstructed, vq_loss = model(images)\n",
    "                recon_loss = criterion(reconstructed, images)\n",
    "                total_loss = recon_loss + vq_loss\n",
    "                \n",
    "                encoded = model.module.encoder(images) if isinstance(model, torch.nn.DataParallel) else model.encoder(images)\n",
    "                _, _, encoding_indices = model.module.vq_layer(encoded) if isinstance(model, torch.nn.DataParallel) else model.vq_layer(encoded)\n",
    "            \n",
    "            scaler.scale(total_loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            reconstructed, vq_loss = model(images)\n",
    "            recon_loss = criterion(reconstructed, images)\n",
    "            total_loss = recon_loss + vq_loss\n",
    "            \n",
    "            encoded = model.encoder(images)\n",
    "            _, _, encoding_indices = model.vq_layer(encoded)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        monitor.update_metrics(recon_loss.item(), vq_loss.item(), encoding_indices, is_training=True)\n",
    "        \n",
    "        train_recon_loss += recon_loss.item()\n",
    "        train_vq_loss += vq_loss.item()\n",
    "        train_total_loss += total_loss.item()\n",
    "        train_kl_div += monitor.calculate_kl_divergence(encoding_indices)\n",
    "        num_batches += 1\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f\"{total_loss.item():.4f}\",\n",
    "            'Recon': f\"{recon_loss.item():.4f}\",\n",
    "            'VQ': f\"{vq_loss.item():.4f}\"\n",
    "        })\n",
    "        \n",
    "        if monitor.should_save_reconstruction(batch_idx):\n",
    "            monitor.visualize_reconstructions(images, reconstructed, epoch + 1, batch_idx)\n",
    "    \n",
    "    train_metrics = {\n",
    "        'recon': train_recon_loss / num_batches,\n",
    "        'vq': train_vq_loss / num_batches,\n",
    "        'total': train_total_loss / num_batches,\n",
    "        'kl': train_kl_div / num_batches\n",
    "    }\n",
    "    \n",
    "    val_metrics = validate(model, val_loader, criterion, device, monitor)\n",
    "    monitor.log_epoch_metrics(epoch + 1, train_metrics, val_metrics)\n",
    "    \n",
    "    return train_metrics, val_metrics\n",
    "\n",
    "def slerp(p0, p1, t):\n",
    "    if t == 0:\n",
    "        return p0\n",
    "    if t == 1:\n",
    "        return p1\n",
    "        \n",
    "    dot = torch.sum(p0 * p1) / (torch.norm(p0) * torch.norm(p1))\n",
    "    dot = torch.clamp(dot, -1.0, 1.0)\n",
    "    \n",
    "    omega = torch.acos(dot)\n",
    "    \n",
    "    if omega == 0:\n",
    "        return (1.0 - t) * p0 + t * p1\n",
    "        \n",
    "    so = torch.sin(omega)\n",
    "    return torch.sin((1.0 - t) * omega) / so * p0 + torch.sin(t * omega) / so * p1\n",
    "\n",
    "def create_slerp_interpolation(model, output_path, transform, device):\n",
    "    logger.info(\"Starting SLERP interpolation generation\")\n",
    "    logger.info(f\"Output path: {output_path}\")\n",
    "    \n",
    "    image_paths = [\n",
    "        \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "        \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "        \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "        \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "    ]\n",
    "    weights = [0.25, 0.35, 0.15, 0.25]\n",
    "    \n",
    "    logger.info(\"City weights:\")\n",
    "    for city, weight in zip(['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh'], weights):\n",
    "        logger.info(f\"  {city}: {weight:.2f}\")\n",
    "    \n",
    "    assert abs(sum(weights) - 1.0) < 1e-6, \"Weights must sum to 1\"\n",
    "    \n",
    "    images = []\n",
    "    logger.info(\"Loading input images:\")\n",
    "    for path in image_paths:\n",
    "        logger.info(f\"  Loading {os.path.basename(path)}\")\n",
    "        try:\n",
    "            img = Image.open(path).convert('RGB')\n",
    "            img_tensor = transform(img)\n",
    "            images.append(img_tensor)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {path}: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        latents = []\n",
    "        for img in images:\n",
    "            img = img.unsqueeze(0).to(device)\n",
    "            if isinstance(model, torch.nn.DataParallel):\n",
    "                encoded = model.module.encoder(img)\n",
    "                quantized, _, _ = model.module.vq_layer(encoded)\n",
    "            else:\n",
    "                encoded = model.encoder(img)\n",
    "                quantized, _, _ = model.vq_layer(encoded)\n",
    "            latents.append(quantized.view(1, -1))\n",
    "        \n",
    "        weight_pairs = [(0, 1), (2, 3)]\n",
    "        pair_weights = [weights[0] + weights[1], weights[2] + weights[3]]\n",
    "        \n",
    "        pair_results = []\n",
    "        for (idx1, idx2), pair_weight in zip(weight_pairs, pair_weights):\n",
    "            w1 = weights[idx1] / pair_weight\n",
    "            w2 = weights[idx2] / pair_weight\n",
    "            pair_result = slerp(latents[idx1], latents[idx2], w2)\n",
    "            pair_results.append((pair_result, pair_weight))\n",
    "        \n",
    "        total_weight = sum(w for _, w in pair_results)\n",
    "        relative_weight = pair_results[1][1] / total_weight\n",
    "        final_latent = slerp(pair_results[0][0], pair_results[1][0], relative_weight)\n",
    "        \n",
    "        original_shape = quantized.shape\n",
    "        final_latent = final_latent.view(original_shape)\n",
    "        if isinstance(model, torch.nn.DataParallel):\n",
    "            final_image = model.module.decoder(final_latent)\n",
    "        else:\n",
    "            final_image = model.decoder(final_latent)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    img = final_image[0].cpu().numpy().transpose(1, 2, 0)\n",
    "    img = (img + 1) / 2\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(output_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved interpolated image to {output_path}\")\n",
    "    \n",
    "    orig_dir = os.path.join(os.path.dirname(output_path), 'originals_slerp')\n",
    "    os.makedirs(orig_dir, exist_ok=True)\n",
    "    \n",
    "    cities = ['boston', 'charlotte', 'manhattan', 'pittsburgh']\n",
    "    for img_tensor, city, weight in zip(images, cities, weights):\n",
    "        img = img_tensor.numpy().transpose(1, 2, 0)\n",
    "        img = (img + 1) / 2\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f'{city} (weight: {weight:.2f})')\n",
    "        plt.axis('off')\n",
    "        save_path = os.path.join(orig_dir, f'original_{city}.png')\n",
    "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0, dpi=300)\n",
    "        plt.close()\n",
    "        logger.info(f\"Saved original image for {city} to {save_path}\")\n",
    "\n",
    "    logger.info(\"SLERP interpolation generation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14 11:14:50,602 - INFO - Model using CUDA with 1 GPU(s)\n",
      "2024-11-14 11:14:50,603 - INFO - AMP (Automatic Mixed Precision) enabled\n",
      "2024-11-14 11:14:50,604 - INFO - Training monitor initialized. Logging to vae-output/vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14/detailed_training_log.csv\n",
      "2024-11-14 11:14:50,605 - INFO - Training parameters saved to vae-output/vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14/vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14_training_params.json\n",
      "2024-11-14 11:14:50,606 - INFO - Starting training...\n",
      "2024-11-14 11:14:50,606 - INFO - Training parameters: {'identifier': 'vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14', 'total_samples': 60000, 'batch_size': 16, 'num_epochs': 100, 'learning_rate': 0.001, 'commitment_cost': 0.25, 'hidden_channels': 64, 'embedding_dim': 32, 'num_embeddings': 1024, 'checkpoint_interval': 10, 'image_size': (512, 512), 'device': 'cuda', 'num_workers': 8, 'pin_memory': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31700603b9d74c989fbc617e508c826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-14 11:16:47,753 - ERROR - Error loading image ../data/nc-charlotte/parcels/parcels_19487.jpg: image file is truncated (58 bytes not processed)\n",
      "2024-11-14 11:16:48,678 - ERROR - Training interrupted: Caught OSError in DataLoader worker process 2.\n",
      "Original Traceback (most recent call last):\n",
      "  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n",
      "    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_792347/1501792018.py\", line 33, in __getitem__\n",
      "    image = Image.open(image_path).convert('RGB')\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/Image.py\", line 995, in convert\n",
      "    self.load()\n",
      "  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/ImageFile.py\", line 290, in load\n",
      "    raise OSError(msg)\n",
      "OSError: image file is truncated (58 bytes not processed)\n",
      "\n",
      "2024-11-14 11:16:48,681 - INFO - Saved final visualization: final_reconstruction.png\n",
      "2024-11-14 11:16:48,686 - INFO - Saved final model to vae-output/vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14/final_model_vq-vae_16-batch_60000-samples_32-1024-vector_100-epochs_2024-11-14_11-14.pth\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Caught OSError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_792347/1501792018.py\", line 33, in __getitem__\n    image = Image.open(image_path).convert('RGB')\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/Image.py\", line 995, in convert\n    self.load()\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/ImageFile.py\", line 290, in load\n    raise OSError(msg)\nOSError: image file is truncated (58 bytes not processed)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 58\u001b[0m         train_metrics, val_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m         scheduler\u001b[38;5;241m.\u001b[39mstep(val_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[3], line 57\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, model, train_loader, val_loader, optimizer, criterion, device, monitor, scaler)\u001b[0m\n\u001b[1;32m     54\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[1;32m     55\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mset_to_none\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    249\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1324\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1323\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rcvd_idx)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m-> 1324\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1327\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1370\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1370\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOSError\u001b[0m: Caught OSError in DataLoader worker process 2.\nOriginal Traceback (most recent call last):\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 309, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n            ~~~~~~~~~~~~^^^^^\n  File \"/tmp/ipykernel_792347/1501792018.py\", line 33, in __getitem__\n    image = Image.open(image_path).convert('RGB')\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/Image.py\", line 995, in convert\n    self.load()\n  File \"/home/ls/sites/re-blocking/.venv/lib/python3.12/site-packages/PIL/ImageFile.py\", line 290, in load\n    raise OSError(msg)\nOSError: image file is truncated (58 bytes not processed)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model, optimizer, criterion and device-specific optimizations\n",
    "model = VQVAE(in_channels=3, hidden_channels=hidden_channels, num_embeddings=num_embeddings,\n",
    "              embedding_dim=embedding_dim, commitment_cost=commitment_cost).to(device)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    model = torch.nn.DataParallel(model) if torch.cuda.device_count() > 1 else model\n",
    "    scaler = torch.amp.GradScaler()\n",
    "    model = model.cuda()\n",
    "    logger.info(f\"Model using CUDA with {torch.cuda.device_count()} GPU(s)\")\n",
    "    logger.info(\"AMP (Automatic Mixed Precision) enabled\")\n",
    "elif device.type == 'mps':\n",
    "    scaler = None\n",
    "    logger.info(\"Model using MPS (Apple Silicon)\")\n",
    "else:\n",
    "    scaler = None\n",
    "    logger.info(\"Model using CPU\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "base_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=5\n",
    ")\n",
    "\n",
    "scheduler = LRSchedulerWrapper(base_scheduler)\n",
    "monitor = TrainingMonitor(output_dir, num_embeddings)\n",
    "\n",
    "training_params = {\n",
    "    \"identifier\": identifier,\n",
    "    \"total_samples\": total_samples,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"commitment_cost\": commitment_cost,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_embeddings\": num_embeddings,\n",
    "    \"checkpoint_interval\": checkpoint_interval,\n",
    "    \"image_size\": image_size,\n",
    "    \"device\": device.type,\n",
    "    \"num_workers\": num_workers,\n",
    "    \"pin_memory\": pin_memory\n",
    "}\n",
    "\n",
    "params_path = os.path.join(output_dir, f'{identifier}_training_params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(training_params, f, indent=4)\n",
    "logger.info(f\"Training parameters saved to {params_path}\")\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "logger.info(f\"Training parameters: {training_params}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics, val_metrics = train_epoch(\n",
    "            epoch, model, train_loader, val_loader, optimizer, criterion, \n",
    "            device, monitor, scaler if device.type == 'cuda' else None\n",
    "        )\n",
    "        \n",
    "        scheduler.step(val_metrics['total'])\n",
    "        \n",
    "        if device.type == 'cuda':\n",
    "            memory_allocated = torch.cuda.memory_allocated() / 1024**2\n",
    "            memory_cached = torch.cuda.memory_reserved() / 1024**2\n",
    "            logger.info(f\"GPU memory allocated: {memory_allocated:.2f} MB\")\n",
    "            logger.info(f\"GPU memory cached: {memory_cached:.2f} MB\")\n",
    "            torch.cuda.empty_cache()\n",
    "        elif device.type == 'mps':\n",
    "            torch.mps.empty_cache()\n",
    "        \n",
    "        if monitor.should_visualize(epoch):\n",
    "            monitor.plot_training_curves(epoch + 1)\n",
    "            monitor.visualize_embedding_space(model, epoch + 1)\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "            checkpoint_data = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "                'train_metrics': train_metrics,\n",
    "                'val_metrics': val_metrics,\n",
    "                'scaler': scaler.state_dict() if device.type == 'cuda' else None\n",
    "            }\n",
    "            torch.save(checkpoint_data, checkpoint_path)\n",
    "            monitor.clean_old_checkpoints(output_dir, epoch + 1)\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training interrupted: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    monitor.save_final_visualizations(num_epochs)\n",
    "    final_model_path = os.path.join(output_dir, f'final_model_{identifier}.pth')\n",
    "    \n",
    "    try:\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "            'final_train_metrics': train_metrics if 'train_metrics' in locals() else None,\n",
    "            'final_val_metrics': val_metrics if 'val_metrics' in locals() else None,\n",
    "            'training_params': training_params,\n",
    "            'scaler': scaler.state_dict() if device.type == 'cuda' else None\n",
    "        }, final_model_path)\n",
    "        logger.info(f\"Saved final model to {final_model_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving final model: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Generating SLERP interpolation...\")\n",
    "    output_path = os.path.join(output_dir, 'slerp_interpolation.png')\n",
    "    create_slerp_interpolation(model, output_path, transform, device)\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error generating SLERP interpolation: {str(e)}\")\n",
    "\n",
    "logger.info(\"\\nTraining Summary:\")\n",
    "logger.info(f\"Total epochs completed: {num_epochs}\")\n",
    "logger.info(f\"Final learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "logger.info(f\"Output directory: {output_dir}\")\n",
    "if device.type == 'cuda':\n",
    "    logger.info(f\"Final GPU memory usage: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "logger.info(\"Training completed successfully\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
