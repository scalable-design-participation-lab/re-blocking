{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPImageProcessor\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "inference_steps = 15000  # increase for better image quality\n",
    "identifier = f\"stable-diffusion-weighted-interpolation_{inference_steps}-inference\"  # Unique identifier for this run\n",
    "output_dir = os.path.join(\"diffusion-output\", identifier)\n",
    "output_image_size = (512, 512)  # width and height of output image\n",
    "max_image_dimension = 1024  # Maximum dimension for resizing images\n",
    "\n",
    "# Example softmax weights (these should sum to 1)\n",
    "weights = [0.25, 0.35, 0.15, 0.25]  # Replace with your actual weights\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8 for the VAE)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float16)  # Ensure consistent data type\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "try:\n",
    "    # Load the pre-trained Stable Diffusion model with DDIM scheduler\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)  # Use mixed precision\n",
    "        pipe = pipe.to(device)\n",
    "        if device.type == \"cuda\":\n",
    "            pipe.enable_attention_slicing()\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    sizes = []\n",
    "    original_sizes = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Processed sizes: {sizes}\")\n",
    "    print(f\"Original sizes: {original_sizes}\")\n",
    "\n",
    "    # Encode images to latents\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "\n",
    "    # Perform weighted interpolation between latents\n",
    "    print(\"Performing weighted interpolation...\")\n",
    "    weighted_latent = sum(w * latent for w, latent in zip(weights, latents))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Scale the latent\n",
    "        weighted_latent = 1 / 0.18215 * weighted_latent\n",
    "        # Decode the weighted latent\n",
    "        decoded_image = pipe.vae.decode(weighted_latent).sample\n",
    "        # Normalize the decoded image\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        # Convert to CPU and then to numpy array\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    # Resize the output image to the desired output size\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "\n",
    "    # Save the weighted interpolated image\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, \"weighted_interpolated_image.png\")\n",
    "    decoded_image_pil.save(output_path, quality=95)\n",
    "\n",
    "    print(f\"Weighted interpolation image saved at: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
