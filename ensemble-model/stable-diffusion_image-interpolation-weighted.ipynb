{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stable Diffusion Image Interpolation (Weighted based on Classifier Predictions) for the Building-to-Parcel Workflow\n",
    "# Leonard Schrage, l.schrage@northeastern.edu / lschrage@mit.edu, 2024-25\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Optional: Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
    "\n",
    "# Paths and configuration\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "model_outputs = {\n",
    "    \"Boston\": \"/home/ls/sites/re-blocking/data/results/ma-boston-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Charlotte\": \"/home/ls/sites/re-blocking/data/results/nc-charlotte-200-150-v100/test_latest/images\",\n",
    "    \"Manhattan\": \"/home/ls/sites/re-blocking/data/results/ny-manhattan-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Pittsburgh\": \"/home/ls/sites/re-blocking/data/results/pa-pittsburgh-p2p-500-150-v100/test_latest/images\"\n",
    "}\n",
    "predictions_file = \"softmax-output/city-predictions/run_20250319_131844/brooklyn/predictions.json\"\n",
    "output_base_dir = \"ensemble-output/stable-diffusion\"\n",
    "inference_steps = 300\n",
    "output_image_size = (512, 512)\n",
    "max_image_dimension = 512\n",
    "output_format = \"jpg\"  # Options: \"png\" or \"jpg\"\n",
    "image_quality = 90  # For JPG (ignored for PNG)\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8 for the VAE)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float16)\n",
    "    \n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for latents.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low_2d + \\\n",
    "          (torch.sin(val * omega) / so).unsqueeze(1) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "def generate_weighted_image(pipe, image_latents, weights, output_path):\n",
    "    \"\"\"Generate a weighted image based on the specified weights.\"\"\"\n",
    "    # Start with the first latent\n",
    "    weighted_latent = image_latents[0].clone()\n",
    "    \n",
    "    # Apply SLERP interpolation with weights\n",
    "    for i in range(1, len(image_latents)):\n",
    "        weighted_latent = slerp(weights[i], weighted_latent, image_latents[i])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Scale the latent\n",
    "        weighted_latent = 1 / 0.18215 * weighted_latent\n",
    "        # Decode the weighted latent\n",
    "        decoded_image = pipe.vae.decode(weighted_latent).sample\n",
    "        # Normalize the decoded image\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        # Convert to CPU and then to numpy array\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    # Resize the output image to the desired output size\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "    \n",
    "    # Save the weighted interpolated image\n",
    "    if output_format == \"jpg\":\n",
    "        decoded_image_pil.save(output_path, \"JPEG\", quality=image_quality)\n",
    "    else:\n",
    "        decoded_image_pil.save(output_path, \"PNG\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def extract_image_number(filename):\n",
    "    \"\"\"Extract the image number from a filename.\"\"\"\n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        r'_(\\d+)_',           # Matches _000123_\n",
    "        r'parcels_(\\d+)',      # Matches parcels_000123\n",
    "        r'(\\d+)\\.jpg',         # Matches 000123.jpg\n",
    "        r'(\\d+)\\.png'          # Matches 000123.png\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1).zfill(6)  # Ensure 6 digits with leading zeros\n",
    "    \n",
    "    # If no pattern matches, return the filename itself for debugging\n",
    "    print(f\"Could not extract number from {filename}\")\n",
    "    return None\n",
    "\n",
    "def create_mapping(predictions):\n",
    "    \"\"\"Create a mapping from image numbers to their weights.\"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    # Print a few examples of the filenames to help debug\n",
    "    if predictions and len(predictions) > 0:\n",
    "        print(\"Example filenames in predictions:\")\n",
    "        for i in range(min(5, len(predictions))):\n",
    "            print(f\"  {Path(predictions[i]['image_path']).name}\")\n",
    "    \n",
    "    for pred in predictions:\n",
    "        image_path = pred[\"image_path\"]\n",
    "        filename = Path(image_path).name\n",
    "        image_num = extract_image_number(filename)\n",
    "        \n",
    "        if image_num:\n",
    "            mapping[image_num] = {\n",
    "                \"Boston\": pred[\"probabilities\"][\"Boston\"],\n",
    "                \"Charlotte\": pred[\"probabilities\"][\"Charlotte\"],\n",
    "                \"Manhattan\": pred[\"probabilities\"][\"Manhattan\"],\n",
    "                \"Pittsburgh\": pred[\"probabilities\"][\"Pittsburgh\"]\n",
    "            }\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Load predictions\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(predictions)} image predictions\")\n",
    "    \n",
    "    # Create mapping from image numbers to weights\n",
    "    number_to_weights = create_mapping(predictions)\n",
    "    print(f\"Created mapping for {len(number_to_weights)} images\")\n",
    "    \n",
    "    # Load the pre-trained Stable Diffusion model with DDIM scheduler\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "        pipe.tokenizer.clean_up_tokenization_spaces = True\n",
    "        pipe = pipe.to(device)\n",
    "        if device.type == \"cuda\":\n",
    "            pipe.enable_attention_slicing()\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Get a list of all image numbers by checking one model directory\n",
    "    sample_model_dir = list(model_outputs.values())[0]\n",
    "    image_numbers = []\n",
    "    \n",
    "    for file in os.listdir(sample_model_dir):\n",
    "        if \"_fake_B\" in file:\n",
    "            image_num = extract_image_number(file)\n",
    "            if image_num:\n",
    "                image_numbers.append(image_num)\n",
    "    \n",
    "    image_numbers.sort()\n",
    "    print(f\"Found {len(image_numbers)} image numbers to process\")\n",
    "    \n",
    "    # Process each image number\n",
    "    with tqdm(total=len(image_numbers), desc=\"Processing images\") as pbar:\n",
    "        for image_num in image_numbers:\n",
    "            try:\n",
    "                # Skip if no weights found for this image\n",
    "                if image_num not in number_to_weights:\n",
    "                    print(f\"No weights found for image {image_num}, skipping\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Get weights for this image\n",
    "                image_weights = number_to_weights[image_num]\n",
    "                \n",
    "                # Load and encode all model outputs for this image\n",
    "                image_latents = []\n",
    "                cities_in_order = [\"Boston\", \"Charlotte\", \"Manhattan\", \"Pittsburgh\"]\n",
    "                weights_in_order = []\n",
    "                \n",
    "                for city in cities_in_order:\n",
    "                    # Find the fake image file\n",
    "                    model_dir = model_outputs[city]\n",
    "                    fake_image_path = None\n",
    "                    \n",
    "                    for file in os.listdir(model_dir):\n",
    "                        if f\"_{image_num}_fake_B\" in file:\n",
    "                            fake_image_path = os.path.join(model_dir, file)\n",
    "                            break\n",
    "                    \n",
    "                    if fake_image_path:\n",
    "                        # Load and preprocess the image\n",
    "                        image_tensor, _, _ = load_and_preprocess_image(fake_image_path)\n",
    "                        \n",
    "                        # Encode to latent\n",
    "                        with torch.no_grad():\n",
    "                            latent = pipe.vae.encode(image_tensor).latent_dist.sample() * 0.18215\n",
    "                            image_latents.append(latent)\n",
    "                            weights_in_order.append(image_weights[city])\n",
    "                    else:\n",
    "                        print(f\"Warning: Could not find fake image for {city}, image number {image_num}\")\n",
    "                \n",
    "                # Skip if we don't have all model outputs\n",
    "                if len(image_latents) < len(cities_in_order):\n",
    "                    print(f\"Missing some model outputs for image {image_num}, skipping\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Generate the ensemble image\n",
    "                output_filename = f\"ensemble_stable-diffusion_{image_num}.{output_format}\"\n",
    "                output_path = os.path.join(output_base_dir, output_filename)\n",
    "                \n",
    "                generate_weighted_image(pipe, image_latents, weights_in_order, output_path)\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_num}: {str(e)}\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "    \n",
    "    print(f\"All images processed. Results saved to {output_base_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
