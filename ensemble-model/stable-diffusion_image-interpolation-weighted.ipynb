{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Stable Diffusion model:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n",
      "Fetching 14 files: 100%|██████████| 14/14 [00:00<00:00, 56.13it/s]\n",
      "/Users/ls/.pyenv/versions/3.12.7/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:14<00:00,  2.00s/it]\n",
      "Loading Stable Diffusion model: 100%|██████████| 1/1 [00:15<00:00, 15.82s/it]\n",
      "Loading and preprocessing images: 100%|██████████| 4/4 [00:00<00:00, 10.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed sizes: []\n",
      "Original sizes: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding images to latent representations: 100%|██████████| 4/4 [00:03<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of latents: [torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 32, 32]), torch.Size([1, 4, 32, 32])]\n",
      "Performing weighted SLERP interpolation...\n",
      "Weighted SLERP interpolation image saved at: diffusion-output/stable-diffusion-weighted-interpolation_15000-inference/weighted_slerp_interpolated_image.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Parameters\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "inference_steps = 15000  # increase for better image quality\n",
    "identifier = f\"stable-diffusion-weighted-interpolation_{inference_steps}-inference\"  # Unique identifier for this run\n",
    "output_dir = os.path.join(\"diffusion-output\", identifier)\n",
    "output_image_size = (512, 512)  # width and height of output image\n",
    "max_image_dimension = 1024  # Maximum dimension for resizing images\n",
    "\n",
    "# Example softmax weights (these should sum to 1)\n",
    "weights = [0.25, 0.35, 0.15, 0.25]  # Replace with your actual weights\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8 for the VAE)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float16)  # Ensure consistent data type\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for latents.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low_2d + \\\n",
    "          (torch.sin(val * omega) / so).unsqueeze(1) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "try:\n",
    "    # Load the pre-trained Stable Diffusion model with DDIM scheduler\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)  # Use mixed precision\n",
    "        pipe = pipe.to(device)\n",
    "        if device.type == \"cuda\":\n",
    "            pipe.enable_attention_slicing()\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    sizes = []\n",
    "    original_sizes = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Processed sizes: {sizes}\")\n",
    "    print(f\"Original sizes: {original_sizes}\")\n",
    "\n",
    "    # Encode images to latents\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "\n",
    "    # Perform SLERP interpolation between latents based on weights\n",
    "    print(\"Performing weighted SLERP interpolation...\")\n",
    "    weighted_latent = latents[0]\n",
    "    for i in range(1, len(latents)):\n",
    "        weighted_latent = slerp(weights[i], weighted_latent, latents[i])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Scale the latent\n",
    "        weighted_latent = 1 / 0.18215 * weighted_latent\n",
    "        # Decode the weighted latent\n",
    "        decoded_image = pipe.vae.decode(weighted_latent).sample\n",
    "        # Normalize the decoded image\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        # Convert to CPU and then to numpy array\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    # Resize the output image to the desired output size\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "\n",
    "    # Save the weighted interpolated image\n",
    "    output_path = os.path.join(output_dir, \"weighted_slerp_interpolated_image.png\")\n",
    "    decoded_image_pil.save(output_path, quality=95)\n",
    "\n",
    "    print(f\"Weighted SLERP interpolation image saved at: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
