{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Run identifier: vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59\n",
      "INFO:__main__:Output directory: vae-output/vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59\n",
      "INFO:__main__:Found 76605 valid image files\n",
      "INFO:__main__:Sampled 5000 images for training\n",
      "INFO:__main__:Successfully loaded 5000 valid images\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:AMP enabled: True\n",
      "INFO:__main__:Training parameters saved to vae-output/vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59/vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59_training_params.json\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e004f7e57b444b86a064ccfc618952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59:   0%|          | 0/100 [00:0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Final model saved to vae-output/vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59/models/vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59_final.pth\n",
      "INFO:__main__:Training completed for vq-vae_256-batch_5000-samples_128-2048-vector_5-epochs_2024-11-13_22-59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Parameters\n",
    "total_samples = 5000  # start with low for testing, should be on +50k\n",
    "batch_size = 256\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "commitment_cost = 0.25\n",
    "hidden_channels = 128\n",
    "embedding_dim = 128\n",
    "num_embeddings = 2048\n",
    "checkpoint_interval = 50\n",
    "image_size = (16, 16)  # Resize to a fixed size\n",
    "normalize_mean = (0.5,)\n",
    "normalize_std = (0.5,)\n",
    "\n",
    "# Create identifier with current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"vq-vae_{batch_size}-batch_{total_samples}-samples_{embedding_dim}-{num_embeddings}-vector_{num_epochs}-epochs_{current_time}\"\n",
    "logger.info(f\"Run identifier: {identifier}\")\n",
    "\n",
    "# Directories\n",
    "output_dir = os.path.join('vae-output', identifier)  # Each run gets its own directory\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "logger.info(f\"Output directory: {output_dir}\")\n",
    "\n",
    "dataset_dirs = [\n",
    "    '../data/ma-boston/parcels',\n",
    "    '../data/nc-charlotte/parcels', \n",
    "    '../data/ny-manhattan/parcels', \n",
    "    '../data/pa-pittsburgh/parcels'  \n",
    "]\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalize_mean, normalize_std)\n",
    "])\n",
    "\n",
    "# Collect all image paths with improved file handling\n",
    "all_image_paths = []\n",
    "for dataset_dir in dataset_dirs:\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            # Skip hidden files and system files\n",
    "            if file.startswith('.') or file.startswith('_'):\n",
    "                continue\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "logger.info(f\"Found {len(all_image_paths)} valid image files\")\n",
    "\n",
    "# Randomly sample the images from the collected paths\n",
    "sampled_image_paths = random.sample(all_image_paths, min(total_samples, len(all_image_paths)))\n",
    "logger.info(f\"Sampled {len(sampled_image_paths)} images for training\")\n",
    "\n",
    "# Custom dataset with improved error handling\n",
    "class SampledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Verify images during initialization\n",
    "        for img_path in image_paths:\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    # Verify the image can be loaded and converted to RGB\n",
    "                    img.convert('RGB')\n",
    "                self.image_paths.append(img_path)\n",
    "            except (IOError, OSError, Image.UnidentifiedImageError) as e:\n",
    "                logger.warning(f\"Skipping corrupted or unreadable image {img_path}: {str(e)}\")\n",
    "        \n",
    "        logger.info(f\"Successfully loaded {len(self.image_paths)} valid images\")\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "                \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, 0  # Returning 0 as a placeholder label\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            # Return a blank image instead of failing\n",
    "            blank_image = Image.new('RGB', image_size, 'black')\n",
    "            if self.transform:\n",
    "                blank_image = self.transform(blank_image)\n",
    "            return blank_image, 0\n",
    "\n",
    "# Create a dataset and dataloader for the sampled images\n",
    "sampled_dataset = SampledImageDataset(sampled_image_paths, transform=transform)\n",
    "dataloader = DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VQ-VAE Model Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.size())\n",
    "\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, num_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "# Set device and initialize AMP\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    use_amp = False  # MPS doesn't support AMP yet\n",
    "    amp_device_type = 'cpu'\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    use_amp = True\n",
    "    amp_device_type = 'cuda'\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    use_amp = False\n",
    "    amp_device_type = 'cpu'\n",
    "\n",
    "logger.info(f\"Using device: {device}\")\n",
    "logger.info(f\"AMP enabled: {use_amp}\")\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = torch.amp.GradScaler(enabled=use_amp) if use_amp else None\n",
    "\n",
    "# Initialize model, optimizer, and criterion\n",
    "model = VQVAE(in_channels=3, hidden_channels=hidden_channels, num_embeddings=num_embeddings,\n",
    "              embedding_dim=embedding_dim, commitment_cost=commitment_cost).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Save training parameters\n",
    "training_params = {\n",
    "    \"identifier\": identifier,\n",
    "    \"total_samples\": total_samples,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"commitment_cost\": commitment_cost,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_embeddings\": num_embeddings,\n",
    "    \"checkpoint_interval\": checkpoint_interval,\n",
    "    \"image_size\": image_size,\n",
    "    \"normalize_mean\": normalize_mean,\n",
    "    \"normalize_std\": normalize_std,\n",
    "    \"use_amp\": use_amp,\n",
    "    \"device\": str(device),\n",
    "    \"dataset_dirs\": dataset_dirs\n",
    "}\n",
    "\n",
    "params_path = os.path.join(output_dir, f'{identifier}_training_params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(training_params, f)\n",
    "logger.info(f\"Training parameters saved to {params_path}\")\n",
    "\n",
    "# Initialize CSV file for logging\n",
    "log_path = os.path.join(output_dir, f'{identifier}_training_log.csv')\n",
    "with open(log_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['epoch', 'loss', 'recon_loss', 'vq_loss']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Training Loop\n",
    "total_iterations = num_epochs * len(dataloader)\n",
    "progress_bar = tqdm(total=total_iterations, desc=f\"Training {identifier}\")\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_recon_loss = 0.0\n",
    "        running_vq_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, _) in enumerate(dataloader):\n",
    "            try:\n",
    "                images = images.to(device)\n",
    "                \n",
    "                # Use appropriate autocast based on device\n",
    "                with torch.amp.autocast(device_type=amp_device_type, enabled=use_amp):\n",
    "                    reconstructed, vq_loss = model(images)\n",
    "                    recon_loss = criterion(reconstructed, images)\n",
    "                    loss = recon_loss + vq_loss\n",
    "\n",
    "                # Optimization step with or without AMP\n",
    "                if use_amp and scaler is not None:\n",
    "                    scaler.scale(loss).backward()\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                # Update running losses\n",
    "                running_loss += loss.item()\n",
    "                running_recon_loss += recon_loss.item()\n",
    "                running_vq_loss += vq_loss.item()\n",
    "                \n",
    "                progress_bar.update(1)\n",
    "                progress_bar.set_description(\n",
    "                    f\"Epoch [{epoch + 1}/{num_epochs}] - {identifier}\"\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        avg_recon_loss = running_recon_loss / len(dataloader)\n",
    "        avg_vq_loss = running_vq_loss / len(dataloader)\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{avg_loss:.4f}',\n",
    "            'Recon': f'{avg_recon_loss:.4f}',\n",
    "            'VQ': f'{avg_vq_loss:.4f}'\n",
    "        })\n",
    "\n",
    "        # Log the losses\n",
    "        with open(log_path, 'a', newline='') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "            writer.writerow({\n",
    "                'epoch': epoch + 1,\n",
    "                'loss': avg_loss,\n",
    "                'recon_loss': avg_recon_loss,\n",
    "                'vq_loss': avg_vq_loss\n",
    "            })\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(output_dir, f'{identifier}_checkpoint_epoch_{epoch + 1}.pth')\n",
    "            checkpoint_dict = {\n",
    "                'identifier': identifier,\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_loss,\n",
    "                'recon_loss': avg_recon_loss,\n",
    "                'vq_loss': avg_vq_loss\n",
    "            }\n",
    "            if use_amp and scaler is not None:\n",
    "                checkpoint_dict['scaler_state_dict'] = scaler.state_dict()\n",
    "                \n",
    "            torch.save(checkpoint_dict, checkpoint_path)\n",
    "            logger.info(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.info(f\"Training interrupted by user for {identifier}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training error for {identifier}: {str(e)}\")\n",
    "finally:\n",
    "    progress_bar.close()\n",
    "    \n",
    "    # Save the final model\n",
    "    try:\n",
    "        model_output_dir = os.path.join(output_dir, 'models')\n",
    "        os.makedirs(model_output_dir, exist_ok=True)\n",
    "        model_save_path = os.path.join(model_output_dir, f\"{identifier}_final.pth\")\n",
    "        torch.save({\n",
    "            'identifier': identifier,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'training_params': training_params,\n",
    "            'final_loss': avg_loss,\n",
    "            'final_recon_loss': avg_recon_loss,\n",
    "            'final_vq_loss': avg_vq_loss\n",
    "        }, model_save_path)\n",
    "        logger.info(f\"Final model saved to {model_save_path}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving final model for {identifier}: {str(e)}\")\n",
    "\n",
    "logger.info(f\"Training completed for {identifier}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
