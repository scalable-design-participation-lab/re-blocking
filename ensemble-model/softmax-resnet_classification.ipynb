{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing dataset...\n",
      "Building dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a428127db3c4c88ac222daff1eb7e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading classes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 24995 images for Boston\n",
      "\n",
      "Found 24995 images for Charlotte\n",
      "\n",
      "Found 25064 images for Manhattan\n",
      "\n",
      "Found 24998 images for Pittsburgh\n",
      "\n",
      "Dataset statistics:\n",
      "Total images: 99988\n",
      "Boston: 24995 images\n",
      "Charlotte: 24995 images\n",
      "Manhattan: 25000 images\n",
      "Pittsburgh: 24998 images\n",
      "\n",
      "Using device: cuda\n",
      "\n",
      "Starting training...\n",
      "\n",
      "Splitting dataset into train/val sets...\n",
      "Training set size: 79990\n",
      "Validation set size: 19998\n",
      "\n",
      "Initializing ResNet50...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1684644937fc422e9b65c76a30cc53da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ce35af15e44eea837adfe2ed0393b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bdcba570394c5eba3ab2d4b0b5555a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 Complete:\n",
      "Train Loss: 0.1708\n",
      "Val Loss: 0.0210\n",
      "Val Accuracy: 0.9939\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6105\n",
      "Charlotte: 0.6184\n",
      "Manhattan: 0.6192\n",
      "Pittsburgh: 0.6202\n",
      "\n",
      "Saving best model with val_loss: 0.0210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6762b405a25b46d3b4ec79cf9976e4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75acfd2c6d404679b947da6c41be4917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 Complete:\n",
      "Train Loss: 0.1143\n",
      "Val Loss: 0.0168\n",
      "Val Accuracy: 0.9944\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6388\n",
      "Charlotte: 0.6433\n",
      "Manhattan: 0.6367\n",
      "Pittsburgh: 0.6431\n",
      "\n",
      "Saving best model with val_loss: 0.0168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3594bcfa9e81473fab84c786d59f5968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8730323ca855469bb01864ff1ab21629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 Complete:\n",
      "Train Loss: 0.1228\n",
      "Val Loss: 0.0074\n",
      "Val Accuracy: 0.9963\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6381\n",
      "Charlotte: 0.6386\n",
      "Manhattan: 0.6378\n",
      "Pittsburgh: 0.6393\n",
      "\n",
      "Saving best model with val_loss: 0.0074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e59de4d2d84c46e4a9548e2bf1608062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c475dc7a7cfc4a1a9c134ae6d0f7c41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 Complete:\n",
      "Train Loss: 0.1085\n",
      "Val Loss: 0.0103\n",
      "Val Accuracy: 0.9948\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6445\n",
      "Charlotte: 0.6430\n",
      "Manhattan: 0.6480\n",
      "Pittsburgh: 0.6437\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38de2e792114d2ba09e203bbf683d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658dd3bfe14842e59dc610e447805076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 Complete:\n",
      "Train Loss: 0.1177\n",
      "Val Loss: 0.0092\n",
      "Val Accuracy: 0.9953\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6145\n",
      "Charlotte: 0.6215\n",
      "Manhattan: 0.6204\n",
      "Pittsburgh: 0.6203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87127788eaa244899d8ae7cc59faab30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9af3013153b4312a5daec55fc6bd5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 Complete:\n",
      "Train Loss: 0.1041\n",
      "Val Loss: 0.0184\n",
      "Val Accuracy: 0.9937\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6488\n",
      "Charlotte: 0.6505\n",
      "Manhattan: 0.6562\n",
      "Pittsburgh: 0.6511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f51a5ed240747d6babe1ecd398c9596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b239a299c2e44dc18bc9e3ef9af0ea3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 Complete:\n",
      "Train Loss: 0.0942\n",
      "Val Loss: 0.0208\n",
      "Val Accuracy: 0.9881\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6386\n",
      "Charlotte: 0.6414\n",
      "Manhattan: 0.6415\n",
      "Pittsburgh: 0.6407\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc2225bfca34353bc1496a40e793d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93957a42f0046cdb3de2107f05a0231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 Complete:\n",
      "Train Loss: 0.0949\n",
      "Val Loss: 0.0095\n",
      "Val Accuracy: 0.9947\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6316\n",
      "Charlotte: 0.6333\n",
      "Manhattan: 0.6365\n",
      "Pittsburgh: 0.6385\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ec681340a54cb3ae703cecaae3b8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a3faf25ac64a88b7bfa4b18415b77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 Complete:\n",
      "Train Loss: 0.0992\n",
      "Val Loss: 0.0067\n",
      "Val Accuracy: 0.9977\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6426\n",
      "Charlotte: 0.6381\n",
      "Manhattan: 0.6471\n",
      "Pittsburgh: 0.6433\n",
      "\n",
      "Saving best model with val_loss: 0.0067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0b8425bb74431c90f2c89e601182c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10:   0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0da3d360e74551a798456316ca73a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 Complete:\n",
      "Train Loss: 0.0863\n",
      "Val Loss: 0.0337\n",
      "Val Accuracy: 0.9912\n",
      "\n",
      "Per-class accuracies:\n",
      "Boston: 0.6316\n",
      "Charlotte: 0.6292\n",
      "Manhattan: 0.6316\n",
      "Pittsburgh: 0.6360\n",
      "\n",
      "Training complete! Model saved to softmax-output/softmax-ResNet50_10-ep_32-bs_25000-images_2024-12-05_16-10/trained-model_softmax-ResNet50_10-ep_32-bs_25000-images_2024-12-05_16-10.pth\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Resnet Model Training / Fine tuning for better feature extraction\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "checkpoint_interval = 25\n",
    "max_images_per_class = 25000\n",
    "resnet_model = 'ResNet50'\n",
    "\n",
    "# Setup directories and paths\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"softmax-{resnet_model}_{num_epochs}-ep_{batch_size}-bs_{max_images_per_class}-images_{current_time}\"\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "folders = {\n",
    "    'Boston': '../data/ma-boston/buildings',\n",
    "    'Charlotte': '../data/nc-charlotte/buildings',\n",
    "    'Manhattan': '../data/ny-manhattan/buildings',\n",
    "    'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "}\n",
    "output_folder = os.path.join('softmax-output', identifier)\n",
    "checkpoint_dir = os.path.join(output_folder, 'checkpoints')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model_{identifier}.pth')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Dataset and model setup\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "num_classes = len(class_names)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=max_images_per_class):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "\n",
    "        print(\"Building dataset...\")\n",
    "        for class_name, folder in tqdm(folders.items(), desc=\"Loading classes\"):\n",
    "            # Filter out macOS system files and get only image files\n",
    "            class_images = [\n",
    "                os.path.join(folder, f) for f in os.listdir(folder) \n",
    "                if (f.lower().endswith(('.jpg', '.jpeg', '.png')) and \n",
    "                    not f.startswith('._') and \n",
    "                    not f.startswith('.DS_Store'))\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nFound {len(class_images)} images for {class_name}\")\n",
    "            \n",
    "            if len(class_images) > max_images_per_class:\n",
    "                class_images = random.sample(class_images, max_images_per_class)\n",
    "            \n",
    "            self.image_paths.extend(class_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Total images: {len(self.image_paths)}\")\n",
    "        for class_name in folders.keys():\n",
    "            class_count = self.labels.count(self.class_to_idx[class_name])\n",
    "            print(f\"{class_name}: {class_count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "# Enhanced transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nInitializing dataset...\")\n",
    "dataset = CityDataset(folders, transform=transform)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_final_model(dataset):\n",
    "    print(\"\\nSplitting dataset into train/val sets...\")\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nInitializing {resnet_model}...\")\n",
    "    if resnet_model == 'ResNet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif resnet_model == 'ResNet50':\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = FocalLoss(gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        per_class_correct = torch.zeros(num_classes)\n",
    "        per_class_total = torch.zeros(num_classes)\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", \n",
    "                         leave=False, position=1)\n",
    "        \n",
    "        for images, labels in batch_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels)\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            current_loss = loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(model(images), 1)\n",
    "                for label, pred in zip(labels, predicted):\n",
    "                    per_class_correct[label] += (label == pred).item()\n",
    "                    per_class_total[label] += 1\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", \n",
    "                       leave=False, position=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_val_loss = loss.item()\n",
    "                val_pbar.set_postfix({'val_loss': f'{current_val_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'accuracy': f'{accuracy:.4f}'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Complete:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        print(\"\\nPer-class accuracies:\")\n",
    "        for i in range(num_classes):\n",
    "            if per_class_total[i] > 0:\n",
    "                class_acc = per_class_correct[i] / per_class_total[i]\n",
    "                print(f\"{class_names[i]}: {class_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"\\nSaving best model with val_loss: {val_loss:.4f}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': accuracy\n",
    "            }, model_save_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting training...\")\n",
    "    final_model = train_final_model(dataset)\n",
    "    print(f\"\\nTraining complete! Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_33651/204315565.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for ../data/ny-brooklyn/buildings/buildings_1370.jpg:\n",
      "Predicted class: Manhattan\n",
      "Class probabilities:\n",
      "Boston: 0.1877\n",
      "Charlotte: 0.1211\n",
      "Manhattan: 0.6134\n",
      "Pittsburgh: 0.0778\n",
      "\n",
      "Predictions for ../data/ny-brooklyn/buildings/buildings_152277.jpg:\n",
      "Predicted class: Manhattan\n",
      "Class probabilities:\n",
      "Boston: 0.2418\n",
      "Charlotte: 0.1805\n",
      "Manhattan: 0.4724\n",
      "Pittsburgh: 0.1053\n",
      "\n",
      "Predictions saved to softmax-output/resnet-softmax-predictions-test.json\n"
     ]
    }
   ],
   "source": [
    "# Part 2: Softmax classifier predictions with Test Time Augmentation using the fine tuned resnet model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "resnet_model = 'ResNet50'\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Transform for prediction (no augmentation)\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model with the improved architecture\"\"\"\n",
    "    if resnet_model == 'ResNet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif resnet_model == 'ResNet50':\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "    \n",
    "    # Use the same improved classifier head as in training\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "def predict_single_pass(model, image_tensor):\n",
    "    \"\"\"Make a single prediction pass\"\"\"\n",
    "    outputs = model(image_tensor)\n",
    "    # Temperature scaling for sharper predictions\n",
    "    temperature = 1.5\n",
    "    scaled_outputs = outputs / temperature\n",
    "    probabilities = F.softmax(scaled_outputs, dim=1)[0]\n",
    "    return probabilities\n",
    "\n",
    "def predict_with_tta(model, image_path, num_augmentations=5):\n",
    "    \"\"\"Predict with Test Time Augmentation\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            # Base prediction\n",
    "            base_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            base_pred = predict_single_pass(model, base_tensor)\n",
    "            predictions.append(base_pred)\n",
    "            \n",
    "            # TTA predictions\n",
    "            tta_transforms = [\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.1),\n",
    "                transforms.RandomAffine(5, translate=(0.05, 0.05)),\n",
    "            ]\n",
    "            \n",
    "            for _ in range(num_augmentations):\n",
    "                aug_tensor = base_tensor.clone()\n",
    "                for t in random.sample(tta_transforms, 2):  # Apply 2 random transforms\n",
    "                    aug_tensor = t(aug_tensor)\n",
    "                aug_pred = predict_single_pass(model, aug_tensor)\n",
    "                predictions.append(aug_pred)\n",
    "            \n",
    "            # Average all predictions\n",
    "            final_pred = torch.mean(torch.stack(predictions), dim=0)\n",
    "            predicted_class = torch.argmax(final_pred).item()\n",
    "            \n",
    "            return final_pred.cpu().numpy(), predicted_class\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image {image_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_batch(model_path, image_paths, output_file=None):\n",
    "    \"\"\"Predict cities for multiple images using TTA\"\"\"\n",
    "    # Load model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    results = []\n",
    "    for image_path in image_paths:\n",
    "        probabilities, predicted_class = predict_with_tta(model, image_path)\n",
    "        \n",
    "        if probabilities is not None:\n",
    "            result = {\n",
    "                'image_path': image_path,\n",
    "                'predicted_class': class_names[predicted_class],\n",
    "                'probabilities': {\n",
    "                    class_name: float(prob) \n",
    "                    for class_name, prob in zip(class_names, probabilities)\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nPredictions for {image_path}:\")\n",
    "            print(f\"Predicted class: {class_names[predicted_class]}\")\n",
    "            print(\"Class probabilities:\")\n",
    "            for class_name, prob in zip(class_names, probabilities):\n",
    "                print(f\"{class_name}: {prob:.4f}\")\n",
    "    \n",
    "    # Save results if output file specified\n",
    "    if output_file and results:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"\\nPredictions saved to {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace these with your actual paths\n",
    "    model_path = \"models/softmax-ResNet50_10-ep_32-bs_25000-images_2024-12-05_16-10.pth\"\n",
    "    image_paths = [\n",
    "        \"../data/ny-brooklyn/buildings/buildings_1370.jpg\",\n",
    "        \"../data/ny-brooklyn/buildings/buildings_152277.jpg\"\n",
    "    ]\n",
    "    output_file = \"softmax-output/resnet-softmax-predictions-test.json\"\n",
    "    \n",
    "    results = predict_batch(model_path, image_paths, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
