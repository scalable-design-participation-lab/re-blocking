{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1, Option A: Resnet Model Training / Fine tuning for better feature extraction with extended evaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import logging\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "import random\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Directory and Data Settings\n",
    "DATA_PARAMS = {\n",
    "    'data_folders': {\n",
    "        'Boston': '../data/ma-boston/buildings',\n",
    "        'Charlotte': '../data/nc-charlotte/buildings',\n",
    "        'Manhattan': '../data/ny-manhattan/buildings',\n",
    "        'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "    },\n",
    "    'output_dir': 'softmax-output',\n",
    "    'model_subdir': 'models',\n",
    "    'log_subdir': 'logs',\n",
    "    'viz_subdir': 'visualizations',\n",
    "    'metrics_subdir': 'metrics'\n",
    "}\n",
    "\n",
    "# Training Parameters\n",
    "TRAINING_PARAMS = {\n",
    "    # Data parameters\n",
    "    'batch_size': 32,\n",
    "    'gradient_accumulation_steps': 4,\n",
    "    'train_val_split': 0.8,\n",
    "    'num_workers': 0,  # Set to 0 to avoid multiprocessing issues\n",
    "    'max_images_per_class': None,  # Set to None for all images, or a number for limit\n",
    "    \n",
    "    # Model parameters\n",
    "    'model_type': 'ResNet50',  # Options: 'ResNet18', 'ResNet50'\n",
    "    'hidden_dim': 512,\n",
    "    'dropout_rate': 0.3,\n",
    "    \n",
    "    # Training parameters\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-5,\n",
    "    'focal_loss_gamma': 2.0,\n",
    "    \n",
    "    # Scheduler parameters\n",
    "    'scheduler_factor': 0.5,\n",
    "    'scheduler_patience': 5,\n",
    "    \n",
    "    # Monitoring parameters\n",
    "    'visualization_interval': 5,  # Export plots every N epochs\n",
    "    'checkpoint_interval': 10,    # Save checkpoint every N epochs\n",
    "    'max_checkpoints': 3,        # Maximum number of checkpoints to keep\n",
    "}\n",
    "\n",
    "# Image Transform Parameters\n",
    "TRANSFORM_PARAMS = {\n",
    "    'image_size': (224, 224),\n",
    "    'rotation_degrees': 15,\n",
    "    'color_jitter': {\n",
    "        'brightness': 0.2,\n",
    "        'contrast': 0.2,\n",
    "        'saturation': 0.2,\n",
    "        'hue': 0.1\n",
    "    },\n",
    "    'normalize_mean': [0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "    'normalize_std': [0.229, 0.224, 0.225]\n",
    "}\n",
    "\n",
    "# Calculate effective batch size\n",
    "TRAINING_PARAMS['effective_batch_size'] = TRAINING_PARAMS['batch_size'] * TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "\n",
    "# Create transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(TRANSFORM_PARAMS['image_size']),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(TRANSFORM_PARAMS['rotation_degrees']),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=TRANSFORM_PARAMS['color_jitter']['brightness'],\n",
    "        contrast=TRANSFORM_PARAMS['color_jitter']['contrast'],\n",
    "        saturation=TRANSFORM_PARAMS['color_jitter']['saturation'],\n",
    "        hue=TRANSFORM_PARAMS['color_jitter']['hue']\n",
    "    ),\n",
    "    transforms.RandomAffine(degrees=TRANSFORM_PARAMS['rotation_degrees'], \n",
    "                          translate=(0.1, 0.1), \n",
    "                          scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=TRANSFORM_PARAMS['normalize_mean'],\n",
    "        std=TRANSFORM_PARAMS['normalize_std']\n",
    "    )\n",
    "])\n",
    "\n",
    "def manage_checkpoints(output_dir, epoch, model, optimizer, scheduler, metrics, device):\n",
    "    \"\"\"Save checkpoint and maintain maximum number of checkpoints\"\"\"\n",
    "    checkpoint_dir = output_dir / 'checkpoints'\n",
    "    checkpoint_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Handle MPS device when saving model\n",
    "    if device.type == \"mps\":\n",
    "        model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        optimizer_state_dict = {k: v.cpu() if isinstance(v, torch.Tensor) else v \n",
    "                             for k, v in optimizer.state_dict().items()}\n",
    "    else:\n",
    "        model_state_dict = model.state_dict()\n",
    "        optimizer_state_dict = optimizer.state_dict()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch+1}.pth'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model_state_dict,\n",
    "        'optimizer_state_dict': optimizer_state_dict,\n",
    "        'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "        'metrics': metrics\n",
    "    }, checkpoint_path)\n",
    "    \n",
    "    # Manage number of checkpoints\n",
    "    checkpoints = sorted(checkpoint_dir.glob('checkpoint_epoch_*.pth'))\n",
    "    if len(checkpoints) > TRAINING_PARAMS['max_checkpoints']:\n",
    "        oldest_checkpoint = checkpoints[0]\n",
    "        oldest_checkpoint.unlink()  # Delete oldest checkpoint\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "        \n",
    "        print(\"Building dataset...\")\n",
    "        for class_name, folder in tqdm(folders.items(), desc=\"Loading classes\"):\n",
    "            # Get all image files\n",
    "            class_images = [\n",
    "                os.path.join(folder, f) for f in os.listdir(folder)\n",
    "                if (f.lower().endswith(('.jpg', '.jpeg', '.png')) and\n",
    "                    not f.startswith('._') and\n",
    "                    not f.startswith('.DS_Store'))\n",
    "            ]\n",
    "            \n",
    "            # Limit images if specified\n",
    "            if max_images_per_class is not None and len(class_images) > max_images_per_class:\n",
    "                class_images = random.sample(class_images, max_images_per_class)\n",
    "            \n",
    "            print(f\"\\nFound {len(class_images)} images for {class_name}\")\n",
    "            \n",
    "            self.image_paths.extend(class_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Total images: {len(self.image_paths)}\")\n",
    "        for class_name in folders.keys():\n",
    "            class_count = self.labels.count(self.class_to_idx[class_name])\n",
    "            print(f\"{class_name}: {class_count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "def setup_logging(identifier):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Clear any existing handlers\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    \n",
    "    logger.setLevel(logging.INFO)\n",
    "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    \n",
    "    # Console handler\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    logger.addHandler(console_handler)\n",
    "    \n",
    "    # File handler\n",
    "    log_dir = Path(DATA_PARAMS['output_dir']) / identifier / DATA_PARAMS['log_subdir']\n",
    "    log_dir.mkdir(parents=True, exist_ok=True)\n",
    "    file_handler = logging.FileHandler(log_dir / f'{identifier}.log')\n",
    "    file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "class LRSchedulerWrapper:\n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def step(self, metric=None):\n",
    "        self.scheduler.step(metric)\n",
    "        current_lr = self.scheduler.get_last_lr()[0]\n",
    "        return current_lr\n",
    "    \n",
    "    def get_last_lr(self):\n",
    "        return self.scheduler.get_last_lr()\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return self.scheduler.state_dict()\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.scheduler.load_state_dict(state_dict)\n",
    "\n",
    "class TrainingMonitor:\n",
    "    def __init__(self, model, train_loader, val_loader, device, logger, output_dir, class_names):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "        self.logger = logger\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.viz_dir = self.output_dir / DATA_PARAMS['viz_subdir']\n",
    "        self.metrics_dir = self.output_dir / DATA_PARAMS['metrics_subdir']\n",
    "        self.viz_dir.mkdir(exist_ok=True)\n",
    "        self.metrics_dir.mkdir(exist_ok=True)\n",
    "        self.class_names = class_names\n",
    "        \n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.start_time = datetime.now()\n",
    "    \n",
    "    def log_metrics(self, epoch, train_loss, val_loss, accuracy, lr):\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'accuracy': accuracy,\n",
    "            'learning_rate': lr\n",
    "        }\n",
    "        \n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.accuracies.append(accuracy)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        self.logger.info(\n",
    "            f\"Epoch {epoch + 1} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "            f\"Accuracy: {accuracy:.4f}, LR: {lr:.6f}\"\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def export_metrics_to_csv(self, epoch):\n",
    "        metrics_file = self.metrics_dir / 'training_metrics.csv'\n",
    "        metrics = {\n",
    "            'epoch': epoch,\n",
    "            'train_loss': self.train_losses[-1],\n",
    "            'val_loss': self.val_losses[-1],\n",
    "            'accuracy': self.accuracies[-1],\n",
    "            'learning_rate': self.learning_rates[-1]\n",
    "        }\n",
    "        \n",
    "        mode = 'a' if metrics_file.exists() else 'w'\n",
    "        write_header = not metrics_file.exists()\n",
    "        \n",
    "        with open(metrics_file, mode, newline='') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=metrics.keys())\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "            writer.writerow(metrics)\n",
    "    \n",
    "    def plot_learning_curves(self, epoch):\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses, label='Train Loss')\n",
    "        plt.plot(self.val_losses, label='Val Loss')\n",
    "        plt.title('Loss Curves')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.accuracies, label='Validation Accuracy')\n",
    "        plt.title('Accuracy Curve')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.learning_rates, label='Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.yscale('log')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'learning_curves_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def plot_feature_space(self, epoch):\n",
    "        features = []\n",
    "        labels = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in tqdm(self.val_loader, desc=\"Extracting features\", \n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                # Get features from the global average pooling layer\n",
    "                features_batch = self.model.avgpool(\n",
    "                    self.model.layer4(\n",
    "                        self.model.layer3(\n",
    "                            self.model.layer2(\n",
    "                                self.model.layer1(\n",
    "                                    self.model.maxpool(\n",
    "                                        self.model.relu(\n",
    "                                            self.model.bn1(\n",
    "                                                self.model.conv1(images)\n",
    "                                            )\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "                features_batch = torch.flatten(features_batch, 1)\n",
    "                features.append(features_batch.cpu().numpy())\n",
    "                labels.extend(target.numpy())\n",
    "        \n",
    "        features = np.vstack(features)\n",
    "        labels = np.array(labels)\n",
    "        \n",
    "        # Reduce dimensionality for visualization\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        features_2d = tsne.fit_transform(features)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                            c=labels, cmap='tab10')\n",
    "        plt.colorbar(scatter, label='Classes', ticks=range(len(self.class_names)))\n",
    "        plt.title(f'Feature Space Visualization (t-SNE) - Epoch {epoch}')\n",
    "        plt.xlabel('t-SNE dimension 1')\n",
    "        plt.ylabel('t-SNE dimension 2')\n",
    "        \n",
    "        # Add legend\n",
    "        handles = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                            markerfacecolor=plt.cm.tab10(i / len(self.class_names)), \n",
    "                            label=name, markersize=8) \n",
    "                  for i, name in enumerate(self.class_names)]\n",
    "        plt.legend(handles=handles, title='Classes', bbox_to_anchor=(1.15, 1.0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'feature_space_epoch_{epoch}.png', \n",
    "                    bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "    \n",
    "    def export_confusion_matrix(self, epoch):\n",
    "        self.model.eval()\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(self.val_loader, desc=\"Computing confusion matrix\",\n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "        \n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=self.class_names,\n",
    "                   yticklabels=self.class_names)\n",
    "        plt.title(f'Confusion Matrix - Epoch {epoch}')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.viz_dir / f'confusion_matrix_epoch_{epoch}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    def export_class_metrics(self, epoch):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, target in tqdm(self.val_loader, desc=\"Computing class metrics\",\n",
    "                                     position=2, leave=False):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                labels.extend(target.numpy())\n",
    "        \n",
    "        report = classification_report(labels, predictions, \n",
    "                                    target_names=self.class_names, \n",
    "                                    output_dict=True)\n",
    "        \n",
    "        with open(self.metrics_dir / f'class_metrics_epoch_{epoch}.json', 'w') as f:\n",
    "            json.dump(report, f, indent=4)\n",
    "    \n",
    "    def export_training_summary(self):\n",
    "        summary = {\n",
    "            'total_epochs': len(self.train_losses),\n",
    "            'best_accuracy': max(self.accuracies),\n",
    "            'final_accuracy': self.accuracies[-1],\n",
    "            'best_val_loss': min(self.val_losses),\n",
    "            'final_val_loss': self.val_losses[-1],\n",
    "            'training_duration': str(datetime.now() - self.start_time),\n",
    "            'learning_rate_progression': self.learning_rates,\n",
    "            'accuracy_progression': self.accuracies,\n",
    "            'val_loss_progression': self.val_losses,\n",
    "            'class_names': self.class_names\n",
    "        }\n",
    "        \n",
    "        with open(self.output_dir / 'training_summary.json', 'w') as f:\n",
    "            json.dump(summary, f, indent=4)\n",
    "\n",
    "def train_final_model(dataset, class_names, identifier):\n",
    "    # Setup\n",
    "    logger = setup_logging(identifier)\n",
    "    output_dir = Path(DATA_PARAMS['output_dir']) / identifier\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Split dataset and create data loaders\n",
    "    train_size = int(TRAINING_PARAMS['train_val_split'] * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    # DataLoader setup with single-process loading\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # Force single process\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=TRAINING_PARAMS['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=0,  # Force single process\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=False\n",
    "    )\n",
    "    \n",
    "    # Device setup with MPS support\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        logger.info(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "        use_mixed_precision = True\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        logger.info(\"Using Apple Silicon (MPS) device\")\n",
    "        use_mixed_precision = False  # MPS doesn't support mixed precision yet\n",
    "        if hasattr(torch.mps, 'empty_cache'):\n",
    "            torch.mps.empty_cache()\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        logger.info(\"Using CPU device\")\n",
    "        use_mixed_precision = False\n",
    "    \n",
    "    # Set up mixed precision training based on device\n",
    "    if use_mixed_precision:\n",
    "        scaler = GradScaler()\n",
    "        logger.info(\"Using mixed precision training\")\n",
    "    else:\n",
    "        scaler = None\n",
    "        logger.info(\"Mixed precision training not available for this device\")\n",
    "    \n",
    "    # Model setup\n",
    "    if TRAINING_PARAMS['model_type'] == 'ResNet18':\n",
    "        model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    else:\n",
    "        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, TRAINING_PARAMS['hidden_dim']),\n",
    "        nn.BatchNorm1d(TRAINING_PARAMS['hidden_dim']),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(TRAINING_PARAMS['dropout_rate']),\n",
    "        nn.Linear(TRAINING_PARAMS['hidden_dim'], len(class_names))\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = FocalLoss(gamma=TRAINING_PARAMS['focal_loss_gamma'])\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=TRAINING_PARAMS['learning_rate'],\n",
    "        weight_decay=TRAINING_PARAMS['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = LRSchedulerWrapper(\n",
    "        torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, \n",
    "            mode='min',\n",
    "            factor=TRAINING_PARAMS['scheduler_factor'],\n",
    "            patience=TRAINING_PARAMS['scheduler_patience']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Initialize monitor\n",
    "    monitor = TrainingMonitor(model, train_loader, val_loader, device, \n",
    "                          logger, output_dir, class_names)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    steps_without_improvement = 0\n",
    "    total_epochs = TRAINING_PARAMS['num_epochs']\n",
    "    \n",
    "    try:\n",
    "        # Add overall progress bar\n",
    "        epoch_pbar = tqdm(range(total_epochs), \n",
    "                         desc=\"Overall Progress\",\n",
    "                         position=0,\n",
    "                         leave=True,\n",
    "                         dynamic_ncols=True)\n",
    "        \n",
    "        for epoch in epoch_pbar:\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Training phase with nested progress bar\n",
    "            batch_pbar = tqdm(train_loader, \n",
    "                            desc=f\"Epoch {epoch+1}/{total_epochs}\",\n",
    "                            position=1,\n",
    "                            leave=False,\n",
    "                            dynamic_ncols=True)\n",
    "            \n",
    "            for i, (images, labels) in enumerate(batch_pbar):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Handle training step based on mixed precision availability\n",
    "                if use_mixed_precision:\n",
    "                    with autocast(device_type=device.type):\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                    loss = loss / TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (i + 1) % TRAINING_PARAMS['gradient_accumulation_steps'] == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss = loss / TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    if (i + 1) % TRAINING_PARAMS['gradient_accumulation_steps'] == 0:\n",
    "                        optimizer.step()\n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                running_loss += loss.item() * TRAINING_PARAMS['gradient_accumulation_steps']\n",
    "                batch_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "                \n",
    "                # Empty cache periodically for MPS\n",
    "                if device.type == \"mps\" and (i + 1) % 50 == 0:\n",
    "                    if hasattr(torch.mps, 'empty_cache'):\n",
    "                        torch.mps.empty_cache()\n",
    "            \n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                val_pbar = tqdm(val_loader, \n",
    "                              desc=\"Validation\",\n",
    "                              position=1,\n",
    "                              leave=False,\n",
    "                              dynamic_ncols=True)\n",
    "                \n",
    "                for images, labels in val_pbar:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    val_pbar.set_postfix({'val_loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # Calculate metrics\n",
    "            train_loss = running_loss / len(train_loader)\n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            accuracy = correct / total\n",
    "            current_lr = scheduler.step(val_loss)\n",
    "            \n",
    "            # Update progress bar with current metrics\n",
    "            epoch_pbar.set_postfix({\n",
    "                'Train Loss': f'{train_loss:.4f}',\n",
    "                'Val Loss': f'{val_loss:.4f}', \n",
    "                'Accuracy': f'{accuracy:.4f}',\n",
    "                'LR': f'{current_lr:.6f}'\n",
    "            })\n",
    "            \n",
    "            # Log and export metrics\n",
    "            metrics = monitor.log_metrics(epoch, train_loss, val_loss, accuracy, current_lr)\n",
    "            monitor.export_metrics_to_csv(epoch)\n",
    "            \n",
    "            # Generate visualizations on interval\n",
    "            if (epoch + 1) % TRAINING_PARAMS['visualization_interval'] == 0:\n",
    "                monitor.plot_learning_curves(epoch)\n",
    "                monitor.plot_feature_space(epoch)\n",
    "                monitor.export_confusion_matrix(epoch)\n",
    "                monitor.export_class_metrics(epoch)\n",
    "            \n",
    "            # Save checkpoint at interval\n",
    "            if (epoch + 1) % TRAINING_PARAMS['checkpoint_interval'] == 0:\n",
    "                manage_checkpoints(output_dir, epoch, model, optimizer, scheduler, metrics, device)\n",
    "            \n",
    "            # Save best model\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                steps_without_improvement = 0\n",
    "                \n",
    "                # Handle MPS device when saving model\n",
    "                if device.type == \"mps\":\n",
    "                    model_state_dict = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "                    optimizer_state_dict = {k: v.cpu() if isinstance(v, torch.Tensor) else v \n",
    "                                         for k, v in optimizer.state_dict().items()}\n",
    "                else:\n",
    "                    model_state_dict = model.state_dict()\n",
    "                    optimizer_state_dict = optimizer.state_dict()\n",
    "                \n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': model_state_dict,\n",
    "                    'optimizer_state_dict': optimizer_state_dict,\n",
    "                    'scheduler_state_dict': scheduler.scheduler.state_dict(),\n",
    "                    'val_loss': val_loss,\n",
    "                    'accuracy': accuracy,\n",
    "                    'metrics': metrics\n",
    "                }, output_dir / 'best_model.pth')\n",
    "            else:\n",
    "                steps_without_improvement += 1\n",
    "            \n",
    "            # Empty MPS cache after each epoch\n",
    "            if device.type == \"mps\" and hasattr(torch.mps, 'empty_cache'):\n",
    "                torch.mps.empty_cache()\n",
    "            \n",
    "            # Early stopping check\n",
    "            if steps_without_improvement >= TRAINING_PARAMS['scheduler_patience'] * 2:\n",
    "                logger.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        logger.info(\"Training interrupted by user\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training error: {str(e)}\")\n",
    "        raise e\n",
    "    finally:\n",
    "        # Export final training summary before exiting\n",
    "        monitor.export_training_summary()\n",
    "    \n",
    "    return model, monitor\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create dataset with optional image limit\n",
    "    dataset = CityDataset(\n",
    "        folders=DATA_PARAMS['data_folders'], \n",
    "        transform=train_transform,\n",
    "        max_images_per_class=TRAINING_PARAMS['max_images_per_class']\n",
    "    )\n",
    "    class_names = list(DATA_PARAMS['data_folders'].keys())\n",
    "\n",
    "    # Create identifier for this run\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "    identifier = (f\"softmax-{TRAINING_PARAMS['model_type']}_\"\n",
    "                 f\"{TRAINING_PARAMS['num_epochs']}-ep_\"\n",
    "                 f\"{TRAINING_PARAMS['effective_batch_size']}-bs_\"\n",
    "                 f\"{current_time}\")\n",
    "    \n",
    "    # Train model\n",
    "    model, monitor = train_final_model(dataset, class_names, identifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Option B: Resnet Model Training / Fine tuning for better feature extraction without extended evaluation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50\n",
    "checkpoint_interval = 25\n",
    "max_images_per_class = 25000\n",
    "resnet_model = 'ResNet50'\n",
    "\n",
    "# Setup directories and paths\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"softmax-{resnet_model}_{num_epochs}-ep_{batch_size}-bs_{max_images_per_class}-images_{current_time}\"\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "folders = {\n",
    "    'Boston': '../data/ma-boston/buildings',\n",
    "    'Charlotte': '../data/nc-charlotte/buildings',\n",
    "    'Manhattan': '../data/ny-manhattan/buildings',\n",
    "    'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "}\n",
    "output_folder = os.path.join('softmax-output', identifier)\n",
    "checkpoint_dir = os.path.join(output_folder, 'checkpoints')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model_{identifier}.pth')\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Dataset and model setup\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "num_classes = len(class_names)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=max_images_per_class):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "\n",
    "        print(\"Building dataset...\")\n",
    "        for class_name, folder in tqdm(folders.items(), desc=\"Loading classes\"):\n",
    "            # Filter out macOS system files and get only image files\n",
    "            class_images = [\n",
    "                os.path.join(folder, f) for f in os.listdir(folder) \n",
    "                if (f.lower().endswith(('.jpg', '.jpeg', '.png')) and \n",
    "                    not f.startswith('._') and \n",
    "                    not f.startswith('.DS_Store'))\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nFound {len(class_images)} images for {class_name}\")\n",
    "            \n",
    "            if len(class_images) > max_images_per_class:\n",
    "                class_images = random.sample(class_images, max_images_per_class)\n",
    "            \n",
    "            self.image_paths.extend(class_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(class_images))\n",
    "        \n",
    "        print(\"\\nDataset statistics:\")\n",
    "        print(f\"Total images: {len(self.image_paths)}\")\n",
    "        for class_name in folders.keys():\n",
    "            class_count = self.labels.count(self.class_to_idx[class_name])\n",
    "            print(f\"{class_name}: {class_count} images\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            \n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "# Enhanced transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Create dataset\n",
    "print(\"\\nInitializing dataset...\")\n",
    "dataset = CityDataset(folders, transform=transform)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        ce_loss = F.cross_entropy(input, target, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma * ce_loss)\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_final_model(dataset):\n",
    "    print(\"\\nSplitting dataset into train/val sets...\")\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_dataset)}\")\n",
    "    print(f\"Validation set size: {len(val_dataset)}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"\\nInitializing {resnet_model}...\")\n",
    "    if resnet_model == 'ResNet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif resnet_model == 'ResNet50':\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = FocalLoss(gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "    scaler = GradScaler('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience = 10\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Training Progress\", position=0)\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        per_class_correct = torch.zeros(num_classes)\n",
    "        per_class_total = torch.zeros(num_classes)\n",
    "        \n",
    "        batch_pbar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", \n",
    "                         leave=False, position=1)\n",
    "        \n",
    "        for images, labels in batch_pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels)\n",
    "            \n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(images)\n",
    "                loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            current_loss = loss.item()\n",
    "            batch_pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _, predicted = torch.max(model(images), 1)\n",
    "                for label, pred in zip(labels, predicted):\n",
    "                    per_class_correct[label] += (label == pred).item()\n",
    "                    per_class_total[label] += 1\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        val_pbar = tqdm(val_loader, desc=\"Validation\", \n",
    "                       leave=False, position=1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_pbar:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                current_val_loss = loss.item()\n",
    "                val_pbar.set_postfix({'val_loss': f'{current_val_loss:.4f}'})\n",
    "        \n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        epoch_pbar.set_postfix({\n",
    "            'train_loss': f'{train_loss:.4f}',\n",
    "            'val_loss': f'{val_loss:.4f}',\n",
    "            'accuracy': f'{accuracy:.4f}'\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1} Complete:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}\")\n",
    "        print(f\"Val Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        print(\"\\nPer-class accuracies:\")\n",
    "        for i in range(num_classes):\n",
    "            if per_class_total[i] > 0:\n",
    "                class_acc = per_class_correct[i] / per_class_total[i]\n",
    "                print(f\"{class_names[i]}: {class_acc:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            print(f\"\\nSaving best model with val_loss: {val_loss:.4f}\")\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'accuracy': accuracy\n",
    "            }, model_save_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            \n",
    "        if epochs_without_improvement >= patience:\n",
    "            print(\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"\\nCheckpoint saved: {checkpoint_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nStarting training...\")\n",
    "    final_model = train_final_model(dataset)\n",
    "    print(f\"\\nTraining complete! Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2, Option A: Softmax classifier predictions with Test Time Augmentation using the fine tuned resnet model (single images)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "resnet_model = 'ResNet50'\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Transform for prediction (no augmentation)\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model with the improved architecture\"\"\"\n",
    "    if resnet_model == 'ResNet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "    elif resnet_model == 'ResNet50':\n",
    "        weights = models.ResNet50_Weights.DEFAULT\n",
    "        model = models.resnet50(weights=weights)\n",
    "    \n",
    "    # Use the same improved classifier head as in training\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(model.fc.in_features, 512),\n",
    "        nn.BatchNorm1d(512),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Load trained weights\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    if 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    return model\n",
    "\n",
    "def predict_single_pass(model, image_tensor):\n",
    "    \"\"\"Make a single prediction pass\"\"\"\n",
    "    outputs = model(image_tensor)\n",
    "    # Temperature scaling for sharper predictions\n",
    "    temperature = 1.5\n",
    "    scaled_outputs = outputs / temperature\n",
    "    probabilities = F.softmax(scaled_outputs, dim=1)[0]\n",
    "    return probabilities\n",
    "\n",
    "def predict_with_tta(model, image_path, num_augmentations=5):\n",
    "    \"\"\"Predict with Test Time Augmentation\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    try:\n",
    "        # Load and prepare image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        with torch.no_grad():  # Disable gradient computation\n",
    "            # Base prediction\n",
    "            base_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            base_pred = predict_single_pass(model, base_tensor)\n",
    "            predictions.append(base_pred)\n",
    "            \n",
    "            # TTA predictions\n",
    "            tta_transforms = [\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(10),\n",
    "                transforms.ColorJitter(brightness=0.1),\n",
    "                transforms.RandomAffine(5, translate=(0.05, 0.05)),\n",
    "            ]\n",
    "            \n",
    "            for _ in range(num_augmentations):\n",
    "                aug_tensor = base_tensor.clone()\n",
    "                for t in random.sample(tta_transforms, 2):  # Apply 2 random transforms\n",
    "                    aug_tensor = t(aug_tensor)\n",
    "                aug_pred = predict_single_pass(model, aug_tensor)\n",
    "                predictions.append(aug_pred)\n",
    "            \n",
    "            # Average all predictions\n",
    "            final_pred = torch.mean(torch.stack(predictions), dim=0)\n",
    "            predicted_class = torch.argmax(final_pred).item()\n",
    "            \n",
    "            return final_pred.cpu().numpy(), predicted_class\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image {image_path}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "def predict_batch(model_path, image_paths, output_file=None):\n",
    "    \"\"\"Predict cities for multiple images using TTA\"\"\"\n",
    "    # Load model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    results = []\n",
    "    for image_path in image_paths:\n",
    "        probabilities, predicted_class = predict_with_tta(model, image_path)\n",
    "        \n",
    "        if probabilities is not None:\n",
    "            result = {\n",
    "                'image_path': image_path,\n",
    "                'predicted_class': class_names[predicted_class],\n",
    "                'probabilities': {\n",
    "                    class_name: float(prob) \n",
    "                    for class_name, prob in zip(class_names, probabilities)\n",
    "                }\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"\\nPredictions for {image_path}:\")\n",
    "            print(f\"Predicted class: {class_names[predicted_class]}\")\n",
    "            print(\"Class probabilities:\")\n",
    "            for class_name, prob in zip(class_names, probabilities):\n",
    "                print(f\"{class_name}: {prob:.4f}\")\n",
    "    \n",
    "    # Save results if output file specified\n",
    "    if output_file and results:\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        print(f\"\\nPredictions saved to {output_file}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"models/softmax-ResNet50_50-ep_128-bs_2024-12-06_10-33.pth\"\n",
    "    image_paths = [\n",
    "        \"../data/ny-brooklyn/buildings/buildings_1370.jpg\",\n",
    "        \"../data/ny-brooklyn/buildings/buildings_152277.jpg\",\n",
    "        \"../data/ma-brookline/buildings/buildings_60.jpg\"\n",
    "    ]\n",
    "    output_file = \"softmax-output/resnet-softmax-predictions-test-50e.json\"\n",
    "    \n",
    "    results = predict_batch(model_path, image_paths, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration Parameters\n",
    "# =============================================================================\n",
    "\n",
    "# Model Configuration\n",
    "RESNET_MODEL = 'ResNet50'  # Options: 'ResNet18', 'ResNet50'\n",
    "CLASS_NAMES = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "MODEL_PATH = \"models/softmax-ResNet50_50-ep_128-bs_2024-12-06_10-33.pth\"\n",
    "\n",
    "# Input/Output Configuration\n",
    "TEST_FOLDERS = {\n",
    "    'brooklyn': \"../data/ny-brooklyn/parcels\",\n",
    "    'brooklyn-boston-model': \"../data/ny-brooklyn/ma-boston-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-charlotte-model': \"../data/ny-brooklyn/nc-charlotte-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-manhattan-model': \"../data/ny-brooklyn/ny-manhattan-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "    'brooklyn-pittsburgh-model': \"../data/ny-brooklyn/pa-pittsburgh-p2p-500-150-v100/test_latest_500e-Brooklyn/images\",\n",
    "}\n",
    "\n",
    "# Verify paths exist\n",
    "valid_folders = {}\n",
    "for name, path in TEST_FOLDERS.items():\n",
    "    if os.path.exists(path):\n",
    "        print(f\"Found valid path: {path}\")\n",
    "        valid_folders[name] = path\n",
    "    else:\n",
    "        print(f\"Path not found: {path}\")\n",
    "TEST_FOLDERS = valid_folders\n",
    "\n",
    "SAMPLE_SIZE = 1000  # Number of images to process per folder (None for all images)\n",
    "OUTPUT_DIR = \"softmax-output/city-predictions\"  # Base directory for saving results\n",
    "\n",
    "# Files to ignore (Mac and hidden files)\n",
    "IGNORE_PATTERNS = {\n",
    "    '.DS_Store',\n",
    "    '._',\n",
    "    '.AppleDouble',\n",
    "    '.LSOverride',\n",
    "    'Icon\\r',\n",
    "    '.Spotlight-V100',\n",
    "    '.Trashes',\n",
    "    '__MACOSX',\n",
    "    'thumbs.db',\n",
    "    'Thumbs.db',\n",
    "    '.git',\n",
    "    '.ipynb_checkpoints'\n",
    "}\n",
    "\n",
    "# Model Parameters\n",
    "NUM_AUGMENTATIONS = 5  # Number of augmentations for test-time augmentation\n",
    "TEMPERATURE = 1.5  # Temperature scaling for prediction sharpening\n",
    "IMG_SIZE = 224  # Input image size\n",
    "NORMALIZE_MEAN = [0.485, 0.456, 0.406]  \n",
    "NORMALIZE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Augmentation Parameters\n",
    "ROTATION_DEGREES = 10  # Max rotation degrees for augmentation\n",
    "BRIGHTNESS_JITTER = 0.1  # Brightness adjustment range\n",
    "TRANSLATION_RANGE = 0.05  # Max translation as fraction of image size\n",
    "\n",
    "# Visualization Parameters\n",
    "CONFIDENCE_BINS = 30  # Number of bins for confidence histogram\n",
    "PLOT_SIZE_LARGE = (12, 6)  # Size for large plots\n",
    "PLOT_SIZE_MEDIUM = (10, 6)  # Size for medium plots\n",
    "\n",
    "# =============================================================================\n",
    "# Model Setup\n",
    "# =============================================================================\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \n",
    "                     \"mps\" if torch.backends.mps.is_available() else \n",
    "                     \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transform for prediction\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_STD),\n",
    "])\n",
    "\n",
    "def is_valid_image_file(file_path):\n",
    "    \"\"\"Check if a file is a valid image and not a system or hidden file\"\"\"\n",
    "    file_name = file_path.name\n",
    "    \n",
    "    # Check for ignored patterns\n",
    "    if any(pattern in str(file_path) for pattern in IGNORE_PATTERNS):\n",
    "        return False\n",
    "    \n",
    "    # Check if it's a hidden file\n",
    "    if file_name.startswith('.') or file_name.startswith('_'):\n",
    "        return False\n",
    "        \n",
    "    # Verify it's a valid image\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            img.verify()\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load the trained model with the improved architecture\"\"\"\n",
    "    try:\n",
    "        if RESNET_MODEL == 'ResNet18':\n",
    "            weights = models.ResNet18_Weights.DEFAULT\n",
    "            model = models.resnet18(weights=weights)\n",
    "        elif RESNET_MODEL == 'ResNet50':\n",
    "            weights = models.ResNet50_Weights.DEFAULT\n",
    "            model = models.resnet50(weights=weights)\n",
    "        \n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Linear(model.fc.in_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, len(CLASS_NAMES))\n",
    "        )\n",
    "        \n",
    "        checkpoint = torch.load(model_path, map_location=device, weights_only=True)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def predict_single_pass(model, image_tensor):\n",
    "    \"\"\"Make a single prediction pass\"\"\"\n",
    "    outputs = model(image_tensor)\n",
    "    scaled_outputs = outputs / TEMPERATURE\n",
    "    probabilities = F.softmax(scaled_outputs, dim=1)[0]\n",
    "    return probabilities\n",
    "\n",
    "def predict_with_tta(model, image_path, num_augmentations=NUM_AUGMENTATIONS):\n",
    "    \"\"\"Predict with Test Time Augmentation\"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Base prediction\n",
    "            base_tensor = transform(image).unsqueeze(0).to(device)\n",
    "            base_pred = predict_single_pass(model, base_tensor)\n",
    "            predictions.append(base_pred)\n",
    "            \n",
    "            # TTA predictions\n",
    "            tta_transforms = [\n",
    "                transforms.RandomHorizontalFlip(p=1.0),\n",
    "                transforms.RandomRotation(ROTATION_DEGREES),\n",
    "                transforms.ColorJitter(brightness=BRIGHTNESS_JITTER),\n",
    "                transforms.RandomAffine(ROTATION_DEGREES, translate=(TRANSLATION_RANGE, TRANSLATION_RANGE)),\n",
    "            ]\n",
    "            \n",
    "            for _ in range(num_augmentations):\n",
    "                aug_tensor = base_tensor.clone()\n",
    "                for t in random.sample(tta_transforms, 2):\n",
    "                    aug_tensor = t(aug_tensor)\n",
    "                aug_pred = predict_single_pass(model, aug_tensor)\n",
    "                predictions.append(aug_pred)\n",
    "            \n",
    "            final_pred = torch.mean(torch.stack(predictions), dim=0)\n",
    "            predicted_class = torch.argmax(final_pred).item()\n",
    "            confidence = float(final_pred[predicted_class])\n",
    "            \n",
    "            return final_pred.cpu().numpy(), predicted_class, confidence\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error predicting image {image_path}: {str(e)}\")\n",
    "        return None, None, None\n",
    "    \n",
    "def analyze_results(results):\n",
    "    \"\"\"Analyze prediction results and generate statistics\"\"\"\n",
    "    if not results:\n",
    "        return {\n",
    "            'confidence_stats': {},\n",
    "            'class_distribution': {},\n",
    "            'probability_stats': {}\n",
    "        }\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate confidence statistics\n",
    "    confidence_stats = {\n",
    "        'mean_confidence': df['confidence'].mean(),\n",
    "        'median_confidence': df['confidence'].median(),\n",
    "        'min_confidence': df['confidence'].min(),\n",
    "        'max_confidence': df['confidence'].max(),\n",
    "        'std_confidence': df['confidence'].std()\n",
    "    }\n",
    "    \n",
    "    # Calculate class distribution\n",
    "    class_distribution = df['predicted_class'].value_counts().to_dict()\n",
    "    \n",
    "    # Calculate probability statistics per class\n",
    "    prob_stats = {}\n",
    "    for class_name in CLASS_NAMES:\n",
    "        probs = [r['probabilities'][class_name] for r in results]\n",
    "        prob_stats[class_name] = {\n",
    "            'mean': np.mean(probs),\n",
    "            'median': np.median(probs),\n",
    "            'min': np.min(probs),\n",
    "            'max': np.max(probs),\n",
    "            'std': np.std(probs)\n",
    "        }\n",
    "    \n",
    "    return {\n",
    "        'confidence_stats': confidence_stats,\n",
    "        'class_distribution': class_distribution,\n",
    "        'probability_stats': prob_stats\n",
    "    }\n",
    "\n",
    "def generate_visualizations(results, output_dir):\n",
    "    \"\"\"Generate and save visualization plots\"\"\"\n",
    "    if not results:\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # 1. Confidence Distribution\n",
    "    plt.figure(figsize=PLOT_SIZE_MEDIUM)\n",
    "    sns.histplot(data=df, x='confidence', bins=CONFIDENCE_BINS)\n",
    "    plt.title('Distribution of Prediction Confidence')\n",
    "    plt.xlabel('Confidence')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(os.path.join(output_dir, 'confidence_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Class Distribution\n",
    "    plt.figure(figsize=PLOT_SIZE_MEDIUM)\n",
    "    sns.countplot(data=df, x='predicted_class')\n",
    "    plt.title('Distribution of Predicted Classes')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'class_distribution.png'))\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Probability Distribution by Class\n",
    "    prob_data = []\n",
    "    for result in results:\n",
    "        for class_name, prob in result['probabilities'].items():\n",
    "            prob_data.append({\n",
    "                'class': class_name,\n",
    "                'probability': prob\n",
    "            })\n",
    "    \n",
    "    prob_df = pd.DataFrame(prob_data)\n",
    "    plt.figure(figsize=PLOT_SIZE_LARGE)\n",
    "    sns.boxplot(data=prob_df, x='class', y='probability')\n",
    "    plt.title('Probability Distribution by Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, 'probability_distribution.png'))\n",
    "    plt.close()\n",
    "\n",
    "def generate_comparative_analysis(all_results, base_output_dir):\n",
    "    \"\"\"Generate comparative visualizations and reports across folders\"\"\"\n",
    "    comparative_dir = base_output_dir / 'comparative_analysis'\n",
    "    comparative_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Collect data for comparison\n",
    "    comparison_data = {\n",
    "        'confidence': [],\n",
    "        'class_distribution': [],\n",
    "        'probability_distribution': []\n",
    "    }\n",
    "    \n",
    "    # Process each folder's results\n",
    "    for folder_name, results in all_results.items():\n",
    "        # Confidence stats\n",
    "        confidence_stats = results['analysis']['confidence_stats']\n",
    "        comparison_data['confidence'].append({\n",
    "            'folder': folder_name,\n",
    "            'mean': confidence_stats['mean_confidence'],\n",
    "            'median': confidence_stats['median_confidence'],\n",
    "            'std': confidence_stats['std_confidence']\n",
    "        })\n",
    "        \n",
    "        # Class distribution\n",
    "        class_dist = results['analysis']['class_distribution']\n",
    "        for class_name in CLASS_NAMES:\n",
    "            comparison_data['class_distribution'].append({\n",
    "                'folder': folder_name,\n",
    "                'class': class_name,\n",
    "                'count': class_dist.get(class_name, 0)\n",
    "            })\n",
    "        \n",
    "        # Probability distributions\n",
    "        for pred in results['predictions']:\n",
    "            for class_name, prob in pred['probabilities'].items():\n",
    "                comparison_data['probability_distribution'].append({\n",
    "                    'folder': folder_name,\n",
    "                    'class': class_name,\n",
    "                    'probability': prob\n",
    "                })\n",
    "    \n",
    "    # Create DataFrames\n",
    "    confidence_df = pd.DataFrame(comparison_data['confidence'])\n",
    "    class_dist_df = pd.DataFrame(comparison_data['class_distribution'])\n",
    "    prob_dist_df = pd.DataFrame(comparison_data['probability_distribution'])\n",
    "    \n",
    "    # 1. Confidence Comparison\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    confidence_summary = confidence_df.melt(\n",
    "        id_vars=['folder'], \n",
    "        value_vars=['mean', 'median', 'std'],\n",
    "        var_name='metric'\n",
    "    )\n",
    "    g = sns.barplot(data=confidence_summary, x='folder', y='value', hue='metric')\n",
    "    plt.title('Confidence Metrics Comparison Across Folders')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title='Metric', bbox_to_anchor=(1.05, 1))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparative_dir / 'confidence_comparison.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Class Distribution Comparison\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    pivot_dist = class_dist_df.pivot(index='folder', columns='class', values='count')\n",
    "    ax = pivot_dist.plot(kind='bar', stacked=True)\n",
    "    plt.title('Class Distribution Comparison Across Folders')\n",
    "    plt.xlabel('Folder')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.legend(title='Predicted Class', bbox_to_anchor=(1.05, 1))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparative_dir / 'class_distribution_comparison.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Probability Distribution Heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    pivot_prob = prob_dist_df.groupby(['folder', 'class'])['probability'].mean().unstack()\n",
    "    sns.heatmap(pivot_prob, annot=True, fmt='.2f', cmap='YlOrRd', cbar_kws={'label': 'Mean Probability'})\n",
    "    plt.title('Mean Prediction Probability Heatmap')\n",
    "    plt.ylabel('Folder')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(comparative_dir / 'probability_heatmap.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate comparative report\n",
    "    comparative_report = {\n",
    "        'confidence_summary': {\n",
    "            folder: {\n",
    "                'mean': float(stats['mean']),\n",
    "                'median': float(stats['median']),\n",
    "                'std': float(stats['std'])\n",
    "            }\n",
    "            for folder, stats in confidence_df.set_index('folder').to_dict('index').items()\n",
    "        },\n",
    "        'class_distribution': pivot_dist.to_dict('index'),\n",
    "        'probability_matrix': pivot_prob.round(3).to_dict('index'),\n",
    "        'relative_metrics': {\n",
    "            'highest_confidence_folder': confidence_df.loc[confidence_df['mean'].idxmax(), 'folder'],\n",
    "            'most_diverse_predictions': class_dist_df.groupby('folder')['count'].std().idxmin(),\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save comparative report\n",
    "    with open(comparative_dir / 'comparative_report.json', 'w') as f:\n",
    "        json.dump(comparative_report, f, indent=4)\n",
    "    \n",
    "    return comparative_report\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
