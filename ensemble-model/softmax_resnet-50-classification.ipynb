{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F  # Import F for softmax\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display  # Import display module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "identifier = 'experiment_01'\n",
    "class_names = ['CityA', 'CityB', 'CityC', 'CityD']\n",
    "folders = {\n",
    "    'CityA': '../data/ma-boston/buildings',\n",
    "    'CityB': '../data/nc-charlotte/buildings',\n",
    "    'CityC': '../data/ny-manhattan/buildings',\n",
    "    'CityD': '../data/pa-pittsburgh/buildings'\n",
    "}\n",
    "output_folder = 'softmax-output'\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "batch_size = 128\n",
    "num_classes = len(class_names)\n",
    "train_split_ratio = 0.8\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "checkpoint_interval = 1\n",
    "checkpoint_dir = os.path.join(output_folder, f'checkpoints-{identifier}')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model-{identifier}.pth')\n",
    "loss_log_path = os.path.join(output_folder, f'loss-log-{identifier}.json')\n",
    "training_params_path = os.path.join(output_folder, f'training-params-{identifier}.json')\n",
    "feature_file_name = f'city-features-{identifier}.npy'\n",
    "new_image_path = '../data/ny-brooklyn/buildings/buildings_1370.jpg' # test an image\n",
    "predictions_output_file = os.path.join(output_folder, f'predictions-{identifier}.txt')\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Define output folder\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "\n",
    "        for class_name, folder in folders.items():\n",
    "            for filename in os.listdir(folder):\n",
    "                if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    self.image_paths.append(os.path.join(folder, filename))\n",
    "                    self.labels.append(self.class_to_idx[class_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create dataset\n",
    "dataset = CityDataset(folders)\n",
    "\n",
    "# Automatically detect the input image size\n",
    "first_image_path = dataset.image_paths[0]\n",
    "first_image = Image.open(first_image_path)\n",
    "image_size = first_image.size  # (width, height)\n",
    "\n",
    "# Define transformations: resize, convert to tensor, normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Update dataset with transform\n",
    "dataset.transform = transform\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load a pre-trained ResNet50 model using the 'weights' parameter\n",
    "weights = models.ResNet50_Weights.DEFAULT\n",
    "model = models.resnet50(weights=weights)\n",
    "\n",
    "# Modify the final layer to match the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Set device to MPS if available, otherwise fall back to CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Save model outputs as features\n",
    "def extract_and_save_features(model, data_loader, file_name):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, label in tqdm(data_loader, desc=\"Extracting features\", leave=False):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            features.append(outputs.cpu().numpy())\n",
    "            labels.append(label.numpy())\n",
    "    \n",
    "    features = np.concatenate(features, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    np.save(os.path.join(output_folder, file_name), {'features': features, 'labels': labels})\n",
    "\n",
    "# Extract and save features\n",
    "extract_and_save_features(model, data_loader, feature_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training\n",
    "def train_and_save_model(model, train_loader, num_epochs, checkpoint_interval, checkpoint_dir):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_log = []\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}]\", leave=False)\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        loss_log.append(epoch_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f'Checkpoint saved at {checkpoint_path}')\n",
    "\n",
    "    # Save the final model weights\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # Save the loss log\n",
    "    with open(loss_log_path, 'w') as f:\n",
    "        json.dump(loss_log, f)\n",
    "\n",
    "    # Save the training parameters and plot\n",
    "    training_params = {\n",
    "        'num_epochs': num_epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'checkpoint_interval': checkpoint_interval,\n",
    "        'batch_size': train_loader.batch_size,\n",
    "        'identifier': identifier\n",
    "    }\n",
    "    with open(training_params_path, 'w') as f:\n",
    "        json.dump(training_params, f)\n",
    "\n",
    "    # Plot the loss log\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs + 1), loss_log, marker='o')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_folder, f'training_loss_plot-{identifier}.png'))\n",
    "    plt.close()\n",
    "\n",
    "# Train and save the model\n",
    "train_size = int(train_split_ratio * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "train_and_save_model(model, train_loader, num_epochs, checkpoint_interval, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25a895b050f4d4593dfc8c3f7cba37e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Classifying new image:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/nc-charlotte/buildings/buildings_1270.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m new_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/nc-charlotte/buildings/buildings_1270.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying new image\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 27\u001b[0m     predictions, input_image \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_new_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_image_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mclassify_new_image\u001b[0;34m(image_path, model, transform)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify_new_image\u001b[39m(image_path, model, transform):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 10\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m transform(input_image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/PIL/Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m builtins\u001b[38;5;241m.\u001b[39mopen(filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/nc-charlotte/buildings/buildings_1270.jpg'"
     ]
    }
   ],
   "source": [
    "# Paths to the model and checkpoint\n",
    "model_path = 'softmax-output/checkpoints-experiment_01/checkpoint_epoch_2.pth'\n",
    "\n",
    "# Load model weights\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Function to classify a new image\n",
    "def classify_new_image(image_path, model, transform):\n",
    "    model.eval()\n",
    "    input_image = Image.open(image_path)\n",
    "    input_tensor = transform(input_image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        probabilities = probabilities.cpu().numpy().flatten()\n",
    "\n",
    "    predictions = [(class_names[i], prob * 100) for i, prob in enumerate(probabilities)]\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions, input_image\n",
    "\n",
    "# Example of classifying a new image with progress bar\n",
    "# new_image_path = '../data/ny-brooklyn/buildings/buildings_1370.jpg'\n",
    "new_image_path = '../data/nc-charlotte/buildings/buildings_131.jpg'\n",
    "\n",
    "with tqdm(total=1, desc=\"Classifying new image\", leave=False) as pbar:\n",
    "    predictions, input_image = classify_new_image(new_image_path, model, transform)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Display the image\n",
    "display(input_image)\n",
    "\n",
    "# Save predictions to a file\n",
    "output_file_path = os.path.join(output_folder, 'predictions.txt')\n",
    "with open(output_file_path, 'w') as f:\n",
    "    f.write(f'Image path: {new_image_path}\\n')  # Write the image path\n",
    "    for label, percentage in predictions:\n",
    "        f.write(f'Predicted class: {label}, Confidence: {percentage:.2f}%\\n')\n",
    "        print(f'Predicted class: {label}, Confidence: {percentage:.2f}%')\n",
    "\n",
    "# Optionally, display the predictions in the notebook\n",
    "print(f'Image path: {new_image_path}')\n",
    "#for label, percentage in predictions:\n",
    "#    print(f'Predicted class: {label}, Confidence: {percentage:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
