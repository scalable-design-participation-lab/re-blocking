{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Downloading pre-trained weights...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e81c7afb9cbd432ebae21f70fe1f9ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "/var/folders/9g/r0ctqhfj26l910sgbwcdndq00000gn/T/tmpi1umpwc4:   0%|          | 0.00/44.7M [00:00<?, ?iB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed. Pre-trained weights downloaded and ready for use.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9g/r0ctqhfj26l910sgbwcdndq00000gn/T/ipykernel_91796/4055001870.py:83: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(temp_file.name, map_location='cpu')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy.stats import linregress\n",
    "import requests\n",
    "import tempfile\n",
    "import gc\n",
    "import contextlib\n",
    "\n",
    "# Parameters\n",
    "identifier = 'softmax-v28-prevent-overfitting'\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "base_folder = '../data'\n",
    "folders = {\n",
    "    'Boston': os.path.join(base_folder, 'ma-boston/buildings'),\n",
    "    'Charlotte': os.path.join(base_folder, 'nc-charlotte/buildings'),\n",
    "    'Manhattan': os.path.join(base_folder, 'ny-manhattan/buildings'),\n",
    "    'Pittsburgh': os.path.join(base_folder, 'pa-pittsburgh/buildings')\n",
    "}\n",
    "output_folder = 'softmax-output'\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "batch_size = 32\n",
    "num_classes = len(class_names)\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "checkpoint_interval = 5\n",
    "checkpoint_dir = os.path.join(output_folder, f'checkpoints-{identifier}')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model-{identifier}.pth')\n",
    "loss_log_path = os.path.join(output_folder, f'loss-log-{identifier}.json')\n",
    "new_image_path = os.path.join(base_folder, 'ny-brooklyn', 'buildings_1370.jpg')\n",
    "predictions_output_file = os.path.join(output_folder, f'predictions-{identifier}.txt')\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def download_file_with_progress(url, filename):\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 KB\n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='iB',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as progress_bar:\n",
    "        for data in response.iter_content(block_size):\n",
    "            size = file.write(data)\n",
    "            progress_bar.update(size)\n",
    "\n",
    "def download_pretrained_weights():\n",
    "    print(\"Downloading pre-trained weights...\")\n",
    "    url = \"https://download.pytorch.org/models/resnet18-5c106cde.pth\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
    "        download_file_with_progress(url, temp_file.name)\n",
    "        state_dict = torch.load(temp_file.name, map_location='cpu')\n",
    "    os.unlink(temp_file.name)\n",
    "    return state_dict\n",
    "\n",
    "# Download weights\n",
    "pretrained_weights = download_pretrained_weights()\n",
    "\n",
    "print(\"Setup completed. Pre-trained weights downloaded and ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset and Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining data transformations...\n",
      "Preparing dataset...\n",
      "Folder paths:\n",
      "Boston: ../data/ma-boston/buildings\n",
      "Charlotte: ../data/nc-charlotte/buildings\n",
      "Manhattan: ../data/ny-manhattan/buildings\n",
      "Pittsburgh: ../data/pa-pittsburgh/buildings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d9b6de29d84feda15a6a070cc558d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading class folders:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 24995 images for class Boston\n",
      "Loaded 24995 images for class Charlotte\n",
      "Loaded 25064 images for class Manhattan\n",
      "Loaded 24998 images for class Pittsburgh\n",
      "Dataset prepared with 100052 images\n",
      "Creating dataset...\n",
      "Splitting dataset into train and test sets...\n",
      "Creating DataLoaders...\n",
      "Initializing the model...\n",
      "Dataset created with 100052 images\n",
      "Training set: 80041 images\n",
      "Test set: 20011 images\n",
      "Model initialized and moved to device: mps\n",
      "Train DataLoader created with 2502 batches\n",
      "Test DataLoader created with 626 batches\n",
      "List of skipped files exported to: softmax-output/skipped_files-softmax-v28-prevent-overfitting.txt\n",
      "Clearing memory...\n",
      "Section 2 completed successfully\n",
      "Testing data loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd133849a8b1406590ed9763518c9d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
      "Batch 2: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
      "Batch 3: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
      "Batch 4: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
      "Batch 5: Images shape: torch.Size([32, 3, 224, 224]), Labels shape: torch.Size([32])\n",
      "Data loading test completed\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image, ImageFile\n",
    "from tqdm.auto import tqdm\n",
    "import contextlib\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "print(\"Defining data transformations...\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std)\n",
    "])\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        try:\n",
    "            with open(image_path, 'rb') as f:\n",
    "                img = Image.open(f).convert('RGB')\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                return img, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {str(e)}\")\n",
    "            return torch.zeros((3, 224, 224)), label\n",
    "\n",
    "print(\"Preparing dataset...\")\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "class_to_idx = {}\n",
    "skipped_files = []\n",
    "\n",
    "print(\"Folder paths:\")\n",
    "for class_name, folder in folders.items():\n",
    "    print(f\"{class_name}: {folder}\")\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"Warning: Folder does not exist: {folder}\")\n",
    "    elif not os.path.isdir(folder):\n",
    "        print(f\"Warning: Path is not a directory: {folder}\")\n",
    "\n",
    "with contextlib.closing(tqdm(folders.items(), desc=\"Loading class folders\")) as pbar:\n",
    "    for class_name, folder in pbar:\n",
    "        if not os.path.isdir(folder):\n",
    "            print(f\"Warning: Folder not found or not a directory: {folder}\")\n",
    "            continue\n",
    "        class_idx = len(class_to_idx)\n",
    "        class_to_idx[class_name] = class_idx\n",
    "        class_images = [os.path.join(folder, img_name) for img_name in os.listdir(folder) if os.path.isfile(os.path.join(folder, img_name)) and img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif'))]\n",
    "        all_image_paths.extend(class_images)\n",
    "        all_labels.extend([class_idx] * len(class_images))\n",
    "        print(f\"Loaded {len(class_images)} images for class {class_name}\")\n",
    "        if len(class_images) == 0:\n",
    "            print(f\"Contents of {folder}:\")\n",
    "            for item in os.listdir(folder):\n",
    "                print(f\"  {item}\")\n",
    "\n",
    "print(f\"Dataset prepared with {len(all_image_paths)} images\")\n",
    "\n",
    "if len(all_image_paths) == 0:\n",
    "    raise ValueError(\"No images found in the specified folders. Please check your folder paths and make sure they contain image files.\")\n",
    "\n",
    "print(\"Creating dataset...\")\n",
    "full_dataset = CustomDataset(all_image_paths, all_labels, transform=transform)\n",
    "\n",
    "print(\"Splitting dataset into train and test sets...\")\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Creating DataLoaders...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "class CustomResNet18(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_weights):\n",
    "        super(CustomResNet18, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=None)\n",
    "        self.resnet.load_state_dict(pretrained_weights)\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "print(\"Initializing the model...\")\n",
    "model = CustomResNet18(len(class_to_idx), pretrained_weights).to(device)\n",
    "\n",
    "print(f\"Dataset created with {len(full_dataset)} images\")\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Test set: {len(test_dataset)} images\")\n",
    "print(f\"Model initialized and moved to device: {device}\")\n",
    "print(f\"Train DataLoader created with {len(train_loader)} batches\")\n",
    "print(f\"Test DataLoader created with {len(test_loader)} batches\")\n",
    "\n",
    "# Export skipped files list\n",
    "skipped_files_path = os.path.join(output_folder, f'skipped_files-{identifier}.txt')\n",
    "with open(skipped_files_path, 'w') as f:\n",
    "    for path, reason in skipped_files:\n",
    "        f.write(f\"{path}: {reason}\\n\")\n",
    "print(f\"List of skipped files exported to: {skipped_files_path}\")\n",
    "\n",
    "print(\"Clearing memory...\")\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"Section 2 completed successfully\")\n",
    "\n",
    "print(\"Testing data loading...\")\n",
    "with contextlib.closing(tqdm(train_loader, desc=\"Testing batches\", total=min(5, len(train_loader)))) as pbar:\n",
    "    for i, (images, labels) in enumerate(pbar):\n",
    "        print(f\"Batch {i+1}: Images shape: {images.shape}, Labels shape: {labels.shape}\")\n",
    "        if i == 4:  # Test first 5 batches\n",
    "            break\n",
    "print(\"Data loading test completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(model, train_loader, val_loader, num_epochs, patience=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    train_loss_log, val_loss_log, val_accuracy_log = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_samples = 0\n",
    "        with contextlib.closing(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")) as pbar:\n",
    "            for images, labels in pbar:\n",
    "                if images.numel() == 0:\n",
    "                    continue\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * images.size(0)\n",
    "                total_samples += images.size(0)\n",
    "                pbar.set_postfix({'train_loss': loss.item()})\n",
    "        \n",
    "        train_loss /= total_samples\n",
    "        train_loss_log.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            with contextlib.closing(tqdm(val_loader, desc=\"Validating\", leave=False)) as pbar:\n",
    "                for images, labels in pbar:\n",
    "                    if images.numel() == 0:\n",
    "                        continue\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * images.size(0)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                    pbar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "        val_loss /= total\n",
    "        val_accuracy = correct / total if total > 0 else 0\n",
    "        val_loss_log.append(val_loss)\n",
    "        val_accuracy_log.append(val_accuracy)\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f'Early stopping triggered after epoch {epoch+1}')\n",
    "                model.load_state_dict(torch.load(model_save_path))\n",
    "                break\n",
    "\n",
    "    # Save the loss and accuracy logs\n",
    "    with open(loss_log_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'train_loss': train_loss_log,\n",
    "            'val_loss': val_loss_log,\n",
    "            'val_accuracy': val_accuracy_log\n",
    "        }, f)\n",
    "\n",
    "    # Plot the loss and accuracy curves\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_loss_log, label='Train Loss')\n",
    "    plt.plot(val_loss_log, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracy_log)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f'training_curves-{identifier}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    return val_accuracy_log[-1]\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    class_correct = list(0. for i in range(num_classes))\n",
    "    class_total = list(0. for i in range(num_classes))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with contextlib.closing(tqdm(data_loader, desc=\"Evaluating\")) as pbar:\n",
    "            for images, labels in pbar:\n",
    "                if images.numel() == 0:\n",
    "                    continue\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                c = (predicted == labels).squeeze()\n",
    "                for i in range(len(labels)):\n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    print(f'Overall accuracy: {accuracy:.2f}%')\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        class_accuracy = 100 * class_correct[i] / class_total[i] if class_total[i] > 0 else 0\n",
    "        print(f'Accuracy of {class_names[i]}: {class_accuracy:.2f}%')\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cross_validation(fold_results):\n",
    "    avg_accuracy = np.mean(fold_results)\n",
    "    std_accuracy = np.std(fold_results)\n",
    "    \n",
    "    print(f\"Cross-validation results:\")\n",
    "    print(f\"Average accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Standard deviation: {std_accuracy:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(1, len(fold_results) + 1), fold_results)\n",
    "    plt.axhline(y=avg_accuracy, color='r', linestyle='--', label='Average')\n",
    "    plt.title('Cross-validation Accuracy per Fold')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(output_folder, f'cross_validation_results-{identifier}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def error_analysis(model, test_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    misclassified_images = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with contextlib.closing(tqdm(test_loader, desc=\"Analyzing errors\")) as pbar:\n",
    "            for images, labels in pbar:\n",
    "                if images.numel() == 0:\n",
    "                    continue\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                misclassified = (preds != labels).nonzero().squeeze()\n",
    "                for idx in misclassified:\n",
    "                    misclassified_images.append((images[idx].cpu(), labels[idx].item(), preds[idx].item()))\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig(os.path.join(output_folder, f'confusion_matrix-{identifier}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < len(misclassified_images):\n",
    "            img, true_label, pred_label = misclassified_images[i]\n",
    "            ax.imshow(img.permute(1, 2, 0))\n",
    "            ax.set_title(f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]}')\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_folder, f'misclassified_samples-{identifier}.png'))\n",
    "    plt.close()\n",
    "\n",
    "def compare_with_baseline(train_data, test_data, model_accuracy):\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    for images, labels in tqdm(train_data, desc=\"Preparing train data\"):\n",
    "        if images.numel() > 0:\n",
    "            X_train.append(images.view(images.size(0), -1).cpu().numpy())\n",
    "            y_train.append(labels.cpu().numpy())\n",
    "    \n",
    "    for images, labels in tqdm(test_data, desc=\"Preparing test data\"):\n",
    "        if images.numel() > 0:\n",
    "            X_test.append(images.view(images.size(0), -1).cpu().numpy())\n",
    "            y_test.append(labels.cpu().numpy())\n",
    "\n",
    "    X_train = np.concatenate(X_train)\n",
    "    y_train = np.concatenate(y_train)\n",
    "    X_test = np.concatenate(X_test)\n",
    "    y_test = np.concatenate(y_test)\n",
    "\n",
    "    print(f\"Train data shape: {X_train.shape}, Train labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
    "\n",
    "    baseline_model = DummyClassifier(strategy='stratified')\n",
    "    baseline_model.fit(X_train, y_train)\n",
    "    baseline_preds = baseline_model.predict(X_test)\n",
    "    baseline_accuracy = accuracy_score(y_test, baseline_preds) * 100\n",
    "\n",
    "    print(f\"Baseline model accuracy: {baseline_accuracy:.2f}%\")\n",
    "    print(f\"Improvement over baseline: {model_accuracy - baseline_accuracy:.2f}%\")\n",
    "\n",
    "def predict_image(model, image_path, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    return probabilities.cpu().numpy(), predicted_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "458b649969734912b7d128b932418a07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/2:   0%|          | 0/2502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf1f65c99f849ea9657675732b167c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0695, Val Loss: 0.1013, Val Accuracy: 0.9694\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a63afed05cb44ea963c5d370adfb009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/2:   0%|          | 0/2502 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a57978c44814621b1a8f21190193f80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0336, Val Loss: 0.0066, Val Accuracy: 0.9981\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5416607a393433e81cec175d4c99bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 99.81%\n",
      "Accuracy of Boston: 99.88%\n",
      "Accuracy of Charlotte: 100.00%\n",
      "Accuracy of Manhattan: 99.56%\n",
      "Accuracy of Pittsburgh: 99.80%\n",
      "Performing error analysis...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef2e77c18012465790a6468dac9aa757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing errors:   0%|          | 0/626 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred during execution: iteration over a 0-d tensor\n",
      "Clearing memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/9g/r0ctqhfj26l910sgbwcdndq00000gn/T/ipykernel_91796/609564052.py\", line 13, in <module>\n",
      "    error_analysis(model, test_loader)\n",
      "  File \"/var/folders/9g/r0ctqhfj26l910sgbwcdndq00000gn/T/ipykernel_91796/4080706123.py\", line 38, in error_analysis\n",
      "    for idx in misclassified:\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py\", line 1043, in __iter__\n",
      "    raise TypeError(\"iteration over a 0-d tensor\")\n",
      "TypeError: iteration over a 0-d tensor\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Train the model\n",
    "        print(\"Training the model...\")\n",
    "        final_accuracy = train_with_early_stopping(model, train_loader, test_loader, num_epochs)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        print(\"Evaluating on test set...\")\n",
    "        test_accuracy = evaluate_model(model, test_loader)\n",
    "        \n",
    "        # Perform error analysis\n",
    "        print(\"Performing error analysis...\")\n",
    "        error_analysis(model, test_loader)\n",
    "        \n",
    "        # Compare with baseline\n",
    "        print(\"Comparing with baseline model...\")\n",
    "        compare_with_baseline(train_loader, test_loader, test_accuracy)\n",
    "        \n",
    "        # Predict on a new image\n",
    "        print(f\"Predicting on new image: {new_image_path}\")\n",
    "        new_image_probabilities, new_image_class = predict_image(model, new_image_path, transform)\n",
    "        \n",
    "        # Print and save predictions\n",
    "        predictions = [f\"{class_names[i]}: {prob:.2f}\" for i, prob in enumerate(new_image_probabilities)]\n",
    "        print(f\"Predictions for {new_image_path}:\")\n",
    "        print(f\"Predicted class: {class_names[new_image_class]}\")\n",
    "        print(\"Class probabilities:\")\n",
    "        for pred in predictions:\n",
    "            print(pred)\n",
    "        \n",
    "        with open(predictions_output_file, 'w') as f:\n",
    "            f.write(f\"Predictions for {new_image_path}:\\n\")\n",
    "            f.write(f\"Predicted class: {class_names[new_image_class]}\\n\")\n",
    "            f.write(\"Class probabilities:\\n\")\n",
    "            for pred in predictions:\n",
    "                f.write(f\"{pred}\\n\")\n",
    "        \n",
    "        print(f\"Predictions saved to {predictions_output_file}\")\n",
    "        print(\"Script execution completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    finally:\n",
    "        # Clear memory\n",
    "        print(\"Clearing memory...\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
