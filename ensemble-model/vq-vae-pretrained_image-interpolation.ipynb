{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters saved to vqgan-output/vqgan_50-steps/parameters.json\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading VQGAN model: 100%|██████████| 1/1 [00:00<00:00, 48.04it/s]\n",
      "Loading and preprocessing images: 100%|██████████| 4/4 [00:00<00:00, 236.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of latents: torch.Size([4, 128, 32, 32])\n",
      "Interpolating between latents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Interpolating and Decoding: 100%|██████████| 50/50 [00:06<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation complete. 50 images generated and saved in vqgan-output/vqgan_50-steps\n",
      "Plotting results...\n",
      "Plot saved to vqgan-output/vqgan_50-steps/interpolation_steps.png\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import gc\n",
    "\n",
    "# Parameters\n",
    "model_id = \"taming-transformers/vqgan_imagenet_f16_16384\"\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "num_steps = 50  # increase for smoother interpolation\n",
    "identifier = f\"vqgan_{num_steps}-steps\"  # Unique identifier for this run\n",
    "output_dir = os.path.join(\"vqgan-output\", identifier)\n",
    "output_image_size = (128, 128)  # Reduced size to save memory\n",
    "max_image_dimension = 512  # Reduced maximum dimension for resizing images\n",
    "batch_size = 2  # Process images in smaller batches\n",
    "\n",
    "# Create a dictionary with all relevant parameters\n",
    "params = {\n",
    "    \"identifier\": identifier,\n",
    "    \"model_id\": model_id,\n",
    "    \"num_steps\": num_steps,\n",
    "    \"output_image_size\": output_image_size,\n",
    "    \"max_image_dimension\": max_image_dimension,\n",
    "    \"batch_size\": batch_size\n",
    "}\n",
    "\n",
    "# Define the path to save the JSON file\n",
    "params_path = os.path.join(output_dir, \"parameters.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the parameters to a JSON file\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(f\"Parameters saved to {params_path}\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation.\"\"\"\n",
    "    omega = torch.acos((low/torch.norm(low, dim=1, keepdim=True) * high/torch.norm(high, dim=1, keepdim=True)).sum(1)).unsqueeze(1)\n",
    "    so = torch.sin(omega)\n",
    "    return (torch.sin((1.0-val)*omega) / so) * low + (torch.sin(val*omega) / so) * high\n",
    "\n",
    "class VQModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(128, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "def load_vqgan_model():\n",
    "    model = VQModel().to(device)\n",
    "    return model.eval()\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    s = min(image.size)\n",
    "    \n",
    "    transform = Compose([\n",
    "        Resize(s),\n",
    "        CenterCrop(s),\n",
    "        Resize(output_image_size),\n",
    "        ToTensor(),\n",
    "        Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "def decode_latents(model, latents):\n",
    "    latents = latents.to(device)\n",
    "    return model.decode(latents)\n",
    "\n",
    "def process_in_batches(model, images, batch_size):\n",
    "    latents = []\n",
    "    for i in range(0, len(images), batch_size):\n",
    "        batch = torch.cat(images[i:i+batch_size]).to(device)\n",
    "        with torch.no_grad():\n",
    "            latent = model.encode(batch)\n",
    "        latents.append(latent.cpu())  # Move latents to CPU to save GPU memory\n",
    "        del batch\n",
    "        torch.cuda.empty_cache()\n",
    "    return latents\n",
    "\n",
    "try:\n",
    "    # Load the VQGAN model\n",
    "    with tqdm(total=1, desc=\"Loading VQGAN model\") as pbar:\n",
    "        model = load_vqgan_model()\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image = preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Encode images to latents\n",
    "    latents = process_in_batches(model, images, batch_size)\n",
    "    latents = torch.cat(latents)\n",
    "    \n",
    "    print(f\"Shapes of latents: {latents.shape}\")\n",
    "\n",
    "    # Interpolate between latents\n",
    "    print(\"Interpolating between latents...\")\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated_images = []\n",
    "    \n",
    "    with tqdm(total=num_steps, desc=\"Interpolating and Decoding\") as pbar:\n",
    "        for alpha in alphas:\n",
    "            # Interpolate between the first and last latent\n",
    "            interpolated_latent = slerp(alpha, latents[0].unsqueeze(0), latents[-1].unsqueeze(0))\n",
    "            interpolated_latent = interpolated_latent.to(device)\n",
    "            with torch.no_grad():\n",
    "                # Decode the latents\n",
    "                decoded_image = decode_latents(model, interpolated_latent)\n",
    "                # Normalize the decoded image\n",
    "                decoded_image = (decoded_image.clamp(-1, 1) + 1) / 2\n",
    "                # Convert to CPU and then to numpy array\n",
    "                decoded_image = decoded_image.cpu().squeeze(0).permute(1, 2, 0).numpy()\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "            \n",
    "            interpolated_images.append(decoded_image_pil)\n",
    "            \n",
    "            # Save the interpolated image\n",
    "            output_path = os.path.join(output_dir, f\"interpolated_{len(interpolated_images)}.png\")\n",
    "            decoded_image_pil.save(output_path, quality=95)\n",
    "            \n",
    "            pbar.update(1)\n",
    "            \n",
    "            # Clear some memory\n",
    "            del interpolated_latent, decoded_image\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"Interpolation complete. {num_steps} images generated and saved in {output_dir}\")\n",
    "\n",
    "    # Plot results and save the plot as an image file\n",
    "    print(\"Plotting results...\")\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(20, 4))\n",
    "    for ax, img in zip(axes, interpolated_images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plot_path = os.path.join(output_dir, \"interpolation_steps.png\")\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.close(fig)  # Close the figure to free up memory\n",
    "    print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
