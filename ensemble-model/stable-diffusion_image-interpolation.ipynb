{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPImageProcessor  # Updated import\n",
    "\n",
    "# Parameters\n",
    "MODEL_ID = \"Jiali/stable-diffusion-1.5\"\n",
    "IMAGE_PATHS = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "NUM_STEPS = 20\n",
    "INFERENCE_STEPS = 10000\n",
    "IDENTIFIER = f\"stable-diffusion-1-5_4-images_{NUM_STEPS}-steps_{INFERENCE_STEPS}-inference\"  # Unique identifier for this run\n",
    "OUTPUT_DIR = os.path.join(\"diffusion-output\", IDENTIFIER)\n",
    "OUTPUT_IMAGE_SIZE = (512, 512)  # width, height\n",
    "MAX_IMAGE_DIMENSION = 768  # Maximum dimension for resizing images\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for 4D tensors.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low_2d + \\\n",
    "          (torch.sin(val * omega) / so).unsqueeze(1) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8 for the VAE)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], MAX_IMAGE_DIMENSION)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], MAX_IMAGE_DIMENSION)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "try:\n",
    "    # Load the pre-trained Stable Diffusion model with DDIM scheduler\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(MODEL_ID, scheduler=scheduler, torch_dtype=torch.float32)\n",
    "        pipe = pipe.to(device)\n",
    "        if device.type == \"cuda\":\n",
    "            pipe.enable_attention_slicing()\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    sizes = []\n",
    "    original_sizes = []\n",
    "    with tqdm(total=len(IMAGE_PATHS), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in IMAGE_PATHS:\n",
    "            image, size, original_size = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            sizes.append(size)\n",
    "            original_sizes.append(original_size)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Processed sizes: {sizes}\")\n",
    "    print(f\"Original sizes: {original_sizes}\")\n",
    "\n",
    "    # Encode images to latents\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "\n",
    "    # Interpolate between latents\n",
    "    print(\"Interpolating between latents...\")\n",
    "    alphas = np.linspace(0, 1, NUM_STEPS)\n",
    "    interpolated_images = []\n",
    "    \n",
    "    with tqdm(total=NUM_STEPS, desc=\"Interpolating and Decoding\") as pbar:\n",
    "        for alpha in alphas:\n",
    "            # Interpolate between the first and last latent\n",
    "            interpolated_latent = slerp(alpha, latents[0], latents[-1])\n",
    "            with torch.no_grad():\n",
    "                # Scale the latents\n",
    "                interpolated_latent = 1 / 0.18215 * interpolated_latent\n",
    "                # Decode the latents\n",
    "                decoded_image = pipe.vae.decode(interpolated_latent).sample\n",
    "                # Normalize the decoded image\n",
    "                decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "                # Convert to CPU and then to numpy array\n",
    "                decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "            # Resize the output image to the desired output size\n",
    "            decoded_image_pil = decoded_image_pil.resize(OUTPUT_IMAGE_SIZE, Image.LANCZOS)\n",
    "            \n",
    "            interpolated_images.append(decoded_image_pil)\n",
    "            \n",
    "            # Save the interpolated image\n",
    "            os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"interpolated_{len(interpolated_images)}.png\")\n",
    "            decoded_image_pil.save(output_path, quality=95)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Interpolation complete. {NUM_STEPS} images generated and saved in {OUTPUT_DIR}\")\n",
    "\n",
    "    # Plot results\n",
    "    print(\"Plotting results...\")\n",
    "    fig, axes = plt.subplots(1, NUM_STEPS, figsize=(20, 4))\n",
    "    for ax, img in zip(axes, interpolated_images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
