{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87850f54552f4badbaeb4a0d240aba58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6102ea997f6f4680bc8e0009c2f4d384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  12%|#1        | 398M/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f23f031e52448198e6874a75cfa60cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:  47%|####7     | 157M/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa98ed07a144447d9cbe3ceaca4c0676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  82%|########1 | 996M/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2633e1a3c4a4195a967d3ae998ffbf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and transforming images...\n",
      "An error occurred: [Errno 89] Operation canceled\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device to MPS if available, otherwise fall back to CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Load the pre-trained Stable Diffusion model\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\" \n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16 if device.type == \"cuda\" else torch.float32)\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    # Enable attention slicing to reduce memory usage\n",
    "    pipe.enable_attention_slicing()\n",
    "\n",
    "    # Define transformations for input images\n",
    "    transform = Compose([\n",
    "        Resize((512, 512)),  # Standard size for Stable Diffusion\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "\n",
    "    # Function to load and transform images\n",
    "    def load_image(image_path):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Load images (update these paths)\n",
    "    image1_path = \"../data/ma-boston/parcels/parcels_50.jpg\" \n",
    "    image2_path = \"../data/pa-pittsburgh/parcels/parcels_263.jpg\"\n",
    "\n",
    "    print(\"Loading and transforming images...\")\n",
    "    image1 = load_image(image1_path)\n",
    "    image2 = load_image(image2_path)\n",
    "\n",
    "    # Encode images to latents\n",
    "    print(\"Encoding images to latent representations...\")\n",
    "    with torch.no_grad():\n",
    "        latents1 = pipe.vae.encode(image1).latent_dist.sample()\n",
    "        latents2 = pipe.vae.encode(image2).latent_dist.sample()\n",
    "\n",
    "    # Interpolate between latents\n",
    "    print(\"Interpolating between latents...\")\n",
    "    num_steps = 5\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated_images = []\n",
    "    \n",
    "    # Initialize the progress bar\n",
    "    progress_bar = tqdm(total=num_steps, desc=\"Interpolating Latents\")\n",
    "\n",
    "    for idx, alpha in enumerate(alphas):\n",
    "        print(f\"Interpolating step {idx + 1}/{num_steps}...\")\n",
    "        interpolated_latent = (1 - alpha) * latents1 + alpha * latents2\n",
    "        with torch.no_grad():\n",
    "            decoded_image = pipe.vae.decode(interpolated_latent).sample[0]\n",
    "        interpolated_images.append(decoded_image)\n",
    "        \n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Close the progress bar\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Plot results\n",
    "    print(\"Plotting results...\")\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(15, 3))\n",
    "    for ax, img in zip(axes, interpolated_images):\n",
    "        ax.imshow(img.permute(1, 2, 0).cpu().numpy().clip(0, 1))\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
