{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPImageProcessor\n",
    "import json\n",
    "\n",
    "# Parameters\n",
    "model_id = \"Jiali/stable-diffusion-1.5\"\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "num_steps = 50 # increase for smoother interpolation\n",
    "inference_steps = 15000 # increase for better image quality\n",
    "identifier = f\"stable-diffusion-1-5_4-images_{num_steps}-steps_{inference_steps}-inference\"  # Unique identifier for this run\n",
    "output_dir = os.path.join(\"diffusion-output\", identifier)\n",
    "output_image_size = (512, 512)  # width and height of output image\n",
    "max_image_dimension = 1024  # Maximum dimension for resizing images, previously at 768\n",
    "\n",
    "# Create a dictionary with all relevant parameters\n",
    "params = {\n",
    "    \"identifier\": identifier,\n",
    "    \"model_id\": model_id,\n",
    "    \"num_steps\": num_steps,\n",
    "    \"inference_steps\": inference_steps,\n",
    "    \"output_image_size\": output_image_size,\n",
    "    \"max_image_dimension\": max_image_dimension\n",
    "}\n",
    "\n",
    "# Define the path to save the JSON file\n",
    "params_path = os.path.join(output_dir, \"parameters.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the parameters to a JSON file\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(f\"Parameters saved to {params_path}\")\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for 4D tensors.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low_2d + \\\n",
    "          (torch.sin(val * omega) / so).unsqueeze(1) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8 for the VAE)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float16)  # Ensure consistent data type\n",
    "    \n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "try:\n",
    "    # Load the pre-trained Stable Diffusion model with DDIM scheduler\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)  # Use mixed precision\n",
    "        pipe = pipe.to(device)\n",
    "        if device.type == \"cuda\":\n",
    "            pipe.enable_attention_slicing()\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    sizes = []\n",
    "    original_sizes = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image, size, original_size = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            sizes.append(size)\n",
    "            original_sizes.append(original_size)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Processed sizes: {sizes}\")\n",
    "    print(f\"Original sizes: {original_sizes}\")\n",
    "\n",
    "    # Encode images to latents\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "\n",
    "    # Interpolate between latents\n",
    "    print(\"Interpolating between latents...\")\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated_images = []\n",
    "    \n",
    "    with tqdm(total=num_steps, desc=\"Interpolating and Decoding\") as pbar:\n",
    "        for alpha in alphas:\n",
    "            # Interpolate between the first and last latent\n",
    "            interpolated_latent = slerp(alpha, latents[0], latents[-1])\n",
    "            with torch.no_grad():\n",
    "                # Scale the latents\n",
    "                interpolated_latent = 1 / 0.18215 * interpolated_latent\n",
    "                # Decode the latents\n",
    "                decoded_image = pipe.vae.decode(interpolated_latent).sample\n",
    "                # Normalize the decoded image\n",
    "                decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "                # Convert to CPU and then to numpy array\n",
    "                decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "            \n",
    "            # Convert to PIL Image\n",
    "            decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "            # Resize the output image to the desired output size\n",
    "            decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "            \n",
    "            interpolated_images.append(decoded_image_pil)\n",
    "            \n",
    "            # Save the interpolated image\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, f\"interpolated_{len(interpolated_images)}.png\")\n",
    "            decoded_image_pil.save(output_path, quality=95)\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Interpolation complete. {num_steps} images generated and saved in {output_dir}\")\n",
    "\n",
    "    # Plot results and save the plot as an image file\n",
    "    print(\"Plotting results...\")\n",
    "    fig, axes = plt.subplots(1, num_steps, figsize=(20, 4))\n",
    "    for ax, img in zip(axes, interpolated_images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    # Save the plot to a file\n",
    "    plot_path = os.path.join(output_dir, \"interpolation_steps.png\")\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
