{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPImageProcessor\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Custom Dataset Class to Load Data from Multiple Folders\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_folders, transform=None):\n",
    "        self.image_paths = []\n",
    "        self.transform = transform\n",
    "        for folder in image_folders:\n",
    "            for filename in os.listdir(folder):\n",
    "                if filename.endswith(\".png\") or filename.endswith(\".jpg\"):\n",
    "                    self.image_paths.append(os.path.join(folder, filename))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Parameters\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "image_folders = [\n",
    "    \"../data/ma-boston/parcel\",\n",
    "    \"../data/nc-charlotte/parcel\",\n",
    "    \"../data/ny-manhattan/parcel\",\n",
    "    \"../data/pa-pittsburgh/parcel\"\n",
    "]\n",
    "batch_size = 4\n",
    "num_steps = 50  # increase for smoother interpolation\n",
    "num_epochs = 3  # fine-tuning for 3 epochs\n",
    "learning_rate = 5e-6\n",
    "output_image_size = (512, 512)  # width and height of output image\n",
    "max_image_dimension = 1024  # Maximum dimension for resizing images\n",
    "\n",
    "# Define the path to save the model\n",
    "output_dir = os.path.join(\"fine-tuned-output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the image transform\n",
    "transform = Compose([\n",
    "    Resize((max_image_dimension, max_image_dimension)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImageDataset(image_folders, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set device: Add MPS support\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using Apple MPS for training.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for training.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for training.\")\n",
    "\n",
    "# Load pre-trained Stable Diffusion model\n",
    "with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "    scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "    pipe = pipe.to(device)\n",
    "    if device.type == \"cuda\":\n",
    "        pipe.enable_attention_slicing()  # Optimize memory on CUDA\n",
    "    pbar.update(1)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = AdamW(pipe.unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for images in tqdm(dataloader, desc=f\"Fine-tuning\"):\n",
    "        images = images.to(device).to(torch.float16)\n",
    "\n",
    "        # Encode the images to latents\n",
    "        latents = pipe.vae.encode(images).latent_dist.sample() * 0.18215\n",
    "        \n",
    "        # Generate noise and forward pass through the model\n",
    "        noise = torch.randn_like(latents).to(device)\n",
    "        latents_noisy = scheduler.add_noise(latents, noise, torch.tensor([0.1]).to(device))\n",
    "\n",
    "        # Calculate loss (using L2 loss as an example)\n",
    "        optimizer.zero_grad()\n",
    "        latent_pred = pipe.unet(latents_noisy, noise)\n",
    "        loss = torch.nn.functional.mse_loss(latent_pred, latents_noisy)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} complete. Loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "pipe.save_pretrained(output_dir)\n",
    "print(f\"Fine-tuning complete. Model saved to {output_dir}\")\n",
    "\n",
    "# Now, you can use the fine-tuned model for image generation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
