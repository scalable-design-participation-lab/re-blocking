{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data generation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "from tqdm.notebook import tqdm\n",
    "import signal\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "import glob\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory for saving checkpoints and logs\n",
    "save_dir = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Ensemble Model/Step 1 outputs'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Directories\n",
    "input_dir1_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ma-boston/buildings'\n",
    "input_dir1_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ma-boston/parcels'\n",
    "output_dir1 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/ma-boston-p2p-500-150-v100/web-boston/images'\n",
    "input_dir2_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/nc-charlotte/buildings'\n",
    "input_dir2_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/nc-charlotte/parcels'\n",
    "output_dir2 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/nc-charlotte-500-150-v100/web-charlotte/images'\n",
    "input_dir3_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ny-manhattan/buildings'\n",
    "input_dir3_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/ny-manhattan/parcels'\n",
    "output_dir3 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/ny-manhattan-p2p-500-150-v100/web-manhattan/images'\n",
    "input_dir4_original = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/pa-pittsburgh/buildings'\n",
    "input_dir4_target = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Generated Images/pa-pittsburgh/parcels'\n",
    "output_dir4 = '/Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawing Participation/Million Neighborhoods/Trained Models/pa-pittsburgh-p2p-500-150-v100/web-pittsburgh/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3514f4c7cd6f4c7a83f581607f98f8fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading images from /Users/ls/Library/CloudStorage/GoogleDrive-l.schrage@northeastern.edu/Shared drives/Drawinâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TimeoutError",
     "evalue": "Image loading timed out",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mload_images_with_checkpointing\u001b[0;34m(input_dir, keyword, checkpoint_file, target_size, max_workers)\u001b[0m\n\u001b[1;32m    113\u001b[0m future_to_path \u001b[38;5;241m=\u001b[39m {executor\u001b[38;5;241m.\u001b[39msubmit(load_single_image, path, transform): path \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m img_paths}\n\u001b[0;32m--> 114\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading images from \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/notebook.py:250\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 250\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:243\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    240\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    241\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 243\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage loading timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Image loading timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(loaded_images) \u001b[38;5;28;01mif\u001b[39;00m loaded_images \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, target_size[\u001b[38;5;241m0\u001b[39m], target_size[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Load original and target input images with checkpointing\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m input_images_model1_original \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_with_checkpointing\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir1_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcheckpoint_model1_original.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m input_images_model1_target \u001b[38;5;241m=\u001b[39m load_images_with_checkpointing(input_dir1_target, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_model1_target.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    130\u001b[0m input_images_model2_original \u001b[38;5;241m=\u001b[39m load_images_with_checkpointing(input_dir2_original, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_model2_original.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 112\u001b[0m, in \u001b[0;36mload_images_with_checkpointing\u001b[0;34m(input_dir, keyword, checkpoint_file, target_size, max_workers)\u001b[0m\n\u001b[1;32m    109\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(input_dir)) \u001b[38;5;28;01mif\u001b[39;00m keyword \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[1;32m    110\u001b[0m img_paths \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m filenames][\u001b[38;5;28mlen\u001b[39m(loaded_images):]  \u001b[38;5;66;03m# Skip already loaded images\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfuture_to_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_single_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg_paths\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfuture_to_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimg_paths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLoading images from \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43minput_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1119\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1119\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:1139\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1140\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "Cell \u001b[0;32mIn[3], line 30\u001b[0m, in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage loading timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Image loading timed out"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/homebrew/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1898, in _run_once\n",
      "    event_list = self._selector.select(timeout)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/selectors.py\", line 566, in select\n",
      "    kev_list = self._selector.control(None, max_ev, timeout)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/9g/r0ctqhfj26l910sgbwcdndq00000gn/T/ipykernel_28156/1728617154.py\", line 30, in handler\n",
      "    raise TimeoutError(\"Image loading timed out\")\n",
      "TimeoutError: Image loading timed out\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Setup logging\n",
    "log_file = os.path.join(save_dir, 'image_loading.log')\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define a custom dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, original_images, target_images, output_images, labels, transform=None):\n",
    "        self.original_images = original_images\n",
    "        self.target_images = target_images\n",
    "        self.output_images = output_images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        original_image = self.original_images[idx]\n",
    "        target_image = self.target_images[idx]\n",
    "        output_image = self.output_images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            original_image = self.transform(original_image)\n",
    "            target_image = self.transform(target_image)\n",
    "            output_image = self.transform(output_image)\n",
    "        return original_image, target_image, output_image, label\n",
    "\n",
    "# Define a timeout handler\n",
    "def handler(signum, frame):\n",
    "    raise TimeoutError(\"Image loading timed out\")\n",
    "\n",
    "# Set the signal handler and a 20-second alarm\n",
    "signal.signal(signal.SIGALRM, handler)\n",
    "\n",
    "# Function to load a single image with retry mechanism\n",
    "def load_single_image(img_path, transform, retries=3):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            signal.alarm(20)  # Trigger alarm in 20 seconds\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = transform(img)\n",
    "            signal.alarm(0)  # Disable the alarm\n",
    "            return img\n",
    "        except (UnidentifiedImageError, TimeoutError, Exception) as e:\n",
    "            logging.warning(f\"Cannot process image file {img_path}, attempt {attempt+1}/{retries}. Error: {e}\")\n",
    "            signal.alarm(0)  # Disable the alarm in case of an exception\n",
    "            attempt += 1\n",
    "    logging.error(f\"Failed to load image {img_path} after {retries} attempts, skipping.\")\n",
    "    return None\n",
    "\n",
    "# Function to remove old checkpoints, keeping only the latest two\n",
    "def remove_old_checkpoints(checkpoint_pattern):\n",
    "    checkpoints = sorted(glob.glob(checkpoint_pattern), key=os.path.getmtime)\n",
    "    while len(checkpoints) > 2:\n",
    "        old_checkpoint = checkpoints.pop(0)\n",
    "        os.remove(old_checkpoint)\n",
    "        logging.info(f\"Removed old checkpoint: {old_checkpoint}\")\n",
    "\n",
    "# Function to load images from a directory, filtering by a keyword, with parallel processing\n",
    "def load_images_from_directory(directory, keyword=None, target_size=(256, 256), max_workers=8):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    images = []\n",
    "    filenames = [f for f in sorted(os.listdir(directory)) if keyword is None or keyword in f]\n",
    "    img_paths = [os.path.join(directory, f) for f in filenames]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_path = {executor.submit(load_single_image, path, transform): path for path in img_paths}\n",
    "        for i, future in enumerate(tqdm(as_completed(future_to_path, timeout=30), total=len(img_paths), desc=f\"Loading images from {directory}\")):\n",
    "            try:\n",
    "                img = future.result()\n",
    "                if img is not None:\n",
    "                    images.append(img)\n",
    "                if (i + 1) % 250 == 0:  # Save progress every 250 images\n",
    "                    checkpoint_file = os.path.join(save_dir, f'checkpoint_{directory.replace(\"/\", \"_\")}_{i+1}.pt')\n",
    "                    torch.save(images, checkpoint_file)\n",
    "                    logging.info(f\"Checkpoint saved: {checkpoint_file}\")\n",
    "                    # Remove old checkpoints\n",
    "                    remove_old_checkpoints(os.path.join(save_dir, f'checkpoint_{directory.replace(\"/\", \"_\")}*.pt'))\n",
    "            except TimeoutError:\n",
    "                logging.error(f\"Timeout error for image {future_to_path[future]}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing image {future_to_path[future]}: {e}\")\n",
    "    \n",
    "    return torch.stack(images) if images else torch.empty((0, 3, target_size[0], target_size[1]))\n",
    "\n",
    "# Load images with checkpointing\n",
    "def load_images_with_checkpointing(input_dir, keyword, checkpoint_file, target_size=(256, 256), max_workers=8):\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        print(f\"Loading from checkpoint: {checkpoint_file}\")\n",
    "        loaded_images = torch.load(checkpoint_file)\n",
    "    else:\n",
    "        loaded_images = []\n",
    "    \n",
    "    def save_checkpoint(images, index):\n",
    "        torch.save(images, checkpoint_file)\n",
    "        checkpoint_pattern = os.path.join(save_dir, f'checkpoint_{os.path.basename(checkpoint_file)}_{index}.pt')\n",
    "        torch.save(images, checkpoint_pattern)\n",
    "        remove_old_checkpoints(os.path.join(save_dir, f'checkpoint_{os.path.basename(checkpoint_file)}*.pt'))\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(target_size),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    filenames = [f for f in sorted(os.listdir(input_dir)) if keyword is None or keyword in f]\n",
    "    img_paths = [os.path.join(input_dir, f) for f in filenames][len(loaded_images):]  # Skip already loaded images\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_path = {executor.submit(load_single_image, path, transform): path for path in img_paths}\n",
    "        for i, future in enumerate(tqdm(as_completed(future_to_path), total=len(img_paths), desc=f\"Loading images from {input_dir}\")):\n",
    "            try:\n",
    "                img = future.result()\n",
    "                if img is not None:\n",
    "                    loaded_images.append(img)\n",
    "                if (i + 1) % 250 == 0:  # Save progress every 250 images\n",
    "                    save_checkpoint(loaded_images, i + 1)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing image {future_to_path[future]}: {e}\")\n",
    "    \n",
    "    save_checkpoint(loaded_images, 'final')  # Final save\n",
    "    return torch.stack(loaded_images) if loaded_images else torch.empty((0, 3, target_size[0], target_size[1]))\n",
    "\n",
    "# Load original and target input images with checkpointing\n",
    "input_images_model1_original = load_images_with_checkpointing(input_dir1_original, '', os.path.join(save_dir, 'checkpoint_model1_original.pt'))\n",
    "input_images_model1_target = load_images_with_checkpointing(input_dir1_target, '', os.path.join(save_dir, 'checkpoint_model1_target.pt'))\n",
    "input_images_model2_original = load_images_with_checkpointing(input_dir2_original, '', os.path.join(save_dir, 'checkpoint_model2_original.pt'))\n",
    "input_images_model2_target = load_images_with_checkpointing(input_dir2_target, '', os.path.join(save_dir, 'checkpoint_model2_target.pt'))\n",
    "input_images_model3_original = load_images_with_checkpointing(input_dir3_original, '', os.path.join(save_dir, 'checkpoint_model3_original.pt'))\n",
    "input_images_model3_target = load_images_with_checkpointing(input_dir3_target, '', os.path.join(save_dir, 'checkpoint_model3_target.pt'))\n",
    "input_images_model4_original = load_images_with_checkpointing(input_dir4_original, '', os.path.join(save_dir, 'checkpoint_model4_original.pt'))\n",
    "input_images_model4_target = load_images_with_checkpointing(input_dir4_target, '', os.path.join(save_dir, 'checkpoint_model4_target.pt'))\n",
    "\n",
    "# Load only the fake_B output images with checkpointing\n",
    "output_images_model1 = load_images_with_checkpointing(output_dir1, 'fake_B', os.path.join(save_dir, 'checkpoint_model1_fake_B.pt'))\n",
    "output_images_model2 = load_images_with_checkpointing(output_dir2, 'fake_B', os.path.join(save_dir, 'checkpoint_model2_fake_B.pt'))\n",
    "output_images_model3 = load_images_with_checkpointing(output_dir3, 'fake_B', os.path.join(save_dir, 'checkpoint_model3_fake_B.pt'))\n",
    "output_images_model4 = load_images_with_checkpointing(output_dir4, 'fake_B', os.path.join(save_dir, 'checkpoint_model4_fake_B.pt'))\n",
    "\n",
    "# Create dataset and labels\n",
    "original_images = []\n",
    "target_images = []\n",
    "output_images_all = []\n",
    "labels = []\n",
    "\n",
    "# Using a progress bar for the image and label appending loop\n",
    "for i in tqdm(range(len(input_images_model1_original)), desc=\"Combining images and labels\"):\n",
    "    original_images.append(input_images_model1_original[i])\n",
    "    target_images.append(input_images_model1_target[i])\n",
    "    output_images_all.append(output_images_model1[i])\n",
    "    labels.append(0)  # Label for model 1\n",
    "    original_images.append(input_images_model2_original[i])\n",
    "    target_images.append(input_images_model2_target[i])\n",
    "    output_images_all.append(output_images_model2[i])\n",
    "    labels.append(1)  # Label for model 2\n",
    "    original_images.append(input_images_model3_original[i])\n",
    "    target_images.append(input_images_model3_target[i])\n",
    "    output_images_all.append(output_images_model3[i])\n",
    "    labels.append(2)  # Label for model 3\n",
    "    original_images.append(input_images_model4_original[i])\n",
    "    target_images.append(input_images_model4_target[i])\n",
    "    output_images_all.append(output_images_model4[i])\n",
    "    labels.append(3)  # Label for model 4\n",
    "\n",
    "# Convert dataset and labels to tensors\n",
    "original_images = torch.stack(original_images)\n",
    "target_images = torch.stack(target_images)\n",
    "output_images_all = torch.stack(output_images_all)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Save datasets to disk\n",
    "torch.save({\n",
    "    'original_images': original_images,\n",
    "    'target_images': target_images,\n",
    "    'output_images_all': output_images_all,\n",
    "    'labels': labels\n",
    "}, os.path.join(save_dir, 'dataset.pt'))\n",
    "\n",
    "# Split dataset according to specifications\n",
    "train_original = original_images[:20000]\n",
    "train_target = target_images[:20000]\n",
    "train_output = output_images_all[:20000]\n",
    "train_labels = labels[:20000]\n",
    "\n",
    "buffer_original = original_images[20000:22500]\n",
    "buffer_target = target_images[20000:22500]\n",
    "buffer_output = output_images_all[20000:22500]\n",
    "buffer_labels = labels[20000:22500]\n",
    "\n",
    "test_original = original_images[22500:23750]\n",
    "test_target = target_images[22500:23750]\n",
    "test_output = output_images_all[22500:23750]\n",
    "test_labels = labels[22500:23750]\n",
    "\n",
    "val_original = original_images[23750:25000]\n",
    "val_target = target_images[23750:25000]\n",
    "val_output = output_images_all[23750:25000]\n",
    "val_labels = labels[23750:25000]\n",
    "\n",
    "# Save splits to disk\n",
    "torch.save({\n",
    "    'train_original': train_original,\n",
    "    'train_target': train_target,\n",
    "    'train_output': train_output,\n",
    "    'train_labels': train_labels,\n",
    "    'buffer_original': buffer_original,\n",
    "    'buffer_target': buffer_target,\n",
    "    'buffer_output': buffer_output,\n",
    "    'buffer_labels': buffer_labels,\n",
    "    'test_original': test_original,\n",
    "    'test_target': test_target,\n",
    "    'test_output': test_output,\n",
    "    'test_labels': test_labels,\n",
    "    'val_original': val_original,\n",
    "    'val_target': val_target,\n",
    "    'val_output': val_output,\n",
    "    'val_labels': val_labels\n",
    "}, os.path.join(save_dir, 'dataset_splits.pt'))\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32 # 16, 32, 64â€¦\n",
    "train_dataset = ImageDataset(train_original, train_target, train_output, train_labels, transform=transforms.ToTensor())\n",
    "test_dataset = ImageDataset(test_original, test_target, test_output, test_labels, transform=transforms.ToTensor())\n",
    "val_dataset = ImageDataset(val_original, val_target, val_output, val_labels, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 64 * 64, 128)  # Adjust according to the input image size\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 classes for 4 models\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 64 * 64)  # Adjust according to the input image size\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNClassifier()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Auto-encoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AutoEncoder model\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 3, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class for the autoencoder\n",
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, images, transform=None):\n",
    "        self.images = images\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, image\n",
    "\n",
    "# Create DataLoader objects for the autoencoder training\n",
    "transform = transforms.ToTensor()\n",
    "autoencoder_dataset = AutoEncoderDataset(output_images_model1 + output_images_model2 + output_images_model3 + output_images_model4, transform=transform)\n",
    "autoencoder_loader = DataLoader(autoencoder_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for images, _ in autoencoder_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(images)\n",
    "        loss = criterion(outputs, images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(autoencoder_loader):.4f}')\n",
    "\n",
    "# Save the trained autoencoder model\n",
    "torch.save(autoencoder.state_dict(), 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Combining latent vectors and reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the trained classifier model\n",
    "classifier = CNNClassifier()\n",
    "classifier.load_state_dict(torch.load('classifier.pth'))\n",
    "classifier.eval()\n",
    "\n",
    "# Load the trained autoencoder model\n",
    "autoencoder = AutoEncoder()\n",
    "autoencoder.load_state_dict(torch.load('autoencoder.pth'))\n",
    "autoencoder.eval()\n",
    "\n",
    "# Define the encoder and decoder separately for easier handling\n",
    "encoder = autoencoder.encoder\n",
    "decoder = autoencoder.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine latent vectors with weights\n",
    "def combine_latent_vectors(vectors, weights):\n",
    "    combined_vector = sum(w * v for w, v in zip(weights, vectors))\n",
    "    return combined_vector\n",
    "\n",
    "# Assume you have an input image to process\n",
    "input_image_path = '/path/to/input/image.png'\n",
    "input_image = Image.open(input_image_path).resize((256, 256))\n",
    "input_image = transforms.ToTensor()(input_image).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "\n",
    "# Get the output images from the four individual models\n",
    "output1 = model1(input_image)\n",
    "output2 = model2(input_image)\n",
    "output3 = model3(input_image)\n",
    "output4 = model4(input_image)\n",
    "\n",
    "# Pass these output images through the encoder part of the auto-encoder to get four latent vectors\n",
    "latent_vector1 = encoder(output1).detach()\n",
    "latent_vector2 = encoder(output2).detach()\n",
    "latent_vector3 = encoder(output3).detach()\n",
    "latent_vector4 = encoder(output4).detach()\n",
    "\n",
    "# Use the classifier to get the softmax weights for the four latent vectors\n",
    "with torch.no_grad():\n",
    "    weights = classifier(input_image).softmax(dim=1).squeeze()\n",
    "\n",
    "# Combine these latent vectors using the weights to get a single combined latent vector\n",
    "combined_vector = combine_latent_vectors([latent_vector1, latent_vector2, latent_vector3, latent_vector4], weights)\n",
    "\n",
    "# Pass the combined latent vector through the decoder part of the auto-encoder to get the final output image\n",
    "final_output_image = decoder(combined_vector.unsqueeze(0))\n",
    "\n",
    "# Convert the output tensor to an image format and save it\n",
    "final_output_image = final_output_image.squeeze().permute(1, 2, 0).numpy()\n",
    "final_output_image = (final_output_image * 255).astype(np.uint8)\n",
    "save_path = '/path/to/output/image.png'\n",
    "Image.fromarray(final_output_image).save(save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
