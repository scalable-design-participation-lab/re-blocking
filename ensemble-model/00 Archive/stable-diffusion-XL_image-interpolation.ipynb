{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430b04e7a044421883407cd1f90b01cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Stable Diffusion model:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10c38543a364cb98d95129669e41b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: CUDA out of memory. Tried to allocate 30.00 MiB. GPU 0 has a total capacity of 7.65 GiB of which 72.69 MiB is free. Including non-PyTorch memory, this process has 5.56 GiB memory in use. Of the allocated memory 5.25 GiB is allocated by PyTorch, and 164.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import CLIPImageProcessor\n",
    "import json\n",
    "\n",
    "# Environment variable to reduce memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Parameters\n",
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "inference_steps = 10  # Reduced steps for lower memory consumption\n",
    "identifier = f\"stable-diffusion-weighted-interpolation_{inference_steps}-inference\"\n",
    "output_dir = os.path.join(\"diffusion-output\", identifier)\n",
    "output_image_size = (256, 256)  # Reduced size to reduce memory load\n",
    "max_image_dimension = 256  # Reduce max dimension to fit GPU memory\n",
    "\n",
    "# Example softmax weights (these should sum to 1)\n",
    "weights = [0.25, 0.35, 0.15, 0.25]  # Replace with your actual weights\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = image.size\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    transform = Compose([Resize(new_size), ToTensor(), Normalize([0.5], [0.5])])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float16)\n",
    "    \n",
    "    return image_tensor\n",
    "\n",
    "try:\n",
    "    with tqdm(total=1, desc=\"Loading Stable Diffusion model\") as pbar:\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=scheduler, torch_dtype=torch.float16)\n",
    "        pipe = pipe.to(device)\n",
    "        pipe.enable_attention_slicing(\"max\")  # Maximize attention slicing\n",
    "        pipe.enable_model_cpu_offload()  # Offload unused parts of the model to CPU\n",
    "        pbar.update(1)\n",
    "\n",
    "    images = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            pbar.update(1)\n",
    "\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = pipe.vae.encode(image).latent_dist.sample() * 0.18215\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "    weighted_latent = sum(w * latent for w, latent in zip(weights, latents))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        weighted_latent = 1 / 0.18215 * weighted_latent\n",
    "        decoded_image = pipe.vae.decode(weighted_latent).sample\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "    output_path = os.path.join(output_dir, \"weighted_interpolated_image.png\")\n",
    "    decoded_image_pil.save(output_path, quality=95)\n",
    "\n",
    "    print(f\"Weighted interpolation image saved at: {output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
