{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "pth_path = '/home/ls/sites/re-blocking/ensemble-model/models/vq-vae_16-80000.pth'\n",
    "state_dict = torch.load(pth_path, map_location='cpu')\n",
    "print(list(state_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded 1000 image predictions\n",
      "Example filenames in predictions:\n",
      "  parcels_000252.jpg\n",
      "  parcels_000556.jpg\n",
      "  parcels_000804.jpg\n",
      "  parcels_000474.jpg\n",
      "  parcels_000729.jpg\n",
      "Created mapping for 1000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading VQ-VAE model: 100%|██████████| 1/1 [00:00<00:00, 70.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 image numbers to process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images:  20%|█▉        | 197/1000 [00:30<01:39,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Could not find model output for Boston, image 000195\n",
      "Warning: Could not find model output for Charlotte, image 000195\n",
      "Warning: Could not find model output for Manhattan, image 000195\n",
      "Warning: Could not find model output for Pittsburgh, image 000195\n",
      "Missing some model outputs for image 000195, skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|██████████| 1000/1000 [02:35<00:00,  6.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images processed. Results saved to /home/ls/sites/re-blocking/ensemble-model/ensemble-output/vqvae\n",
      "Weight visualizations saved to /home/ls/sites/re-blocking/ensemble-model/ensemble-output/vqvae-weights-visuals\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# VQ-VAE Image Interpolation (Weighted based on Classifier Predictions) for the Building-to-Parcel Workflow\n",
    "# Leonard Schrage, l.schrage@northeastern.edu / lschrage@mit.edu, 2024-25\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "###############################################################################\n",
    "#                         CONFIGURABLE PARAMETERS                             #\n",
    "###############################################################################\n",
    "\n",
    "# ------------------ Model & Data Paths ------------------ #\n",
    "vqvae_model_path = \"/home/ls/sites/re-blocking/ensemble-model/models/vq-vae_16-80000.pth\"\n",
    "model_outputs = {\n",
    "    \"Boston\": \"/home/ls/sites/re-blocking/data/results/ma-boston-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Charlotte\": \"/home/ls/sites/re-blocking/data/results/nc-charlotte-200-150-v100/test_latest/images\",\n",
    "    \"Manhattan\": \"/home/ls/sites/re-blocking/data/results/ny-manhattan-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Pittsburgh\": \"/home/ls/sites/re-blocking/data/results/pa-pittsburgh-p2p-500-150-v100/test_latest/images\"\n",
    "}\n",
    "predictions_file = \"/home/ls/sites/re-blocking/ensemble-model/softmax-output/city-predictions/run_20250319_131844/brooklyn/predictions.json\"\n",
    "output_base_dir = \"/home/ls/sites/re-blocking/ensemble-model/ensemble-output/vqvae\"\n",
    "weights_viz_dir = \"/home/ls/sites/re-blocking/ensemble-model/ensemble-output/vqvae-weights-visuals\"\n",
    "\n",
    "# ------------------ Output Settings ------------------ #\n",
    "output_image_size = (512, 512)\n",
    "max_image_dimension = 1024\n",
    "output_format = \"jpg\"\n",
    "image_quality = 90  # JPEG quality\n",
    "\n",
    "# ------------------ VQ-VAE Architecture Parameters ------------------ #\n",
    "in_channels = 3\n",
    "hidden_channels = 64\n",
    "num_embeddings = 1024\n",
    "embedding_dim = 64\n",
    "commitment_cost = 0.25\n",
    "\n",
    "###############################################################################\n",
    "#                          SETUP & UTILITY FUNCTIONS                          #\n",
    "###############################################################################\n",
    "\n",
    "# Ensure the output directories exist\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "os.makedirs(weights_viz_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Improved spherical linear interpolation for tensors with better handling of edge cases.\"\"\"\n",
    "    # Reshape tensors to 2D: (batch, -1)\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    \n",
    "    # Compute cosine similarity and clamp values for safety\n",
    "    dot_products = (low_norm * high_norm).sum(1)\n",
    "    dot_products = torch.clamp(dot_products, -1.0, 1.0)\n",
    "    omega = torch.acos(dot_products)\n",
    "    so = torch.sin(omega)\n",
    "    \n",
    "    # Create a mask for near-parallel vectors\n",
    "    parallel_mask = (so < 1e-8)\n",
    "    result = torch.zeros_like(low_2d)\n",
    "    \n",
    "    if parallel_mask.any():\n",
    "        result[parallel_mask] = (1.0 - val) * low_2d[parallel_mask] + val * high_2d[parallel_mask]\n",
    "    \n",
    "    if (~parallel_mask).any():\n",
    "        valid_so = so[~parallel_mask].unsqueeze(1)\n",
    "        valid_omega = omega[~parallel_mask].unsqueeze(1)\n",
    "        result[~parallel_mask] = (torch.sin((1.0 - val) * valid_omega) / valid_so) * low_2d[~parallel_mask] + \\\n",
    "                                (torch.sin(val * valid_omega) / valid_so) * high_2d[~parallel_mask]\n",
    "    \n",
    "    return result.view(low.shape)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = image.size\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float32)\n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "def re_quantize(latent, codebook):\n",
    "    \"\"\"\n",
    "    Re-quantize a continuous latent representation by finding the nearest\n",
    "    embedding vector from the codebook.\n",
    "    \"\"\"\n",
    "    B, C, H, W = latent.shape\n",
    "    flat_latent = latent.view(B, C, -1).permute(0, 2, 1)  # shape: (B, N, C)\n",
    "    flat_latent_expanded = flat_latent.unsqueeze(2)         # shape: (B, N, 1, C)\n",
    "    codebook_expanded = codebook.unsqueeze(0).unsqueeze(0)    # shape: (1, 1, num_embeddings, C)\n",
    "    \n",
    "    distances = torch.sum((flat_latent_expanded - codebook_expanded) ** 2, dim=-1)  # shape: (B, N, num_embeddings)\n",
    "    encoding_indices = distances.argmin(dim=-1)             # shape: (B, N)\n",
    "    quantized_flat = codebook[encoding_indices]             # shape: (B, N, C)\n",
    "    quantized = quantized_flat.permute(0, 2, 1).view(B, C, H, W)\n",
    "    return quantized\n",
    "\n",
    "def generate_weighted_image(model, image_latents, weights, output_path):\n",
    "    \"\"\"\n",
    "    Generate an ensemble image by combining continuous latent representations\n",
    "    using SLERP. The weighted latent is then re-quantized with the model's codebook\n",
    "    and decoded to produce the final output image.\n",
    "    \"\"\"\n",
    "    total_weight = sum(weights)\n",
    "    normalized_weights = [w / total_weight for w in weights]\n",
    "    \n",
    "    # Start with the first latent representation (continuous, from encoder)\n",
    "    weighted_latent = image_latents[0].clone()\n",
    "    # Iteratively blend remaining latents using SLERP with the normalized weights\n",
    "    for i in range(1, len(image_latents)):\n",
    "        weighted_latent = slerp(normalized_weights[i], weighted_latent, image_latents[i])\n",
    "    \n",
    "    # Re-quantize using the model's codebook\n",
    "    codebook = model.vq_layer.embedding.weight\n",
    "    quantized_latent = re_quantize(weighted_latent, codebook)\n",
    "    \n",
    "    # Decode the quantized latent\n",
    "    with torch.no_grad():\n",
    "        decoded_image = model.decoder(quantized_latent)\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    \n",
    "    # Convert to PIL image, resize, and save\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "    decoded_image_pil.save(output_path, \"JPEG\", quality=image_quality)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def extract_image_number(filename):\n",
    "    \"\"\"Extract the image number from a filename.\"\"\"\n",
    "    patterns = [\n",
    "        r'parcels_(\\d+)',\n",
    "        r'_(\\d+)_',\n",
    "        r'(\\d+)_fake_B',\n",
    "        r'(\\d+)\\.jpg',\n",
    "        r'(\\d+)\\.png'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1).zfill(6)\n",
    "    print(f\"Could not extract number from {filename}\")\n",
    "    return None\n",
    "\n",
    "def create_mapping(predictions):\n",
    "    \"\"\"Create a mapping from image numbers to their weights.\"\"\"\n",
    "    mapping = {}\n",
    "    if predictions and len(predictions) > 0:\n",
    "        print(\"Example filenames in predictions:\")\n",
    "        for i in range(min(5, len(predictions))):\n",
    "            print(f\"  {Path(predictions[i]['image_path']).name}\")\n",
    "    for pred in predictions:\n",
    "        image_path = pred[\"image_path\"]\n",
    "        filename = Path(image_path).name\n",
    "        image_num = extract_image_number(filename)\n",
    "        if image_num:\n",
    "            mapping[image_num] = {\n",
    "                \"Boston\": pred[\"probabilities\"][\"Boston\"],\n",
    "                \"Charlotte\": pred[\"probabilities\"][\"Charlotte\"],\n",
    "                \"Manhattan\": pred[\"probabilities\"][\"Manhattan\"],\n",
    "                \"Pittsburgh\": pred[\"probabilities\"][\"Pittsburgh\"]\n",
    "            }\n",
    "    return mapping\n",
    "\n",
    "###############################################################################\n",
    "#                           VQ-VAE MODEL DEFINITIONS                           #\n",
    "###############################################################################\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "    def forward(self, x):\n",
    "        flat_x = x.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flat_x, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.shape)\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        quantized = x + (quantized - x).detach()  # Straight-through estimator\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        return quantized\n",
    "    def decode(self, quantized):\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "def load_vqvae_model(model_path):\n",
    "    \"\"\"Load the VQ-VAE model from a checkpoint using the top-level parameters.\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model = VQVAE(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=embedding_dim,\n",
    "            commitment_cost=commitment_cost\n",
    "        )\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "###############################################################################\n",
    "#                                 MAIN SCRIPT                                  #\n",
    "###############################################################################\n",
    "\n",
    "try:\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    print(f\"Loaded {len(predictions)} image predictions\")\n",
    "    \n",
    "    number_to_weights = create_mapping(predictions)\n",
    "    print(f\"Created mapping for {len(number_to_weights)} images\")\n",
    "    \n",
    "    with tqdm(total=1, desc=\"Loading VQ-VAE model\") as pbar:\n",
    "        vqvae_model = load_vqvae_model(vqvae_model_path)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    image_numbers = sorted(list(number_to_weights.keys()))\n",
    "    print(f\"Found {len(image_numbers)} image numbers to process\")\n",
    "    \n",
    "    with tqdm(total=len(image_numbers), desc=\"Processing images\") as pbar:\n",
    "        for image_num in image_numbers:\n",
    "            try:\n",
    "                image_weights = number_to_weights[image_num]\n",
    "                city_latents = {}\n",
    "                cities = [\"Boston\", \"Charlotte\", \"Manhattan\", \"Pittsburgh\"]\n",
    "                missing_files = False\n",
    "                \n",
    "                for city in cities:\n",
    "                    try:\n",
    "                        city_dir = model_outputs[city]\n",
    "                        city_file = None\n",
    "                        for file in os.listdir(city_dir):\n",
    "                            if f\"_{image_num}_\" in file or f\"_{image_num}.\" in file:\n",
    "                                city_file = os.path.join(city_dir, file)\n",
    "                                break\n",
    "                        if city_file:\n",
    "                            image_tensor, _, _ = load_and_preprocess_image(city_file)\n",
    "                            # Get the continuous latent from the encoder (before final quantization)\n",
    "                            with torch.no_grad():\n",
    "                                latent = vqvae_model.encoder(image_tensor)\n",
    "                                city_latents[city] = latent\n",
    "                        else:\n",
    "                            print(f\"Warning: Could not find model output for {city}, image {image_num}\")\n",
    "                            missing_files = True\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {city} model output for image {image_num}: {e}\")\n",
    "                        missing_files = True\n",
    "                \n",
    "                if missing_files or len(city_latents) < len(cities):\n",
    "                    print(f\"Missing some model outputs for image {image_num}, skipping\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                latents = []\n",
    "                weights = []\n",
    "                for city in cities:\n",
    "                    latents.append(city_latents[city])\n",
    "                    weights.append(image_weights[city])\n",
    "                \n",
    "                output_filename = f\"ensemble_vqvae_{image_num}.{output_format}\"\n",
    "                output_path = os.path.join(output_base_dir, output_filename)\n",
    "                generate_weighted_image(vqvae_model, latents, weights, output_path)\n",
    "                \n",
    "                # Create and save weight visualization\n",
    "                plt.figure(figsize=(8, 5))\n",
    "                plt.bar(cities, weights, color='slateblue')\n",
    "                plt.ylim(0, 1.0)\n",
    "                plt.title(f\"Classifier Weights for Image {image_num}\")\n",
    "                plt.ylabel(\"Weight\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                weights_path = os.path.join(weights_viz_dir, f\"weights_{image_num}.png\")\n",
    "                plt.savefig(weights_path)\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_num}: {e}\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"All images processed. Results saved to {output_base_dir}\")\n",
    "    print(f\"Weight visualizations saved to {weights_viz_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
