{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VQ-VAE Image Interpolation (Weighted based on Classifier Predictions) for the Building-to-Parcel Workflow\n",
    "# Leonard Schrage, l.schrage@northeastern.edu / lschrage@mit.edu, 2024-25\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from pathlib import Path\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# Parameters with correct paths\n",
    "vqvae_model_path = \"/home/ls/sites/re-blocking/ensemble-model/models/vq-vae_16-80000.pth\"\n",
    "model_outputs = {\n",
    "    \"Boston\": \"/home/ls/sites/re-blocking/data/results/ma-boston-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Charlotte\": \"/home/ls/sites/re-blocking/data/results/nc-charlotte-200-150-v100/test_latest/images\",\n",
    "    \"Manhattan\": \"/home/ls/sites/re-blocking/data/results/ny-manhattan-p2p-200-150-v100/test_latest/images\",\n",
    "    \"Pittsburgh\": \"/home/ls/sites/re-blocking/data/results/pa-pittsburgh-p2p-500-150-v100/test_latest/images\"\n",
    "}\n",
    "predictions_file = \"/home/ls/sites/re-blocking/ensemble-model/softmax-output/city-predictions/run_20250319_131844/brooklyn/predictions.json\"\n",
    "output_base_dir = \"/home/ls/sites/re-blocking/ensemble-model/ensemble-output/vqvae\"\n",
    "output_image_size = (512, 512)\n",
    "max_image_dimension = 1024\n",
    "output_format = \"png\"\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for tensors.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    # Account for the case where vectors are parallel (omega=0)\n",
    "    if so.item() == 0:\n",
    "        return (1.0-val) * low + val * high\n",
    "    res = (torch.sin((1.0 - val) * omega) / so).unsqueeze(1) * low_2d + \\\n",
    "          (torch.sin(val * omega) / so).unsqueeze(1) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    \"\"\"Load and preprocess an image.\"\"\"\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float32)\n",
    "    \n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "def generate_weighted_image(model, image_latents, weights, output_path):\n",
    "    \"\"\"Generate a weighted image based on the specified weights.\"\"\"\n",
    "    # Start with the first latent\n",
    "    weighted_latent = image_latents[0].clone()\n",
    "    \n",
    "    # Apply SLERP interpolation with weights\n",
    "    for i in range(1, len(image_latents)):\n",
    "        weighted_latent = slerp(weights[i], weighted_latent, image_latents[i])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Decode the weighted latent\n",
    "        decoded_image = model.decode(weighted_latent)\n",
    "        # Normalize the decoded image\n",
    "        decoded_image = (decoded_image / 2 + 0.5).clamp(0, 1)\n",
    "        # Convert to CPU and then to numpy array\n",
    "        decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    \n",
    "    # Convert to PIL Image\n",
    "    decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "    # Resize the output image to the desired output size\n",
    "    decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "    \n",
    "    # Save the weighted interpolated image\n",
    "    decoded_image_pil.save(output_path)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def extract_image_number(filename):\n",
    "    \"\"\"Extract the image number from a filename.\"\"\"\n",
    "    # Try different patterns\n",
    "    patterns = [\n",
    "        r'_(\\d+)_',           # Matches _000123_\n",
    "        r'parcels_(\\d+)',      # Matches parcels_000123\n",
    "        r'(\\d+)\\.jpg',         # Matches 000123.jpg\n",
    "        r'(\\d+)\\.png'          # Matches 000123.png\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, filename)\n",
    "        if match:\n",
    "            return match.group(1).zfill(6)  # Ensure 6 digits with leading zeros\n",
    "    \n",
    "    # If no pattern matches, return None\n",
    "    print(f\"Could not extract number from {filename}\")\n",
    "    return None\n",
    "\n",
    "def create_mapping(predictions):\n",
    "    \"\"\"Create a mapping from image numbers to their weights.\"\"\"\n",
    "    mapping = {}\n",
    "    \n",
    "    # Print a few examples of the filenames to help debug\n",
    "    if predictions and len(predictions) > 0:\n",
    "        print(\"Example filenames in predictions:\")\n",
    "        for i in range(min(5, len(predictions))):\n",
    "            print(f\"  {Path(predictions[i]['image_path']).name}\")\n",
    "    \n",
    "    for pred in predictions:\n",
    "        image_path = pred[\"image_path\"]\n",
    "        filename = Path(image_path).name\n",
    "        image_num = extract_image_number(filename)\n",
    "        \n",
    "        if image_num:\n",
    "            mapping[image_num] = {\n",
    "                \"Boston\": pred[\"probabilities\"][\"Boston\"],\n",
    "                \"Charlotte\": pred[\"probabilities\"][\"Charlotte\"],\n",
    "                \"Manhattan\": pred[\"probabilities\"][\"Manhattan\"],\n",
    "                \"Pittsburgh\": pred[\"probabilities\"][\"Pittsburgh\"]\n",
    "            }\n",
    "    \n",
    "    return mapping\n",
    "\n",
    "# VQ-VAE Model definitions\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, embedding_dim, height, width]\n",
    "        # Flatten the input\n",
    "        flat_x = x.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "        # Compute distances\n",
    "        distances = torch.cdist(flat_x, self.embedding.weight)\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        # Quantize\n",
    "        quantized = self.embedding(encoding_indices).view(x.shape)\n",
    "        # Losses\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        # Straight-through estimator\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=64, num_embeddings=1024, embedding_dim=64, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        return quantized\n",
    "\n",
    "    def decode(self, quantized):\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "def load_vqvae_model(model_path):\n",
    "    # Load the VQ-VAE model\n",
    "    try:\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model = VQVAE(\n",
    "            in_channels=3,\n",
    "            hidden_channels=64,\n",
    "            num_embeddings=1024,\n",
    "            embedding_dim=64,\n",
    "            commitment_cost=0.25\n",
    "        )\n",
    "        model.load_state_dict(checkpoint)\n",
    "        model = model.to(device)\n",
    "        model.eval()\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "# Main execution\n",
    "try:\n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    # Load predictions\n",
    "    with open(predictions_file, 'r') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(predictions)} image predictions\")\n",
    "    \n",
    "    # Create mapping from image numbers to weights\n",
    "    number_to_weights = create_mapping(predictions)\n",
    "    print(f\"Created mapping for {len(number_to_weights)} images\")\n",
    "    \n",
    "    # Load the pre-trained VQ-VAE model\n",
    "    with tqdm(total=1, desc=\"Loading VQ-VAE model\") as pbar:\n",
    "        vqvae_model = load_vqvae_model(vqvae_model_path)\n",
    "        pbar.update(1)\n",
    "    \n",
    "    # Get a list of all image numbers from the predictions mapping\n",
    "    image_numbers = sorted(list(number_to_weights.keys()))\n",
    "    print(f\"Found {len(image_numbers)} image numbers to process\")\n",
    "    \n",
    "    # Process each image number\n",
    "    with tqdm(total=len(image_numbers), desc=\"Processing images\") as pbar:\n",
    "        for image_num in image_numbers:\n",
    "            try:\n",
    "                # Get weights for this image\n",
    "                image_weights = number_to_weights[image_num]\n",
    "                \n",
    "                # Get model outputs for each city\n",
    "                city_latents = {}\n",
    "                cities = [\"Boston\", \"Charlotte\", \"Manhattan\", \"Pittsburgh\"]\n",
    "                \n",
    "                for city in cities:\n",
    "                    try:\n",
    "                        city_dir = model_outputs[city]\n",
    "                        city_file = None\n",
    "                        \n",
    "                        # Try to find the corresponding file in the city directory\n",
    "                        for file in os.listdir(city_dir):\n",
    "                            if f\"_{image_num}_fake_B\" in file:\n",
    "                                city_file = os.path.join(city_dir, file)\n",
    "                                break\n",
    "                        \n",
    "                        if city_file:\n",
    "                            # Load and preprocess the image\n",
    "                            image_tensor, _, _ = load_and_preprocess_image(city_file)\n",
    "                            \n",
    "                            # Encode to latent\n",
    "                            with torch.no_grad():\n",
    "                                latent = vqvae_model.encode(image_tensor)\n",
    "                                city_latents[city] = latent\n",
    "                        else:\n",
    "                            print(f\"Warning: Could not find model output for {city}, image {image_num}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {city} model output for image {image_num}: {e}\")\n",
    "                \n",
    "                # Skip if we don't have all model outputs\n",
    "                if len(city_latents) < len(cities):\n",
    "                    print(f\"Missing some model outputs for image {image_num}, skipping\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "                \n",
    "                # Prepare latents and weights in consistent order\n",
    "                latents = []\n",
    "                weights = []\n",
    "                \n",
    "                for city in cities:\n",
    "                    latents.append(city_latents[city])\n",
    "                    weights.append(image_weights[city])\n",
    "                \n",
    "                # Generate the ensemble image\n",
    "                output_filename = f\"ensemble_vqvae_{image_num}.{output_format}\"\n",
    "                output_path = os.path.join(output_base_dir, output_filename)\n",
    "                \n",
    "                generate_weighted_image(vqvae_model, latents, weights, output_path)\n",
    "                \n",
    "                # Create visualization of weights\n",
    "                weights_viz_dir = os.path.join(output_base_dir, \"weights_viz\")\n",
    "                os.makedirs(weights_viz_dir, exist_ok=True)\n",
    "                \n",
    "                plt.figure(figsize=(8, 5))\n",
    "                plt.bar(cities, weights, color='slateblue')\n",
    "                plt.ylim(0, 1.0)\n",
    "                plt.title(f\"Classifier Weights for Image {image_num}\")\n",
    "                plt.ylabel(\"Weight\")\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.tight_layout()\n",
    "                \n",
    "                weights_path = os.path.join(weights_viz_dir, f\"weights_{image_num}.png\")\n",
    "                plt.savefig(weights_path)\n",
    "                plt.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image {image_num}: {e}\")\n",
    "            \n",
    "            pbar.update(1)\n",
    "    \n",
    "    print(f\"All images processed. Results saved to {output_base_dir}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
