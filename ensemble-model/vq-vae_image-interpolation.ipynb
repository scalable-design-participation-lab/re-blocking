{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters saved to vqvae-output/vqvae_4-images_50-steps/parameters.json\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading VQ-VAE model:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Error(s) in loading state_dict for VQVAE:\n",
      "\tMissing key(s) in state_dict: \"encoder.conv1.weight\", \"encoder.conv1.bias\", \"encoder.conv2.weight\", \"encoder.conv2.bias\", \"decoder.conv1.weight\", \"decoder.conv1.bias\", \"decoder.conv2.weight\", \"decoder.conv2.bias\", \"vq_layer.embedding.weight\". \n",
      "\tUnexpected key(s) in state_dict: \"fc_mu.weight\", \"fc_mu.bias\", \"fc_logvar.weight\", \"fc_logvar.bias\", \"decoder_input.weight\", \"decoder_input.bias\", \"encoder.0.weight\", \"encoder.0.bias\", \"encoder.2.weight\", \"encoder.2.bias\", \"encoder.4.weight\", \"encoder.4.bias\", \"decoder.0.weight\", \"decoder.0.bias\", \"decoder.2.weight\", \"decoder.2.bias\", \"decoder.4.weight\", \"decoder.4.bias\". \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm  # Use tqdm instead of tqdm.notebook if not in a notebook environment\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Parameters\n",
    "vqvae_model_path = \"models/vq-vae_16-80000.pth\"  # Update with your model path\n",
    "image_paths = [\n",
    "    \"../data/results/ma-boston_200250_fake_B.png\",\n",
    "    \"../data/results/nc-charlotte_200250_fake_B.png\",\n",
    "    \"../data/results/ny-manhattan_200250_fake_B.png\",\n",
    "    \"../data/results/pa-pittsburgh_200250_fake_B.png\"\n",
    "]\n",
    "num_steps = 50  # Increase for smoother interpolation\n",
    "identifier = f\"vqvae_4-images_{num_steps}-steps\"  # Unique identifier for this run\n",
    "output_dir = os.path.join(\"vqvae-output\", identifier)\n",
    "output_image_size = (512, 512)  # Width and height of output image\n",
    "max_image_dimension = 1024  # Maximum dimension for resizing images\n",
    "\n",
    "# Create a dictionary with all relevant parameters\n",
    "params = {\n",
    "    \"identifier\": identifier,\n",
    "    \"vqvae_model_path\": vqvae_model_path,\n",
    "    \"num_steps\": num_steps,\n",
    "    \"output_image_size\": output_image_size,\n",
    "    \"max_image_dimension\": max_image_dimension\n",
    "}\n",
    "\n",
    "# Define the path to save the JSON file\n",
    "params_path = os.path.join(output_dir, \"parameters.json\")\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the parameters to a JSON file\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(f\"Parameters saved to {params_path}\")\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def slerp(val, low, high):\n",
    "    \"\"\"Spherical linear interpolation for 4D tensors.\"\"\"\n",
    "    low_2d = low.view(low.shape[0], -1)\n",
    "    high_2d = high.view(high.shape[0], -1)\n",
    "    low_2d_norm = low_2d / torch.norm(low_2d, dim=1, keepdim=True)\n",
    "    high_2d_norm = high_2d / torch.norm(high_2d, dim=1, keepdim=True)\n",
    "    omega = torch.acos((low_2d_norm * high_2d_norm).sum(1).clamp(-1, 1))\n",
    "    so = torch.sin(omega)\n",
    "    res = (torch.sin((1.0 - val) * omega).unsqueeze(1) / so.unsqueeze(1)) * low_2d + \\\n",
    "          (torch.sin(val * omega).unsqueeze(1) / so.unsqueeze(1)) * high_2d\n",
    "    return res.view(low.shape)\n",
    "\n",
    "def load_and_preprocess_image(image_path):\n",
    "    # Open the image and convert to RGB\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Get the original size\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Calculate the aspect ratio\n",
    "    aspect_ratio = original_size[0] / original_size[1]\n",
    "    \n",
    "    # Determine the new size (ensuring it's divisible by 8)\n",
    "    if aspect_ratio > 1:\n",
    "        new_width = min(original_size[0], max_image_dimension)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "        new_height = int(new_width / aspect_ratio)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "    else:\n",
    "        new_height = min(original_size[1], max_image_dimension)\n",
    "        new_height = new_height - (new_height % 8)\n",
    "        new_width = int(new_height * aspect_ratio)\n",
    "        new_width = new_width - (new_width % 8)\n",
    "    \n",
    "    new_size = (new_width, new_height)\n",
    "    \n",
    "    # Create the transform\n",
    "    transform = Compose([\n",
    "        Resize(new_size),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Adjusted for 3 channels\n",
    "    ])\n",
    "    \n",
    "    # Apply the transform\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device).to(torch.float32)  # Use float32 for VQ-VAE\n",
    "    \n",
    "    return image_tensor, new_size, original_size\n",
    "\n",
    "# VQ-VAE Model Definitions (from your provided code)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, embedding_dim, height, width]\n",
    "        # Flatten the input\n",
    "        flat_x = x.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "        # Compute distances\n",
    "        distances = torch.cdist(flat_x, self.embedding.weight)\n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        # Quantize\n",
    "        quantized = self.embedding(encoding_indices).view(x.shape)\n",
    "        # Losses\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        # Straight-through estimator\n",
    "        quantized = x + (quantized - x).detach()\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=64, num_embeddings=1024, embedding_dim=64, commitment_cost=0.25):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def encode(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        return quantized\n",
    "\n",
    "    def decode(self, quantized):\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "def load_vqvae_model(model_path):\n",
    "    # Load your VQ-VAE model\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model = VQVAE(\n",
    "        in_channels=3,\n",
    "        hidden_channels=64,\n",
    "        num_embeddings=1024,\n",
    "        embedding_dim=64,\n",
    "        commitment_cost=0.25\n",
    "    )\n",
    "    model.load_state_dict(checkpoint)  # Load the state dict directly\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "try:\n",
    "    # Suppress the FutureWarning from torch.load\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    # Load the pre-trained VQ-VAE model\n",
    "    with tqdm(total=1, desc=\"Loading VQ-VAE model\") as pbar:\n",
    "        vqvae_model = load_vqvae_model(vqvae_model_path)\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Load and preprocess images\n",
    "    images = []\n",
    "    sizes = []\n",
    "    original_sizes = []\n",
    "    with tqdm(total=len(image_paths), desc=\"Loading and preprocessing images\") as pbar:\n",
    "        for image_path in image_paths:\n",
    "            image, size, original_size = load_and_preprocess_image(image_path)\n",
    "            images.append(image)\n",
    "            sizes.append(size)\n",
    "            original_sizes.append(original_size)\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Processed sizes: {sizes}\")\n",
    "    print(f\"Original sizes: {original_sizes}\")\n",
    "\n",
    "    # Encode images to latents using VQ-VAE\n",
    "    latents = []\n",
    "    with tqdm(total=len(images), desc=\"Encoding images to latent representations\") as pbar:\n",
    "        with torch.no_grad():\n",
    "            for image in images:\n",
    "                latent = vqvae_model.encode(image)\n",
    "                latents.append(latent)\n",
    "                pbar.update(1)\n",
    "\n",
    "    print(f\"Shapes of latents: {[latent.shape for latent in latents]}\")\n",
    "\n",
    "    # Interpolate between latents\n",
    "    print(\"Interpolating between latents...\")\n",
    "    alphas = np.linspace(0, 1, num_steps)\n",
    "    interpolated_images = []\n",
    "\n",
    "    with tqdm(total=num_steps, desc=\"Interpolating and Decoding\") as pbar:\n",
    "        for alpha in alphas:\n",
    "            # Interpolate between the first and last latent\n",
    "            interpolated_latent = slerp(alpha, latents[0], latents[-1])\n",
    "            with torch.no_grad():\n",
    "                # Decode the latents\n",
    "                decoded_image = vqvae_model.decode(interpolated_latent)\n",
    "                # Denormalize the decoded image\n",
    "                decoded_image = (decoded_image + 1) / 2  # Shift from [-1, 1] to [0, 1]\n",
    "                decoded_image = decoded_image.clamp(0, 1)\n",
    "                # Convert to CPU and then to numpy array\n",
    "                decoded_image = decoded_image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "\n",
    "            # Convert to PIL Image\n",
    "            decoded_image_pil = Image.fromarray((decoded_image * 255).astype(np.uint8))\n",
    "            # Resize the output image to the desired output size\n",
    "            decoded_image_pil = decoded_image_pil.resize(output_image_size, Image.LANCZOS)\n",
    "\n",
    "            interpolated_images.append(decoded_image_pil)\n",
    "\n",
    "            # Save the interpolated image\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            output_path = os.path.join(output_dir, f\"interpolated_{len(interpolated_images)}.png\")\n",
    "            decoded_image_pil.save(output_path, quality=95)\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "    print(f\"Interpolation complete. {num_steps} images generated and saved in {output_dir}\")\n",
    "\n",
    "    # Plot results and save the plot as an image file\n",
    "    print(\"Plotting results...\")\n",
    "    fig, axes = plt.subplots(1, len(interpolated_images), figsize=(20, 4))\n",
    "    for ax, img in zip(axes, interpolated_images):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plot_path = os.path.join(output_dir, \"interpolation_steps.png\")\n",
    "    fig.savefig(plot_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"Plot saved to {plot_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
