{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Parameters\n",
    "total_samples = 80000  # start  low for testing, should be on +50k\n",
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "commitment_cost = 0.25\n",
    "hidden_channels = 128\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "checkpoint_interval = 50\n",
    "image_size = (16, 16)  # Resize to a fixed size\n",
    "normalize_mean = (0.5,)\n",
    "normalize_std = (0.5,)\n",
    "\n",
    "# Directories\n",
    "output_dir = 'vae-output/'  # Replace with the actual path to your output directory\n",
    "dataset_dirs = [\n",
    "    '../data/ma-boston/parcels',\n",
    "    '../data/nc-charlotte/parcels', \n",
    "    '../data/ny-manhattan/parcels', \n",
    "    '../data/pa-pittsburgh/parcels'  \n",
    "]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),  # Resize to a fixed size\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize(normalize_mean, normalize_std)  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Collect all image paths\n",
    "all_image_paths = []\n",
    "for dataset_dir in dataset_dirs:\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):  # Consider image files only\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Randomly sample the images from the collected paths\n",
    "sampled_image_paths = random.sample(all_image_paths, total_samples)\n",
    "\n",
    "# Custom dataset to load images from the sampled paths\n",
    "class SampledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Returning 0 as a placeholder label\n",
    "\n",
    "# Create a dataset and dataloader for the sampled images\n",
    "sampled_dataset = SampledImageDataset(sampled_image_paths, transform=transform)\n",
    "dataloader = DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VQ-VAE Model Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.size())\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()  # Straight-through estimator\n",
    "\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, num_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "# Set device to MPS if available, otherwise fall back to CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VQVAE(in_channels=3, hidden_channels=hidden_channels, num_embeddings=num_embeddings,\n",
    "              embedding_dim=embedding_dim, commitment_cost=commitment_cost).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Save training parameters to a JSON file\n",
    "training_params = {\n",
    "    \"total_samples\": total_samples,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"commitment_cost\": commitment_cost,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_embeddings\": num_embeddings,\n",
    "    \"checkpoint_interval\": checkpoint_interval\n",
    "}\n",
    "params_path = os.path.join(output_dir, 'training_params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(training_params, f)\n",
    "print(f\"Training parameters saved to {params_path}\")\n",
    "\n",
    "# Initialize CSV file for logging loss and validation metrics\n",
    "log_path = os.path.join(output_dir, 'training_log.csv')\n",
    "with open(log_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['epoch', 'loss']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Calculate the total number of iterations\n",
    "total_iterations = num_epochs * len(dataloader)\n",
    "\n",
    "# Create a single progress bar for all epochs and batches\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Training Progress\")\n",
    "\n",
    "# Training Loop with a single Progress Bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, vq_loss = model(images)\n",
    "        recon_loss = criterion(reconstructed, images)\n",
    "        loss = recon_loss + vq_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    progress_bar.set_postfix(Loss=avg_loss)\n",
    "\n",
    "    # Log the loss to the CSV file\n",
    "    with open(log_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'epoch': epoch + 1, 'loss': avg_loss})\n",
    "\n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Save the final trained model\n",
    "model_output_dir = os.path.join(output_dir, 'models')\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_output_dir, \"vq-vae_model_4-cities-200-epochs-1024-bs-2k-images.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "\n",
    "# Generate images using the trained model\n",
    "\n",
    "# Assuming 'model' and 'images' are already defined\n",
    "# output the latent vectors (e.g. 1000 dimensional vectors for each image)\n",
    "vector = model.encoder(images)\n",
    "output = model.decoder(vector)\n",
    "\n",
    "# Convert the output tensor to a NumPy array\n",
    "output_np = output.detach().cpu().numpy()\n",
    "\n",
    "# Plot and save the output images one at a time\n",
    "# Assuming the output shape is (batch_size, channels, height, width)\n",
    "batch_size = output_np.shape[0]\n",
    "\n",
    "# Ensure the directory exists\n",
    "img_output_dir = os.path.join(output_dir, 'images')\n",
    "os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "# List to store the paths of saved images\n",
    "saved_image_paths = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    img = output_np[i].transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Define the output path for the image\n",
    "    img_output_path = os.path.join(img_output_dir, f'output_image_{i}.png')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(img_output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "    \n",
    "    # Add the path to the list\n",
    "    saved_image_paths.append(img_output_path)\n",
    "\n",
    "# Print a summary of saved images\n",
    "print(f\"Saved {len(saved_image_paths)} images to {img_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Parameters\n",
    "total_samples = 2000  # start with low for testing, should be on +50k\n",
    "batch_size = 1024\n",
    "num_epochs = 200\n",
    "learning_rate = 1e-3\n",
    "commitment_cost = 0.25\n",
    "hidden_channels = 128\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "checkpoint_interval = 50\n",
    "image_size = (16, 16)  # Resize to a fixed size\n",
    "normalize_mean = (0.5,)\n",
    "normalize_std = (0.5,)\n",
    "\n",
    "# Directories\n",
    "output_dir = 'vae-output/'  # All output will be within this directory\n",
    "dataset_dirs = [\n",
    "    '../data/ma-boston/parcels',\n",
    "    '../data/nc-charlotte/parcels', \n",
    "    '../data/ny-manhattan/parcels', \n",
    "    '../data/pa-pittsburgh/parcels'  \n",
    "]\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(normalize_mean, normalize_std)\n",
    "])\n",
    "\n",
    "# Collect all image paths\n",
    "all_image_paths = []\n",
    "for dataset_dir in dataset_dirs:\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Randomly sample the images from the collected paths\n",
    "sampled_image_paths = random.sample(all_image_paths, total_samples)\n",
    "\n",
    "# Custom dataset to load images from the sampled paths\n",
    "class SampledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Returning 0 as a placeholder label\n",
    "\n",
    "# Create a dataset and dataloader for the sampled images\n",
    "sampled_dataset = SampledImageDataset(sampled_image_paths, transform=transform)\n",
    "dataloader = DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VQ-VAE Model Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.size())\n",
    "\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()\n",
    "\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, num_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VQVAE(in_channels=3, hidden_channels=hidden_channels, num_embeddings=num_embeddings,\n",
    "              embedding_dim=embedding_dim, commitment_cost=commitment_cost).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Save training parameters\n",
    "training_params = {\n",
    "    \"total_samples\": total_samples,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"commitment_cost\": commitment_cost,\n",
    "    \"hidden_channels\": hidden_channels,\n",
    "    \"embedding_dim\": embedding_dim,\n",
    "    \"num_embeddings\": num_embeddings,\n",
    "    \"checkpoint_interval\": checkpoint_interval\n",
    "}\n",
    "params_path = os.path.join(output_dir, 'training_params.json')\n",
    "with open(params_path, 'w') as f:\n",
    "    json.dump(training_params, f)\n",
    "print(f\"Training parameters saved to {params_path}\")\n",
    "\n",
    "# Initialize CSV file for logging\n",
    "log_path = os.path.join(output_dir, 'training_log.csv')\n",
    "with open(log_path, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['epoch', 'loss']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "# Training Loop\n",
    "total_iterations = num_epochs * len(dataloader)\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Training Progress\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        reconstructed, vq_loss = model(images)\n",
    "        recon_loss = criterion(reconstructed, images)\n",
    "        loss = recon_loss + vq_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    progress_bar.set_postfix(Loss=avg_loss)\n",
    "\n",
    "    with open(log_path, 'a', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writerow({'epoch': epoch + 1, 'loss': avg_loss})\n",
    "\n",
    "    if (epoch + 1) % checkpoint_interval == 0:\n",
    "        checkpoint_path = os.path.join(output_dir, f'checkpoint_epoch_{epoch + 1}.pth')\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "# Save the final trained model\n",
    "model_output_dir = os.path.join(output_dir, 'models')\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_output_dir, \"vq-vae_model_4-cities-200-epochs-1024-bs-2k-images.pth\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Generate and save images\n",
    "img_output_dir = os.path.join(output_dir, 'images')\n",
    "os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_images, _ = next(iter(dataloader))\n",
    "    sample_images = sample_images.to(device)\n",
    "    reconstructed, _ = model(sample_images)\n",
    "\n",
    "progress_bar = tqdm(total=batch_size, desc=\"Generating Images\")\n",
    "\n",
    "for i in range(batch_size):\n",
    "    img = reconstructed[i].cpu().numpy().transpose(1, 2, 0)\n",
    "    img = (img - img.min()) / (img.max() - img.min())\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    img_output_path = os.path.join(img_output_dir, f'output_image_{i}.png')\n",
    "    plt.savefig(img_output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "    \n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "print(f\"Generated and saved {batch_size} images to {img_output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
