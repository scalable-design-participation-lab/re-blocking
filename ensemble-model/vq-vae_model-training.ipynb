{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm  # Use tqdm.notebook for Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "total_samples = 20000 # start withlow for testing, should be on +50k\n",
    "batch_size = 512\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-3\n",
    "commitment_cost = 0.25\n",
    "hidden_channels = 128\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "\n",
    "# Directories containing the images\n",
    "dataset_dirs = [\n",
    "    '../data/ma-boston/parcels',\n",
    "    '../data/nc-charlotte/parcels', \n",
    "    '../data/ny-manhattan/parcels', \n",
    "    '../data/pa-pittsburgh/parcels'  \n",
    "]\n",
    "\n",
    "# Output directory for saving the trained model\n",
    "output_dir = 'vae-output/'  # Replace with the actual path to your output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((16, 16)),  # Resize to a fixed size\n",
    "    transforms.ToTensor(),        # Convert to tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Collect all image paths\n",
    "all_image_paths = []\n",
    "for dataset_dir in dataset_dirs:\n",
    "    for root, _, files in os.walk(dataset_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(('.png', '.jpg', '.jpeg')):  # Consider image files only\n",
    "                all_image_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Randomly sample the images from the collected paths\n",
    "sampled_image_paths = random.sample(all_image_paths, total_samples)\n",
    "\n",
    "# Custom dataset to load images from the sampled paths\n",
    "class SampledImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')  # Ensure image is in RGB format\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Returning 0 as a placeholder label\n",
    "\n",
    "# Create a dataset and dataloader for the sampled images\n",
    "sampled_dataset = SampledImageDataset(sampled_image_paths, transform=transform)\n",
    "dataloader = DataLoader(sampled_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# VQ-VAE Model Definition\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(hidden_channels, embedding_dim, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.conv1 = nn.ConvTranspose2d(embedding_dim, hidden_channels, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(hidden_channels, out_channels, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1/self.num_embeddings, 1/self.num_embeddings)\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Quantize\n",
    "        flattened = x.view(-1, self.embedding_dim)\n",
    "        distances = torch.cdist(flattened, self.embedding.weight)\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        quantized = self.embedding(encoding_indices).view(x.size())\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), x)\n",
    "        q_latent_loss = F.mse_loss(quantized, x.detach())\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = x + (quantized - x).detach()  # Straight-through estimator\n",
    "\n",
    "        return quantized, loss, encoding_indices\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super(VQVAE, self).__init__()\n",
    "        self.encoder = Encoder(in_channels, hidden_channels, num_embeddings, embedding_dim)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_channels, in_channels)\n",
    "        self.vq_layer = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        quantized, vq_loss, _ = self.vq_layer(encoded)\n",
    "        decoded = self.decoder(quantized)\n",
    "        return decoded, vq_loss\n",
    "\n",
    "# Set device to MPS if available, otherwise fall back to CUDA or CPU\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = VQVAE(in_channels=3, hidden_channels=hidden_channels, num_embeddings=num_embeddings,\n",
    "              embedding_dim=embedding_dim, commitment_cost=commitment_cost).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b691c0666f594a418e9698cbf2cf42fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to vae-output/models/vq-vae_model_4-cities.pth\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of iterations\n",
    "total_iterations = num_epochs * len(dataloader)\n",
    "\n",
    "# Create a single progress bar for all epochs and batches\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Training Progress\")\n",
    "\n",
    "# Training Loop with a single Progress Bar\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        reconstructed, vq_loss = model(images)\n",
    "        recon_loss = criterion(reconstructed, images)\n",
    "        loss = recon_loss + vq_loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Update the progress bar\n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
    "\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    progress_bar.set_postfix(Loss=avg_loss)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Save the trained model\n",
    "# Ensure the directory exists\n",
    "model_output_dir = os.path.join(output_dir, 'models')\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model_save_path = os.path.join(model_output_dir, \"vq-vae_model_4-cities.pth\")\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 4, 4])\n",
      "torch.Size([32, 3, 16, 16])\n",
      "Saved 32 images to vae-output/images\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'model' and 'images' are already defined\n",
    "# output the latent vectors (e.g. 1000 dimensional vectors for each image)\n",
    "vector = model.encoder(images)\n",
    "print(vector.shape)\n",
    "output = model.decoder(vector)\n",
    "print(output.shape)\n",
    "\n",
    "# Convert the output tensor to a NumPy array\n",
    "output_np = output.detach().cpu().numpy()\n",
    "\n",
    "# Plot and save the output images one at a time\n",
    "# Assuming the output shape is (batch_size, channels, height, width)\n",
    "batch_size = output_np.shape[0]\n",
    "\n",
    "# Ensure the directory exists\n",
    "img_output_dir = os.path.join(output_dir, 'images')\n",
    "os.makedirs(img_output_dir, exist_ok=True)\n",
    "\n",
    "# List to store the paths of saved images\n",
    "saved_image_paths = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    img = output_np[i].transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n",
    "    img = (img - img.min()) / (img.max() - img.min())  # Normalize to [0, 1]\n",
    "    \n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Define the output path for the image\n",
    "    img_output_path = os.path.join(img_output_dir, f'output_image_{i}.png')\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(img_output_path, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "    \n",
    "    # Add the path to the list\n",
    "    saved_image_paths.append(img_output_path)\n",
    "\n",
    "# Print a summary of saved images\n",
    "print(f\"Saved {len(saved_image_paths)} images to {img_output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
