{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from PIL import Image, ImageFile\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Parameters to tweak (quick test in #)\n",
    "batch_size = 64  # 128\n",
    "learning_rate = 1e-5 #0.001\n",
    "num_epochs = 100  # 5\n",
    "checkpoint_interval = 50\n",
    "\n",
    "# Directories\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "identifier = f\"softmax-resnet-18_{num_epochs}-ep_{batch_size}-bs_{current_time}\"\n",
    "class_names = ['Boston', 'Charlotte', 'Manhattan', 'Pittsburgh']\n",
    "folders = {\n",
    "    'Boston': '../data/ma-boston/buildings',\n",
    "    'Charlotte': '../data/nc-charlotte/buildings',\n",
    "    'Manhattan': '../data/ny-manhattan/buildings',\n",
    "    'Pittsburgh': '../data/pa-pittsburgh/buildings'\n",
    "}\n",
    "output_folder = os.path.join('softmax-output', identifier)\n",
    "checkpoint_dir = os.path.join(output_folder, 'checkpoints')\n",
    "model_save_path = os.path.join(output_folder, f'trained-model_{identifier}.pth')\n",
    "loss_log_path = os.path.join(output_folder, f'loss-log_{identifier}.json')\n",
    "training_curves_path = os.path.join(output_folder, f'training-curves_{identifier}.png')\n",
    "confusion_matrix_path = os.path.join(output_folder, f'confusion-matrix_{identifier}.png')\n",
    "cross_validation_path = os.path.join(output_folder, f'cross-validation_{identifier}.png')\n",
    "misclassified_samples_path = os.path.join(output_folder, f'misclassified-samples_{identifier}.png')\n",
    "report_path = os.path.join(output_folder, f'report_{identifier}.txt')\n",
    "new_image_path = '../data/ny-brooklyn/buildings/buildings_1370.jpg'\n",
    "predictions_output_file = os.path.join(output_folder, f'predictions_{identifier}.txt')\n",
    "\n",
    "# More Parameters\n",
    "normalize_mean = [0.485, 0.456, 0.406]\n",
    "normalize_std = [0.229, 0.224, 0.225]\n",
    "num_classes = len(class_names)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Allow loading of truncated images\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Define output folder\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class CityDataset(Dataset):\n",
    "    def __init__(self, folders, transform=None, max_images_per_class=500):  # Limit images per class\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(folders.keys())}\n",
    "\n",
    "        for class_name, folder in folders.items():\n",
    "            class_images = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "            selected_images = random.sample(class_images, min(max_images_per_class, len(class_images)))\n",
    "            self.image_paths.extend(selected_images)\n",
    "            self.labels.extend([self.class_to_idx[class_name]] * len(selected_images))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Create dataset\n",
    "dataset = CityDataset(folders)\n",
    "\n",
    "# Define transformations with data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to a smaller, fixed size\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=normalize_mean, std=normalize_std),\n",
    "])\n",
    "\n",
    "# Update dataset with transform\n",
    "dataset.transform = transform\n",
    "\n",
    "# Set device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load a pre-trained ResNet18 model (smaller than ResNet50)\n",
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=weights)\n",
    "\n",
    "# Modify the final layer to match the number of classes\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Model training function\n",
    "def train_and_save_model(model, train_loader, val_loader, num_epochs, checkpoint_interval, checkpoint_dir):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    train_loss_log = []\n",
    "    val_loss_log = []\n",
    "    val_accuracy_log = []\n",
    "    epoch_times = []\n",
    "\n",
    "    total_iterations = num_epochs * len(train_loader)\n",
    "    progress_bar = tqdm(total=total_iterations, desc=\"Training Progress\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_log.append(epoch_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_accuracy = correct / total\n",
    "        val_loss_log.append(val_epoch_loss)\n",
    "        val_accuracy_log.append(val_accuracy)\n",
    "\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_duration = epoch_end_time - epoch_start_time\n",
    "        epoch_times.append(epoch_duration)\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'Epoch': f'{epoch + 1}/{num_epochs}',\n",
    "            'Train Loss': f'{epoch_loss:.4f}',\n",
    "            'Val Loss': f'{val_epoch_loss:.4f}',\n",
    "            'Val Accuracy': f'{val_accuracy:.4f}',\n",
    "            'Epoch Time (s)': f'{epoch_duration:.2f}'\n",
    "        })\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}_{identifier}.pth')\n",
    "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    progress_bar.close()\n",
    "\n",
    "    # Save the final model weights\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # Save the loss and accuracy logs\n",
    "    with open(loss_log_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'train_loss': train_loss_log,\n",
    "            'val_loss': val_loss_log,\n",
    "            'val_accuracy': val_accuracy_log,\n",
    "            'epoch_times': epoch_times\n",
    "        }, f)\n",
    "\n",
    "    # Plot the loss and accuracy curves\n",
    "    plot_training_curves(train_loss_log, val_loss_log, val_accuracy_log)\n",
    "\n",
    "    return train_loss_log, val_loss_log, val_accuracy_log\n",
    "\n",
    "def plot_training_curves(train_loss_log, val_loss_log, val_accuracy_log):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(1, num_epochs + 1), train_loss_log, label='Train Loss')\n",
    "    plt.plot(range(1, num_epochs + 1), val_loss_log, label='Validation Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), val_accuracy_log)\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(training_curves_path)\n",
    "    plt.close()\n",
    "\n",
    "def k_fold_cross_validation(dataset, num_folds=3):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    fold_results = []\n",
    "    fold_times = []\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_train_loss_logs = []\n",
    "    all_val_loss_logs = []\n",
    "    all_val_accuracy_logs = []\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset), 1):\n",
    "        print(f\"Fold {fold}\")\n",
    "        fold_start_time = time.time()\n",
    "        \n",
    "        train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
    "        val_subsampler = torch.utils.data.SubsetRandomSampler(val_ids)\n",
    "        \n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_subsampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_subsampler)\n",
    "        \n",
    "        model = models.resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "        model.to(device)\n",
    "        \n",
    "        train_loss_log, val_loss_log, val_accuracy_log = train_and_save_model(\n",
    "            model, train_loader, val_loader, num_epochs, checkpoint_interval, \n",
    "            os.path.join(checkpoint_dir, f'fold_{fold}')\n",
    "        )\n",
    "        \n",
    "        all_train_loss_logs.append(train_loss_log)\n",
    "        all_val_loss_logs.append(val_loss_log)\n",
    "        all_val_accuracy_logs.append(val_accuracy_log)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        fold_labels = []\n",
    "        fold_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                fold_labels.extend(labels.cpu().numpy())\n",
    "                fold_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        accuracy = correct / total\n",
    "        fold_results.append(accuracy)\n",
    "        all_labels.extend(fold_labels)\n",
    "        all_predictions.extend(fold_predictions)\n",
    "        fold_end_time = time.time()\n",
    "        fold_duration = fold_end_time - fold_start_time\n",
    "        fold_times.append(fold_duration)\n",
    "        print(f\"Fold {fold} accuracy: {accuracy:.4f}, Time: {fold_duration:.2f} seconds\")\n",
    "    \n",
    "    average_accuracy = sum(fold_results) / len(fold_results)\n",
    "    print(f\"Average accuracy across folds: {average_accuracy:.4f}\")\n",
    "    print(f\"Average time per fold: {sum(fold_times) / len(fold_times):.2f} seconds\")\n",
    "\n",
    "    # Plot cross-validation results\n",
    "    plot_cross_validation_results(fold_results, average_accuracy, num_folds)\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "    # Plot misclassified samples\n",
    "    plot_misclassified_samples(dataset, all_labels, all_predictions)\n",
    "\n",
    "    # Generate classification report\n",
    "    generate_classification_report(all_labels, all_predictions)\n",
    "\n",
    "    # Calculate and save additional metrics\n",
    "    calculate_additional_metrics(all_train_loss_logs, all_val_loss_logs)\n",
    "\n",
    "def plot_cross_validation_results(fold_results, average_accuracy, num_folds):\n",
    "    plt.figure()\n",
    "    plt.bar(range(1, num_folds + 1), fold_results, tick_label=[f'Fold {i}' for i in range(1, num_folds + 1)])\n",
    "    plt.axhline(y=average_accuracy, color='r', linestyle='--', label=f'Average Accuracy: {average_accuracy:.4f}')\n",
    "    plt.title('Cross-Validation Accuracy')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(cross_validation_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(all_labels, all_predictions):\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.savefig(confusion_matrix_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_misclassified_samples(dataset, all_labels, all_predictions):\n",
    "    misclassified_indices = [i for i, (label, pred) in enumerate(zip(all_labels, all_predictions)) if label != pred]\n",
    "    if misclassified_indices:\n",
    "        plt.figure(figsize=(12, 12))\n",
    "        for i, idx in enumerate(misclassified_indices[:16]):  # Show up to 16 misclassified samples\n",
    "            image, label = dataset[idx]\n",
    "            plt.subplot(4, 4, i + 1)\n",
    "            plt.imshow(image.permute(1, 2, 0).numpy())\n",
    "            plt.title(f'True: {class_names[label]}, Pred: {class_names[all_predictions[idx]]}')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(misclassified_samples_path)\n",
    "        plt.close()\n",
    "\n",
    "def generate_classification_report(all_labels, all_predictions):\n",
    "    report = classification_report(all_labels, all_predictions, target_names=class_names)\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "    print(f\"Classification report saved to {report_path}\")\n",
    "\n",
    "def calculate_additional_metrics(all_train_loss_logs, all_val_loss_logs):\n",
    "    avg_train_loss_log = np.mean(all_train_loss_logs, axis=0)\n",
    "    avg_val_loss_log = np.mean(all_val_loss_logs, axis=0)\n",
    "\n",
    "    convergence_rate = (avg_train_loss_log[-1] - avg_train_loss_log[0]) / num_epochs\n",
    "    overfitting_score = (avg_val_loss_log[-1] - avg_train_loss_log[-1]) / avg_val_loss_log[-1]\n",
    "    learning_plateau = np.mean(avg_val_loss_log[-5:]) - np.mean(avg_val_loss_log[:5])\n",
    "\n",
    "    with open(report_path, 'a') as f:\n",
    "        f.write(f\"\\nConvergence Rate: {convergence_rate:.4f}\\n\")\n",
    "        f.write(f\"Overfitting Score: {overfitting_score:.4f}\\n\")\n",
    "        f.write(f\"Learning Plateau: {learning_plateau:.4f}\\n\")\n",
    "\n",
    "# Run k-fold cross-validation\n",
    "k_fold_cross_validation(dataset)\n",
    "\n",
    "# Function to predict on a new image\n",
    "def predict_image(model, image_path, transform):\n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        probabilities = F.softmax(outputs, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities).item()\n",
    "    \n",
    "    return probabilities, predicted_class\n",
    "\n",
    "# Predict on a new image\n",
    "new_image_probabilities, new_image_class = predict_image(model, new_image_path, transform)\n",
    "\n",
    "# Print and save predictions\n",
    "predictions = [f\"{class_names[i]}: {prob:.2f}\" for i, prob in enumerate(new_image_probabilities)]\n",
    "print(f\"Predictions for {new_image_path}:\")\n",
    "print(f\"Predicted class: {class_names[new_image_class]}\")\n",
    "print(\"Class probabilities:\")\n",
    "for pred in predictions:\n",
    "    print(pred)\n",
    "\n",
    "with open(predictions_output_file, 'w') as f:\n",
    "    f.write(f\"Predictions for {new_image_path}:\\n\")\n",
    "    f.write(f\"Predicted class: {class_names[new_image_class]}\\n\")\n",
    "    f.write(\"Class probabilities:\\n\")\n",
    "    for pred in predictions:\n",
    "        f.write(f\"{pred}\\n\")\n",
    "\n",
    "print(f\"Predictions saved to {predictions_output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
